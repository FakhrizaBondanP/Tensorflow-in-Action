{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CHAPTER 12: Sequence-to-Sequence Learning dengan Attention\n",
        "\n",
        "## Ringkasan\n",
        "\n",
        "Chapter ini melanjutkan pembangunan machine translator dari Chapter 11 dengan menambahkan **Bahdanau attention mechanism** untuk meningkatkan accuracy secara signifikan. Tanpa attention, decoder hanya akses context vector final dari encoder (bottleneck besar). Dengan attention, decoder dapat \"melihat\" semua output timestep encoder dan secara dinamis memilih bagian mana yang paling relevan untuk setiap decoding step. Implementasi menggunakan custom `DecoderRNNAttentionWrapper` layer berbasis Keras subclassing API. Chapter juga menunjukkan bagaimana visualisasi attention weights memberikan interpretability—mengungkap word alignment patterns antara English dan German yang sesuai secara linguistik.\n",
        "\n",
        "---\n",
        "\n",
        "## Konsep Attention Mechanism\n",
        "\n",
        "### Problem Tanpa Attention\n",
        "\n",
        "Dalam seq2seq model standard (Chapter 11), encoder mengubah English sentence menjadi single context vector—representasi \"compact\" dari seluruh input. Decoder kemudian harus decode entire German sentence menggunakan hanya context vector ini. Ini adalah massive bottleneck, khususnya untuk sentences panjang: tidak praktis untuk single vector merangkum semua information dari 20+ word sentence. Hasilnya, model kesulitan pada long sequences dan tidak bisa \"track\" dengan baik word correspondence antara languages.\n",
        "\n",
        "### Bahdanau Attention Solution\n",
        "\n",
        "**Bahdanau attention** memecahkan ini dengan membiarkan decoder akses ke **semua** encoder outputs (bukan hanya final context vector). Untuk setiap decoding step, attention mechanism menghitung **attention weights** yang merepresentasikan berapa banyak decoder harus \"focus\" pada setiap encoder position. Weights ini digunakan untuk compute weighted sum dari encoder outputs—creating dynamic context vector yang specific untuk current decoding step. Dengan cara ini, decoder bisa adapt attention based on apa yang sedang diterjemahkan.\n",
        "\n",
        "### Attention Computation\n",
        "\n",
        "Untuk decoding step i dengan decoder state s_{i-1} dan semua encoder outputs h_j, attention dilakukan:\n",
        "\n",
        "1. **Energy calculation**: e_{ij} = v^T tanh(W·s_{i-1} + U·h_j) - mengukur \"relevance\" encoder position j untuk decoding step i\n",
        "2. **Normalization**: α_{ij} = exp(e_{ij}) / Σ_k exp(e_{ik}) - convert energies ke probability distribution\n",
        "3. **Context aggregation**: c_i = Σ_j α_{ij}·h_j - weighted sum dari encoder outputs\n",
        "\n",
        "Weights W, U, v adalah learnable matrices yang model adjust selama training. Hasilnya adalah context vector c_i yang focused pada relevant parts dari encoder input, di-concatenate dengan decoder input untuk next GRU cell.\n",
        "\n",
        "---\n",
        "\n",
        "## Custom Attention Layer Implementation\n",
        "\n",
        "### DecoderRNNAttentionWrapper Architecture\n",
        "\n",
        "Karena TensorFlow tidak provide built-in Bahdanau attention untuk seq2seq, kami implement custom layer menggunakan Keras subclassing API dengan 3 key methods:\n",
        "\n",
        "**`__init__`**: Initializes layer dengan GRUCell (decoder cell function) dan units (attention hidden dimension).\n",
        "\n",
        "**`build`**: Declares 3 weight matrices:\n",
        "- W_a: shape [encoder_hidden, attention_hidden]\n",
        "- U_a: shape [decoder_hidden, attention_hidden]\n",
        "- V_a: shape [attention_hidden, 1]\n",
        "\n",
        "Weights initialized uniform dan trainable.\n",
        "\n",
        "**`call`**: Main computation yang iterate melalui decoder timesteps menggunakan K.rnn() backend function. Untuk setiap timestep, `_step` function computes attention weights menggunakan encoder outputs dan current decoder state, generate attention-weighted context vector, concatenate dengan decoder input, feed ke GRUCell untuk generate output dan next state.\n",
        "\n",
        "### GRUCell vs GRU Layer\n",
        "\n",
        "GRUCell adalah primitive building block yang compute single timestep: `output, next_state = GRUCell(input, state)`. GRU layer adalah full implementation yang process entire sequences dengan convenience methods. Kami gunakan GRUCell di attention layer karena perlu fine-grained control over individual timesteps untuk attention computation.\n",
        "\n",
        "---\n",
        "\n",
        "## Model dengan Attention\n",
        "\n",
        "### Encoder (Unchanged)\n",
        "\n",
        "Encoder tetap identik dengan Chapter 11: string input → TextVectorization → Embedding → Bidirectional GRU. Bidirectional processing penting untuk understand context dari kedua directions. Output adalah context vector ditambah full sequence dari encoder hidden states (needed untuk attention).\n",
        "\n",
        "### Decoder dengan Attention\n",
        "\n",
        "Decoder dimodifikasi signifikan:\n",
        "\n",
        "1. **Input**: German sequence vectorized dan embedded (seperti sebelumnya)\n",
        "2. **Attention layer**: DecoderRNNAttentionWrapper menerima encoder output sequence dan decoder embeddings, outputs attention-weighted outputs plus attention weights matrix\n",
        "3. **Concatenation**: Attention outputs (already concatenated dengan decoder input internally) dipass ke Dense hidden layer\n",
        "4. **Output**: Final Dense layer outputs probability distribution atas German vocabulary\n",
        "\n",
        "Key improvement: decoder sekarang dapat akses ALL encoder information, bukan hanya final context vector.\n",
        "\n",
        "---\n",
        "\n",
        "## Training dan Evaluation\n",
        "\n",
        "### Performance Improvement\n",
        "\n",
        "Model dengan attention mencapai BLEU score **2x higher** dari model tanpa attention:\n",
        "- Tanpa attention: ~0.1 BLEU\n",
        "- Dengan attention: ~0.20 BLEU di validation set\n",
        "\n",
        "Ini significant improvement menunjukkan attention mechanism benar-benar solve bottleneck problem. Accuracy juga naik dari ~73% ke ~83%, dan training berjalan dalam 5 menit untuk 5 epochs.\n",
        "\n",
        "### BLEU Score Interpretation\n",
        "\n",
        "BLEU score 0.20 means model getting roughly 20% of n-gram matches exact dengan reference translations. Dibandingkan state-of-the-art 0.35 pada WMT dataset, score kami decent considering model simplicity dan training time. Gap merefleksikan:\n",
        "- Smaller dataset (50K vs millions)\n",
        "- Simpler architecture (no transformers, layer normalization, etc.)\n",
        "- No special training tricks (curriculum learning, back-translation, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "## Attention Visualization untuk Interpretability\n",
        "\n",
        "### Mengapa Visualisasi Penting\n",
        "\n",
        "Salah satu keuntungan major dari attention adalah **interpretability**. Attention weights matrix (α) memiliki entry untuk setiap encoder-decoder timestep pair. Matriks ini bisa diviz sebagai heatmap—lighter colors = higher attention. Ini reveals word alignment patterns: jika German word \"und\" (and) attend highly ke English word \"and\", model benar-benar belajar linguistic correspondences.\n",
        "\n",
        "### Visualization Process\n",
        "\n",
        "1. **Load trained model** dan extract semua intermediate layers\n",
        "2. **Create visualizer model** yang outputs predictions PLUS attention weights matrix PLUS vectorized tokens\n",
        "3. **Untuk sample English sentence**: predict German translation dan get attention weights\n",
        "4. **Create heatmap** dengan:\n",
        "   - X-axis: predicted German words\n",
        "   - Y-axis: input English words\n",
        "   - Cell values: attention weights (darker = more attention)\n",
        "5. **Analyze patterns**: observe word alignments, check jika sensible\n",
        "\n",
        "### Example Patterns\n",
        "\n",
        "Pada \"Tom and Mary haven't heard from John in a long time\" → \"Tom und Maria [UNK] schon seit [UNK] [UNK] [UNK] Johannes gestohlen\":\n",
        "- \"und\" attends heavily to \"and\"\n",
        "- \"Maria\" attends to \"Mary\"\n",
        "- \"gestohlen\" attends to surrounding context\n",
        "- Attention pattern roughly diagonal (natural untuk closely related languages)\n",
        "\n",
        "---\n",
        "\n",
        "## Program-Program Implementasi\n",
        "\n",
        "### Program 1: Custom Attention Layer\n",
        "\n",
        "```python\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "class DecoderRNNAttentionWrapper(keras.layers.Layer):\n",
        "    def __init__(self, cell_fn, units, **kwargs):\n",
        "        self._cell_fn = cell_fn\n",
        "        self.units = units\n",
        "        super(DecoderRNNAttentionWrapper, self).__init__(**kwargs)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        # W_a: [encoder_hidden, attention_hidden]\n",
        "        self.W_a = self.add_weight(\n",
        "            name='W_a',\n",
        "            shape=(input_shape[0][2], input_shape[0][2]),\n",
        "            initializer='uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        # U_a: [decoder_hidden, attention_hidden]\n",
        "        self.U_a = self.add_weight(\n",
        "            name='U_a',\n",
        "            shape=(self._cell_fn.units, self._cell_fn.units),\n",
        "            initializer='uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        # V_a: [attention_hidden, 1]\n",
        "        self.V_a = self.add_weight(\n",
        "            name='V_a',\n",
        "            shape=(input_shape[0][2], 1),\n",
        "            initializer='uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        super(DecoderRNNAttentionWrapper, self).build(input_shape)\n",
        "    \n",
        "    def call(self, inputs, initial_state, training=False):\n",
        "        def _step(inputs, states):\n",
        "            encoder_outputs = states[-1]  # Extract dari constants\n",
        "            \n",
        "            # Compute energy: e_ij = v^T tanh(W·s + U·h)\n",
        "            W_a_dot_h = K.dot(encoder_outputs, self.W_a)\n",
        "            U_a_dot_s = K.expand_dims(K.dot(states[0], self.U_a), 1)\n",
        "            Wh_plus_Us = K.tanh(W_a_dot_h + U_a_dot_s)\n",
        "            e_i = K.squeeze(K.dot(Wh_plus_Us, self.V_a), axis=-1)\n",
        "            \n",
        "            # Normalize ke probabilities\n",
        "            a_i = K.softmax(e_i)\n",
        "            \n",
        "            # Weighted sum dari encoder outputs\n",
        "            c_i = K.sum(encoder_outputs * K.expand_dims(a_i, -1), axis=1)\n",
        "            \n",
        "            # Concatenate attention context dengan decoder input\n",
        "            # Feed ke GRUCell\n",
        "            s, states = self._cell_fn(\n",
        "                K.concatenate([inputs, c_i], axis=-1), states\n",
        "            )\n",
        "            \n",
        "            return (s, a_i), states\n",
        "        \n",
        "        encoder_outputs, decoder_inputs = inputs\n",
        "        \n",
        "        # K.rnn iterates _step untuk setiap decoder timestep\n",
        "        _, attn_outputs, _ = K.rnn(\n",
        "            step_function=_step,\n",
        "            inputs=decoder_inputs,\n",
        "            initial_states=[initial_state],\n",
        "            constants=[encoder_outputs]\n",
        "        )\n",
        "        \n",
        "        attn_out, attn_energy = attn_outputs\n",
        "        return attn_out, attn_energy\n",
        "```\n",
        "\n",
        "**Penjelasan**: Custom layer implement Bahdanau attention. K.rnn() adalah TensorFlow backend function yang iteratively apply _step function untuk setiap decoder timestep, passing encoder outputs sebagai constants. Attention weights computed sebagai softmax dari energy scores, digunakan untuk weight encoder outputs.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 2: Final Model dengan Attention\n",
        "\n",
        "```python\n",
        "def get_final_seq2seq_model_with_attention(n_vocab, encoder, de_vectorizer):\n",
        "    # Encoder input dan get encoder outputs\n",
        "    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')\n",
        "    fwd_state, bwd_state, en_states = encoder(e_inp)\n",
        "    \n",
        "    # Decoder input\n",
        "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
        "    d_vectorized = de_vectorizer(d_inp)\n",
        "    d_emb = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True)(d_vectorized)\n",
        "    \n",
        "    # Initial state: concatenate forward dan backward encoder states\n",
        "    d_init_state = tf.keras.layers.Concatenate(axis=-1)([fwd_state, bwd_state])\n",
        "    \n",
        "    # Attention layer\n",
        "    gru_cell = tf.keras.layers.GRUCell(256)\n",
        "    attn_out, _ = DecoderRNNAttentionWrapper(\n",
        "        cell_fn=gru_cell, units=512\n",
        "    )([en_states, d_emb], initial_state=d_init_state)\n",
        "    \n",
        "    # Dense layers\n",
        "    d_dense1 = tf.keras.layers.Dense(512, activation='relu')(attn_out)\n",
        "    d_final = tf.keras.layers.Dense(n_vocab+2, activation='softmax')(d_dense1)\n",
        "    \n",
        "    model = tf.keras.models.Model(\n",
        "        inputs=[e_inp, d_inp],\n",
        "        outputs=d_final,\n",
        "        name='seq2seq_with_attention'\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "encoder = get_encoder(en_vocab, en_vectorizer)\n",
        "model_with_attention = get_final_seq2seq_model_with_attention(\n",
        "    de_vocab, encoder, de_vectorizer\n",
        ")\n",
        "model_with_attention.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "```\n",
        "\n",
        "**Penjelasan**: Model kombinasi encoder dan decoder dengan attention. Key difference: `en_states` (semua encoder timestep outputs) dipass ke attention layer, bukan hanya final context vector. Attention layer outputs attention-weighted features yang concatenate dengan decoder input before passing ke GRUCell.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 3: Model Training\n",
        "\n",
        "```python\n",
        "def train_model(model, de_vectorizer, train_df, valid_df, test_df,\n",
        "                epochs=5, batch_size=128):\n",
        "    bleu_metric = BLEUMetric(de_vocabulary)\n",
        "    data = prepare_data(train_df, valid_df, test_df)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        train_data = data['train']\n",
        "        indices = np.random.permutation(len(train_data['encoder_inputs']))\n",
        "        for key in train_data:\n",
        "            train_data[key] = train_data[key][indices]\n",
        "        \n",
        "        loss_list, acc_list, bleu_list = [], [], []\n",
        "        n_batches = len(train_data['encoder_inputs']) // batch_size\n",
        "        \n",
        "        for i in range(n_batches):\n",
        "            start, end = i * batch_size, (i + 1) * batch_size\n",
        "            x = [\n",
        "                train_data['encoder_inputs'][start:end],\n",
        "                train_data['decoder_inputs'][start:end]\n",
        "            ]\n",
        "            y = de_vectorizer(train_data['decoder_labels'][start:end])\n",
        "            \n",
        "            model.train_on_batch(x, y)\n",
        "            loss, acc = model.evaluate(x, y, verbose=0)\n",
        "            pred_y = model.predict(x, verbose=0)\n",
        "            bleu = bleu_metric.calculate_bleu(y, pred_y)\n",
        "            \n",
        "            loss_list.append(loss), acc_list.append(acc), bleu_list.append(bleu)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}: Loss={np.mean(loss_list):.4f}, \"\n",
        "              f\"Acc={np.mean(acc_list):.4f}, BLEU={np.mean(bleu_list):.4f}\")\n",
        "\n",
        "train_model(model_with_attention, de_vectorizer, train_df, valid_df, test_df)\n",
        "```\n",
        "\n",
        "**Penjelasan**: Training loop sama seperti sebelumnya—batch training dengan shuffle, compute loss/accuracy/BLEU metrics. Key output: BLEU score ~0.20 (2x improvement vs tanpa attention).\n",
        "\n",
        "---\n",
        "\n",
        "### Program 4: Attention Visualizer Model\n",
        "\n",
        "```python\n",
        "def attention_visualizer(save_path):\n",
        "    model = tf.keras.models.load_model(save_path)\n",
        "    \n",
        "    # Encoder setup\n",
        "    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')\n",
        "    en_model = model.get_layer(\"encoder\")\n",
        "    fwd_state, bwd_state, en_states = en_model(e_inp)\n",
        "    e_vec_out = en_model.get_layer(\"e_vectorizer\")(e_inp)\n",
        "    \n",
        "    # Decoder setup\n",
        "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
        "    d_vec_out = model.get_layer('d_vectorizer')(d_inp)\n",
        "    d_emb_out = model.get_layer('d_embedding')(d_vec_out)\n",
        "    \n",
        "    # Attention\n",
        "    d_init_state = tf.keras.layers.Concatenate(axis=-1)([fwd_state, bwd_state])\n",
        "    d_attn_layer = model.get_layer(\"d_attention\")\n",
        "    attn_out, attn_states = d_attn_layer(\n",
        "        [en_states, d_emb_out], initial_state=d_init_state\n",
        "    )\n",
        "    \n",
        "    # Dense layers\n",
        "    d_dense1_out = model.get_layer(\"d_dense_1\")(attn_out)\n",
        "    d_final_out = model.get_layer(\"d_dense_final\")(d_dense1_out)\n",
        "    \n",
        "    # Return model yang output predictions, attention weights, vectorized tokens\n",
        "    visualizer_model = tf.keras.models.Model(\n",
        "        inputs=[e_inp, d_inp],\n",
        "        outputs=[d_final_out, attn_states, e_vec_out, d_vec_out]\n",
        "    )\n",
        "    \n",
        "    return visualizer_model\n",
        "\n",
        "visualizer_model = attention_visualizer('models/seq2seq_attention')\n",
        "```\n",
        "\n",
        "**Penjelasan**: Visualizer model extract layers dari trained model dan reconstruct forward pass untuk capture intermediate outputs (especially attention weights). Multiple outputs: predictions, attention weights, vectorized token IDs untuk axes labeling.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 5: Attention Heatmap Visualization\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_attention(visualizer_model, en_vocab, de_vocab,\n",
        "                        sample_en_text, sample_de_text, save_path):\n",
        "    # Get predictions dan attention weights\n",
        "    d_pred, attention_weights, e_out, d_out = visualizer_model.predict(\n",
        "        [np.array([sample_en_text]), np.array([sample_de_text])]\n",
        "    )\n",
        "    \n",
        "    # Convert predictions ke word tokens\n",
        "    d_pred_ids = np.argmax(d_pred[0], axis=-1)\n",
        "    \n",
        "    # Build y-labels (English words)\n",
        "    y_labels = []\n",
        "    for e_id in e_out[0]:\n",
        "        if en_vocab[e_id] == \"\":  # Stop at padding\n",
        "            break\n",
        "        y_labels.append(en_vocab[e_id])\n",
        "    \n",
        "    # Build x-labels (German words)\n",
        "    x_labels = []\n",
        "    for d_id in d_pred_ids:\n",
        "        if de_vocab[d_id] == 'eos':  # Stop at EOS\n",
        "            break\n",
        "        x_labels.append(de_vocab[d_id])\n",
        "    \n",
        "    # Extract relevant attention weights (prune padding/EOS)\n",
        "    attn_filtered = attention_weights[0, :len(y_labels), :len(x_labels)]\n",
        "    \n",
        "    # Create heatmap\n",
        "    fig, ax = plt.subplots(figsize=(14, 14))\n",
        "    im = ax.imshow(attn_filtered)\n",
        "    \n",
        "    # Set labels\n",
        "    ax.set_xticks(np.arange(len(x_labels)))\n",
        "    ax.set_yticks(np.arange(len(y_labels)))\n",
        "    ax.set_xticklabels(x_labels)\n",
        "    ax.set_yticklabels(y_labels)\n",
        "    ax.tick_params(labelsize=20, axis='x', labelrotation=90)\n",
        "    \n",
        "    plt.colorbar(im)\n",
        "    plt.subplots_adjust(left=0.2, bottom=0.2)\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "# Visualize beberapa examples\n",
        "for i in range(10):\n",
        "    en_text = test_df[\"EN\"].iloc[i]\n",
        "    de_text = test_df[\"DE\"].iloc[i:i+1].str.rsplit(n=1, expand=True).iloc[:, 0]\n",
        "    visualize_attention(visualizer_model, en_vocab, de_vocab,\n",
        "                       en_text, de_text, f'plots/attention_{i}.png')\n",
        "```\n",
        "\n",
        "**Penjelasan**: Heatmap visualization menunjukkan attention weights sebagai colored grid. Lighter = higher attention. Untuk setiap German predicted word (column), heatmap shows mana English words (rows) model attended. Diagonal pattern menunjukkan word order correlation antara languages.\n",
        "\n",
        "---\n",
        "\n",
        "## Kesimpulan\n",
        "\n",
        "Bahdanau attention mechanism adalah simple namun powerful idea: instead of bottlenecking dengan single context vector, decoder akses semua encoder outputs dan dynamically weight mereka based on current decoding step. Hasil: 2x BLEU improvement, better alignment antara languages, dan interpretability via attention weights visualization. Chapter berikutnya akan explore Transformer architecture yang pure attention-based (no recurrence), providing even more powerful model untuk NLP tasks.\n"
      ],
      "metadata": {
        "id": "QUu7G6MtRx3V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jWelNu4CRzuf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}