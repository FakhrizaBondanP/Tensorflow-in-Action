{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BAB 9: Natural Language Processing dengan TensorFlow - Sentiment Analysis\n",
        "\n",
        "### Pendahuluan\n",
        "\n",
        "Bab ini membahas tentang **Natural Language Processing (NLP)** dengan fokus pada **Sentiment Analysis** (analisis sentimen). Berbeda dengan bab sebelumnya yang berfokus pada visi komputer (klasifikasi gambar dan segmentasi), bab ini mengeksplorasi cara memproses dan menganalisis data tekstual menggunakan deep learning.\n",
        "\n",
        "### Konsep NLP Dasar\n",
        "\n",
        "#### Tugas-Tugas NLP\n",
        "1. **Stop Word Removal** - Menghilangkan kata-kata yang tidak informatif seperti \"dan\", \"itu\", \"yang\"\n",
        "2. **Lemmatization** - Mengubah kata ke bentuk dasarnya (contoh: \"berjalan\" → \"jalan\")\n",
        "3. **Part of Speech (PoS) Tagging** - Menandai jenis kata (noun, verb, adjective, dll)\n",
        "4. **Named Entity Recognition (NER)** - Mengekstrak entitas seperti nama orang, lokasi\n",
        "5. **Language Modeling** - Memprediksi kata berikutnya dari kata-kata sebelumnya\n",
        "6. **Sentiment Analysis** - Mengidentifikasi sentimen positif/negatif dari teks\n",
        "7. **Machine Translation** - Menerjemahkan dari satu bahasa ke bahasa lain\n",
        "\n",
        "### Dataset: Amazon Video Game Reviews\n",
        "\n",
        "- Dataset berisi review video game dari Amazon dengan rating 1-5 bintang\n",
        "- Setiap review memiliki teks review dan rating bintang\n",
        "- Total ~332,504 verified reviews (dari 497,419 total reviews)\n",
        "- Dataset sangat **imbalanced**: 83% positif, 17% negatif\n",
        "- Setelah mapping: 4-5 bintang = positif (label=1), 1-3 bintang = negatif (label=0)\n",
        "\n",
        "## Tahap 1: Preprocessing Text\n",
        "\n",
        "### Langkah-Langkah Preprocessing\n",
        "\n",
        "Proses cleaning teks meliputi:\n",
        "1. **Lowercase** - Mengubah semua karakter menjadi huruf kecil\n",
        "2. **Expand Contractions** - Mengubah \"isn't\" menjadi \"is not\"\n",
        "3. **Remove Digits** - Menghilangkan angka-angka\n",
        "4. **Tokenization** - Memecah teks menjadi kata-kata individual\n",
        "5. **Remove Stop Words** - Menghilangkan kata-kata umum (TAPI: simpan \"not\" dan \"no\")\n",
        "6. **Lemmatization** - Mengubah kata ke bentuk dasar dengan mempertimbangkan Part of Speech\n",
        "\n",
        "---\n",
        "\n",
        "## Program-Program Penting dengan Penjelasan Detail\n",
        "\n",
        "### 1. Download Dataset\n",
        "\n",
        "**Penjelasan Ringkas:**\n",
        "Program ini mengunduh dataset review video game dari Amazon (format JSON.gzip) dari server UCSD dan mengekstrak file terkompresi menjadi JSON untuk diproses. Ini adalah langkah pertama dalam pipelined data yang menggabungkan fetching, caching (jika sudah ada, tidak download lagi), dan dekompresi.\n",
        "\n",
        "**Langkah-Langkah:**\n",
        "1. Cek apakah file .gz sudah ada di folder 'data' (cache check)\n",
        "2. Jika belum, unduh dari URL UCSD\n",
        "3. Simpan file terkompresi\n",
        "4. Dekompresi file .gz menjadi JSON\n",
        "5. Siap untuk loading ke memory\n",
        "\n",
        "```python\n",
        "import os\n",
        "import requests\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "# ===== STEP 1: Download file terkompresi =====\n",
        "# Fungsi os.path.exists() mengecek apakah file sudah ada di local disk\n",
        "# Ini penting untuk menghindari re-download data yang sama\n",
        "if not os.path.exists(os.path.join('data','Video_Games_5.json.gz')):  \n",
        "    # URL dataset dari UCSD (public dataset)\n",
        "    # Format: .gz (gzip compressed) untuk menghemat bandwidth\n",
        "    url = \"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\"\n",
        "    \n",
        "    # requests.get(url) melakukan HTTP GET request\n",
        "    # Mengembalikan response object dengan konten file\n",
        "    r = requests.get(url)\n",
        "    \n",
        "    # Buat direktori 'data' jika belum ada\n",
        "    # os.mkdir() hanya bisa membuat 1 level direktori\n",
        "    if not os.path.exists('data'):\n",
        "        os.mkdir('data')\n",
        "    \n",
        "    # Tulis (write) konten file binary ke disk\n",
        "    # 'wb' mode = write binary (karena .gz adalah binary format)\n",
        "    # r.content = raw bytes dari response\n",
        "    with open(os.path.join('data','Video_Games_5.json.gz'), 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    \n",
        "    print(\"✓ File downloaded successfully\")\n",
        "\n",
        "# ===== STEP 2: Extract file terkompresi =====\n",
        "# File .gz adalah gzip compressed format\n",
        "# Perlu di-dekompresi sebelum bisa di-baca sebagai JSON\n",
        "if not os.path.exists(os.path.join('data', 'Video_Games_5.json')):     \n",
        "    # gzip.open() membuka file terkompresi\n",
        "    # 'rb' mode = read binary (membaca bytes terkompresi)\n",
        "    with gzip.open(os.path.join('data','Video_Games_5.json.gz'), 'rb') as f_in:\n",
        "        # Buka file output untuk menulis data yang sudah didekompresi\n",
        "        # 'wb' mode = write binary\n",
        "        with open(os.path.join('data','Video_Games_5.json'), 'wb') as f_out:\n",
        "            # shutil.copyfileobj() copy file secara efficient (chunk by chunk)\n",
        "            # Lebih baik daripada read semua ke memory lalu write\n",
        "            # Hemat memory untuk file besar\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "    \n",
        "    print(\"✓ File extracted successfully\")\n",
        "else:\n",
        "    print(\"✓ File already exists, skipping download\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Load dan Explore Data\n",
        "\n",
        "**Penjelasan Ringkas:**\n",
        "Program ini membaca file JSON yang sudah diekstrak ke pandas DataFrame, melakukan data exploration (melihat distribusi rating, jumlah review verified vs unverified), dan membuat label biner untuk sentiment classification (4-5 stars = positif, 1-3 stars = negatif).\n",
        "\n",
        "**Langkah-Langkah:**\n",
        "1. Load JSON file dengan pandas\n",
        "2. Pilih kolom yang relevan\n",
        "3. Remove missing/empty reviews\n",
        "4. Filter hanya verified reviews (biar lebih reliable)\n",
        "5. Periksa class distribution (data imbalance)\n",
        "6. Convert rating 5-star menjadi binary labels\n",
        "7. Shuffle data untuk training yang lebih baik\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# ===== STEP 1: Load JSON file ke DataFrame =====\n",
        "# pd.read_json() membaca JSON file\n",
        "# lines=True: setiap baris adalah satu JSON object (JSONL format)\n",
        "# orient='records': mengkonversi JSON objects menjadi row records\n",
        "review_df = pd.read_json(\n",
        "    os.path.join('data', 'Video_Games_5.json'),\n",
        "    lines=True,  # JSONL format (JSON Lines)\n",
        "    orient='records'\n",
        ")\n",
        "\n",
        "# ===== STEP 2: Pilih kolom yang relevan =====\n",
        "# Dataset punya banyak kolom (reviewerID, asin, verified, overall, reviewText, dll)\n",
        "# Kita hanya butuh: rating (overall), verified status, dan review text\n",
        "review_df = review_df[[\"overall\", \"verified\", \"reviewTime\", \"reviewText\"]]\n",
        "\n",
        "print(\"Data shape:\", review_df.shape)  # Berapa banyak rows dan columns\n",
        "print(review_df.head())  # Tampilkan 5 rows pertama\n",
        "\n",
        "# ===== STEP 3: Remove empty atau null reviews =====\n",
        "# Beberapa review mungkin kosong atau cuma whitespace\n",
        "# isna() return True jika value adalah null/NaN\n",
        "# ~(tilde) adalah NOT operator\n",
        "review_df = review_df[~review_df[\"reviewText\"].isna()]  # Remove null\n",
        "\n",
        "# .str accessor mengakses string methods\n",
        "# .strip() remove leading/trailing whitespace\n",
        "# .str.len() > 0 filter hanya reviews dengan content\n",
        "review_df = review_df[review_df[\"reviewText\"].str.strip().str.len()>0]\n",
        "\n",
        "print(\"After removing empty:\", review_df.shape)\n",
        "\n",
        "# ===== STEP 4: Filter verified reviews only =====\n",
        "# Verified reviews = dari pembeli asli yang terbukti beli produk\n",
        "# Lebih reliable daripada review dari sembarang orang\n",
        "# .loc[condition, :] = filter rows based on condition\n",
        "verified_df = review_df.loc[review_df[\"verified\"], :]\n",
        "\n",
        "print(\"Verified reviews count:\", len(verified_df))\n",
        "\n",
        "# ===== STEP 5: Check class distribution =====\n",
        "# Lihat berapa banyak review untuk setiap rating\n",
        "# PENTING: untuk mendeteksi class imbalance problem\n",
        "print(\"\\nRating distribution:\")\n",
        "print(verified_df[\"overall\"].value_counts())\n",
        "\n",
        "# Output menunjukkan:\n",
        "# 5 stars: 222,335 reviews (paling banyak)\n",
        "# 4 stars: 54,878 reviews\n",
        "# 3 stars: 27,973 reviews\n",
        "# 1 star:  15,200 reviews\n",
        "# 2 stars: 12,118 reviews\n",
        "# IMBALANCED! 5-star jauh lebih banyak dari yang lain\n",
        "\n",
        "# ===== STEP 6: Create binary labels =====\n",
        "# Sentiment classification = 2 classes\n",
        "# Assumption: 4-5 stars = POSITIF (label=1), 1-3 stars = NEGATIF (label=0)\n",
        "# .map() fungsi pandas untuk transform values berdasarkan dictionary\n",
        "verified_df[\"label\"] = verified_df[\"overall\"].map({\n",
        "    5: 1,  # 5 stars -> positive\n",
        "    4: 1,  # 4 stars -> positive\n",
        "    3: 0,  # 3 stars -> negative\n",
        "    2: 0,  # 2 stars -> negative\n",
        "    1: 0   # 1 star -> negative\n",
        "})\n",
        "\n",
        "print(\"\\nLabel distribution after mapping:\")\n",
        "print(verified_df[\"label\"].value_counts())\n",
        "# Output: 277,213 positive (label=1), 55,291 negative (label=0)\n",
        "# Still imbalanced! 83% positive, 17% negative\n",
        "\n",
        "# ===== STEP 7: Shuffle data =====\n",
        "# PENTING: untuk menghindari bias karena data mungkin sorted by rating\n",
        "# .sample(frac=1.0) = sample 100% dari data (shuffle semua)\n",
        "# random_state=42 = fixed seed untuk reproducibility\n",
        "verified_df = verified_df.sample(frac=1.0, random_state=42)\n",
        "\n",
        "print(\"\\n✓ Data shuffled\")\n",
        "\n",
        "# ===== STEP 8: Separate inputs dan labels =====\n",
        "# Siapkan untuk preprocessing step berikutnya\n",
        "inputs, labels = verified_df[\"reviewText\"], verified_df[\"label\"]\n",
        "\n",
        "print(f\"\\nTotal samples: {len(inputs)}\")\n",
        "print(f\"Positive: {(labels==1).sum()}, Negative: {(labels==0).sum()}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Download NLTK Resources\n",
        "\n",
        "**Penjelasan Ringkas:**\n",
        "NLTK (Natural Language Toolkit) memerlukan beberapa resource tambahan untuk preprocessing text seperti:\n",
        "- **stopwords**: daftar kata-kata umum yang tidak informatif\n",
        "- **wordnet**: database untuk lemmatization (mencari base form kata)\n",
        "- **punkt**: tokenizer untuk memecah text menjadi kata-kata\n",
        "- **pos_tag**: untuk mengenali jenis kata (noun, verb, dll)\n",
        "\n",
        "Program ini download semua resources yang diperlukan.\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "\n",
        "# ===== Download NLTK resources =====\n",
        "# NLTK library comes with many tools, tapi resources (data) perlu di-download terpisah\n",
        "\n",
        "# averaged_perceptron_tagger: model untuk Part-of-Speech tagging\n",
        "# Digunakan untuk mengidentifikasi apakah kata adalah noun, verb, adjective, dll\n",
        "nltk.download('averaged_perceptron_tagger', download_dir='nltk')\n",
        "\n",
        "# wordnet: lexical database untuk lemmatization\n",
        "# Contain base forms dari kata-kata (contoh: \"running\" -> \"run\")\n",
        "nltk.download('wordnet', download_dir='nltk')\n",
        "\n",
        "# omw-1.4: Open Multilingual Wordnet untuk berbagai bahasa\n",
        "nltk.download('omw-1.4', download_dir='nltk')\n",
        "\n",
        "# stopwords: daftar kata-kata umum per bahasa\n",
        "# Untuk English, termasuk: \"the\", \"a\", \"and\", \"is\", \"it\", dll\n",
        "nltk.download('stopwords', download_dir='nltk')\n",
        "\n",
        "# punkt: sentence dan word tokenizer\n",
        "# Digunakan untuk memecah teks menjadi sentences/words\n",
        "nltk.download('punkt', download_dir='nltk')\n",
        "\n",
        "# Tambahkan path local ke NLTK data path\n",
        "# Agar NLTK mencari resources di folder lokal juga\n",
        "nltk.data.path.append(os.path.abspath('nltk'))\n",
        "\n",
        "print(\"✓ All NLTK resources downloaded\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Text Cleaning Function\n",
        "\n",
        "**Penjelasan Ringkas:**\n",
        "Ini adalah **core function** untuk membersihkan text sebelum masuk ke model. Function melakukan:\n",
        "1. Lowercase semua kata\n",
        "2. Expand contractions (aren't → are not)\n",
        "3. Hapus digits\n",
        "4. Tokenization (split ke words)\n",
        "5. Hapus stop words (tapi PERTAHANKAN \"not\"/\"no\" karena penting untuk sentiment!)\n",
        "6. Lemmatization (convert ke base form dengan mempertimbangkan part-of-speech)\n",
        "\n",
        "Output: list of clean words yang siap untuk embedding/encoding.\n",
        "\n",
        "```python\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# ===== Setup preprocessing components =====\n",
        "# Get English stopwords tapi EXCLUDE 'not' dan 'no' (penting untuk sentiment!)\n",
        "# \"This is NOT a great game\" vs \"This is a great game\" = completely different meaning\n",
        "EN_STOPWORDS = set(stopwords.words('english')) - {'not', 'no'}\n",
        "\n",
        "# WordNetLemmatizer: mengubah kata ke base form\n",
        "# Contoh: \"running\" -> \"run\", \"better\" -> \"good\", \"players\" -> \"player\"\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(doc):\n",
        "    \"\"\"\n",
        "    Clean dan preprocess satu review text\n",
        "    Input: doc = raw text string (review)\n",
        "    Output: list of cleaned words\n",
        "    \n",
        "    Process:\n",
        "    1. Lowercase\n",
        "    2. Expand contractions (isn't -> is not)\n",
        "    3. Remove shortened forms ('ll, 're, 'd, 've)\n",
        "    4. Remove digits (1, 2, 3, dll)\n",
        "    5. Tokenize dan remove stop words\n",
        "    6. Lemmatize (convert to base form)\n",
        "    \"\"\"\n",
        "    \n",
        "    # ===== STEP 1: Lowercase =====\n",
        "    # Mengubah semua character menjadi huruf kecil\n",
        "    # Agar \"Game\" dan \"game\" diperlakukan sebagai kata yang sama\n",
        "    doc = doc.lower()\n",
        "    \n",
        "    # ===== STEP 2: Expand contractions =====\n",
        "    # \"can't\" -> \"can not\", \"don't\" -> \"do not\", dll\n",
        "    # Penting untuk mempertahankan semantic meaning\n",
        "    # Menggunakan string.replace() untuk simple replacement\n",
        "    doc = doc.replace(\"n't \", ' not ')\n",
        "    \n",
        "    # ===== STEP 3: Remove shortened forms =====\n",
        "    # \"won't\" -> \"wo not\" (akan dihapus \"wo\" di step stop word removal)\n",
        "    # \"I'll\" -> \"I\", \"they're\" -> \"they\", dll\n",
        "    # Menggunakan regex (regular expression) untuk pattern matching\n",
        "    # Pattern: \\\\'ll (escaped quote) atau \\\\'re atau \\\\'d atau \\\\'ve\n",
        "    doc = re.sub(r\"(?:\\\\'ll |\\\\'re |\\\\'d |\\\\'ve )\", \" \", doc)\n",
        "    \n",
        "    # ===== STEP 4: Remove digits =====\n",
        "    # Hapus semua angka (0-9)\n",
        "    # Regex pattern: \\d+ (satu atau lebih digit)\n",
        "    # Replace dengan empty string \"\"\n",
        "    doc = re.sub(r\"\\d+\", \"\", doc)\n",
        "    \n",
        "    # ===== STEP 5: Tokenization dan remove stop words =====\n",
        "    # word_tokenize(doc) memecah text menjadi list of words\n",
        "    # List comprehension: filter hanya words yang BUKAN stop words dan BUKAN punctuation\n",
        "    # string.punctuation = \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
        "    tokens = [\n",
        "        w for w in word_tokenize(doc)\n",
        "        if w not in EN_STOPWORDS and w not in string.punctuation\n",
        "    ]\n",
        "    \n",
        "    # ===== STEP 6: Lemmatization =====\n",
        "    # Pertama, get Part-of-Speech tag untuk setiap word\n",
        "    # nltk.pos_tag() return list of (word, POS_tag) tuples\n",
        "    # Contoh: [('running', 'VBG'), ('faster', 'RBR'), ('than', 'IN')]\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    \n",
        "    # Lemmatize hanya NOUNS dan VERBS (karena paling berubah-ubah)\n",
        "    # Adjectives, adverbs, dll di-skip untuk menghemat computation\n",
        "    clean_text_result = [\n",
        "        # lemmatizer.lemmatize(word, pos=POS_tag)\n",
        "        # pos parameter penting! lemmatization logic berbeda untuk noun vs verb\n",
        "        lemmatizer.lemmatize(w, pos=p[0].lower())\n",
        "        if p[0]=='N' or p[0]=='V'  # N = Noun, V = Verb (POS tag dari Penn Treebank)\n",
        "        else w  # Jika bukan noun/verb, keep original word\n",
        "        for (w, p) in pos_tags\n",
        "    ]\n",
        "    \n",
        "    return clean_text_result\n",
        "\n",
        "# ===== Example usage =====\n",
        "sample_doc = \"She sells seashells by the seashore. I can't believe it's not butter!\"\n",
        "print(\"Before cleaning:\", sample_doc)\n",
        "print(\"After cleaning:\", clean_text(sample_doc))\n",
        "# Output: ['sell', 'seashell', 'seashore', 'not', 'believe', 'butter']\n",
        "\n",
        "# ===== STEP 7: Apply to entire dataset =====\n",
        "# Gunakan pandas .apply() untuk apply function ke setiap row\n",
        "# lambda x: clean_text(x) = anonymous function yang call clean_text untuk setiap review\n",
        "# ⚠️ WARNING: Ini sangat lambat! Bisa ambil 30-60 menit tergantung CPU\n",
        "# Improvement: gunakan parallel processing dengan joblib atau multiprocessing\n",
        "print(\"\\nCleaning all reviews (ini akan lambat, tunggu ~1 jam)...\")\n",
        "inputs = inputs.apply(lambda x: clean_text(x))\n",
        "\n",
        "# ===== STEP 8: Save untuk future use =====\n",
        "# Menyimpan ke pickle file untuk menghindari re-processing\n",
        "# Pickle = Python's native serialization format\n",
        "inputs.to_pickle(os.path.join('data','sentiment_inputs.pkl'))\n",
        "labels.to_pickle(os.path.join('data','sentiment_labels.pkl'))\n",
        "\n",
        "print(\"✓ Data cleaned dan saved\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Train/Validation/Test Split Function\n",
        "\n",
        "**Penjelasan Ringkas:**\n",
        "Ini adalah **CRITICAL** function untuk proper data handling. Karena dataset imbalanced (83% positif, 17% negatif), kita perlu:\n",
        "1. Split train/val/test SEBELUM melakukan analysis (vocab, sequence length, dll)\n",
        "2. Buat validation dan test set BALANCED (50% pos, 50% neg)\n",
        "3. Sisa data untuk training (masih imbalanced, tapi itu OK)\n",
        "4. Ini mencegah **DATA LEAKAGE** - masalah di mana validation/test data bocor ke training\n",
        "\n",
        "**Mengapa penting:** Jika kita compute vocab size dari semua data (termasuk val/test), maka model akan \"tahu\" tentang val/test data, dan performance metrics jadi tidak akurat.\n",
        "\n",
        "```python\n",
        "def train_valid_test_split(inputs, labels, train_fraction=0.8, random_seed=42):\n",
        "    \"\"\"\n",
        "    Split data menjadi train/validation/test dengan strategi balanced\n",
        "    \n",
        "    PENTING: Data dengan 83% positif, 17% negatif (IMBALANCED)\n",
        "    Strategi:\n",
        "    1. Pisahkan positive dan negative indices\n",
        "    2. Sample balanced val dan test set dari masing-masing class\n",
        "    3. Sisa untuk training (akan lebih imbalanced, tapi OK)\n",
        "    4. Ini ensure val/test set balanced untuk fair evaluation\n",
        "    \n",
        "    Args:\n",
        "        inputs: Series of cleaned texts (list of words per review)\n",
        "        labels: Series of binary labels (0 or 1)\n",
        "        train_fraction: berapa % untuk training (default 0.8 = 80%)\n",
        "        random_seed: seed untuk reproducibility\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of ((tr_x, tr_y), (v_x, v_y), (ts_x, ts_y))\n",
        "    \"\"\"\n",
        "    \n",
        "    # ===== STEP 1: Separate positive dan negative indices =====\n",
        "    # Perlu handle positif dan negatif secara terpisah karena imbalanced\n",
        "    # labels.loc[(labels==0)].index = indices dari semua negative examples\n",
        "    neg_indices = pd.Series(labels.loc[(labels==0)].index)  \n",
        "    pos_indices = pd.Series(labels.loc[(labels==1)].index)  \n",
        "    \n",
        "    print(f\"Total negative: {len(neg_indices)}, Total positive: {len(pos_indices)}\")\n",
        "    \n",
        "    # ===== STEP 2: Calculate validation dan test size =====\n",
        "    # Formula: n_valid = min(neg_count, pos_count) × ((1-train_frac)/2)\n",
        "    # Menggunakan min(neg, pos) agar seimbang dari class yang lebih sedikit\n",
        "    # ((1-train_frac)/2) karena 1-train_frac untuk val+test, dibagi 2 untuk val dan test\n",
        "    # Contoh: train=0.8 -> (1-0.8)/2 = 0.1 = 10% untuk val, 10% untuk test\n",
        "    n_valid = int(min([len(neg_indices), len(pos_indices)])\n",
        "       * ((1-train_fraction)/2.0))\n",
        "    n_test = n_valid  # Validation dan test size sama\n",
        "    \n",
        "    print(f\"Validation size: {n_valid}, Test size: {n_test}\")\n",
        "    \n",
        "    # ===== STEP 3: Sample negative indices =====\n",
        "    # Random sample untuk test set\n",
        "    neg_test_inds = neg_indices.sample(n=n_test, random_state=random_seed)\n",
        "    \n",
        "    # Random sample untuk validation set dari negative yang belum diambil\n",
        "    # ~neg_indices.isin(neg_test_inds) = filter indices yang TIDAK di test set\n",
        "    neg_valid_inds = neg_indices.loc[~neg_indices.isin(\n",
        "        neg_test_inds)].sample(n=n_test, random_state=random_seed)\n",
        "    \n",
        "    # Sisa negative indices untuk training\n",
        "    neg_train_inds = neg_indices.loc[~neg_indices.isin(\n",
        "        neg_test_inds.tolist()+neg_valid_inds.tolist())]\n",
        "    \n",
        "    # ===== STEP 4: Sample positive indices (sama seperti negative) =====\n",
        "    pos_test_inds = pos_indices.sample(n=n_test, random_state=random_seed)\n",
        "    pos_valid_inds = pos_indices.loc[~pos_indices.isin(\n",
        "        pos_test_inds)].sample(n=n_test, random_state=random_seed)\n",
        "    pos_train_inds = pos_indices.loc[~pos_indices.isin(\n",
        "        pos_test_inds.tolist()+pos_valid_inds.tolist())]\n",
        "    \n",
        "    # ===== STEP 5: Create actual datasets =====\n",
        "    # Combine negative dan positive indices untuk setiap set\n",
        "    # .sample(frac=1.0) = shuffle data sebelum return\n",
        "    \n",
        "    tr_x = inputs.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(\n",
        "        frac=1.0, random_state=random_seed)\n",
        "    tr_y = labels.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(\n",
        "        frac=1.0, random_state=random_seed)\n",
        "    \n",
        "    v_x = inputs.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(\n",
        "        frac=1.0, random_state=random_seed)\n",
        "    v_y = labels.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(\n",
        "        frac=1.0, random_state=random_seed)\n",
        "    \n",
        "    ts_x = inputs.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(\n",
        "        frac=1.0, random_state=random_seed)\n",
        "    ts_y = labels.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(\n",
        "        frac=1.0, random_state=random_seed)\n",
        "    \n",
        "    # ===== STEP 6: Verify splits =====\n",
        "    print(f'\\n✓ Training data: {len(tr_x)} samples')\n",
        "    print(f'  Positive: {(tr_y==1).sum()}, Negative: {(tr_y==0).sum()}')\n",
        "    print(f'✓ Validation data: {len(v_x)} samples')\n",
        "    print(f'  Positive: {(v_y==1).sum()}, Negative: {(v_y==0).sum()}')\n",
        "    print(f'✓ Test data: {len(ts_x)} samples')\n",
        "    print(f'  Positive: {(ts_y==1).sum()}, Negative: {(ts_y==0).sum()}')\n",
        "    \n",
        "    return (tr_x, tr_y), (v_x, v_y), (ts_x, ts_y)\n",
        "\n",
        "# ===== Usage =====\n",
        "(tr_x, tr_y), (v_x, v_y), (ts_x, ts_y) = train_valid_test_split(\n",
        "    inputs, labels, train_fraction=0.8, random_seed=42\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Analyze Vocabulary\n",
        "\n",
        "**Penjelasan Ringkas:**\n",
        "Program ini menganalisis vocabulary dari **training set SAJA** (tidak val/test set!). Tujuannya:\n",
        "1. Lihat berapa banyak unique words\n",
        "2. Lihat distribusi frequency (apakah ada words yang jarang muncul?)\n",
        "3. Tentukan **vocabulary size hyperparameter**: berapa banyak top words yang akan digunakan di model\n",
        "\n",
        "Insight: Menyimpan semua 133K unique words terlalu boros. Cukup ambil top 11,800 words yang muncul ≥25 kali. Words lain di-treat sebagai \"unknown\" token.\n",
        "\n",
        "```python\n",
        "from collections import Counter\n",
        "\n",
        "# ===== STEP 1: Create word frequency dictionary =====\n",
        "# tr_x adalah list of list of words (setelah cleaning)\n",
        "# [['great', 'game'], ['love', 'it'], ['amazing', 'product'], ...]\n",
        "#\n",
        "# Flatten semua words dari semua reviews menjadi satu list\n",
        "# List comprehension: iterate setiap doc, iterate setiap word dalam doc\n",
        "data_list = [w for doc in tr_x for w in doc]\n",
        "\n",
        "# Counter = special dict yang count frequency\n",
        "# Hasilnya: {'game': 5234, 'great': 3421, 'love': 2987, ...}\n",
        "cnt = Counter(data_list)\n",
        "\n",
        "print(f\"Total unique words: {len(cnt)}\")\n",
        "print(f\"Total word occurrences: {len(data_list)}\")\n",
        "\n",
        "# ===== STEP 2: Create frequency dataframe =====\n",
        "# Ubah counter menjadi pandas Series untuk easier manipulation\n",
        "freq_df = pd.Series(\n",
        "    list(cnt.values()),\n",
        "    index=list(cnt.keys())\n",
        ").sort_values(ascending=False)  # Sort by frequency descending\n",
        "\n",
        "# ===== STEP 3: Inspect top words =====\n",
        "print(\"\\nTop 20 most frequent words:\")\n",
        "print(freq_df.head(n=20))\n",
        "# Expected: game, great, love, fun, play, etc.\n",
        "\n",
        "# ===== STEP 4: Get summary statistics =====\n",
        "print(\"\\nFrequency statistics:\")\n",
        "print(freq_df.describe())\n",
        "# Output:\n",
        "# count  133714 (total unique words)\n",
        "# mean   ~76 (rata-rata frequency)\n",
        "# std    ~1754 (standard deviation)\n",
        "# min    1 (some words appear only once)\n",
        "# max    5234 (most common word appears 5234 times)\n",
        "\n",
        "# ===== STEP 5: Determine vocabulary size =====\n",
        "# DECISION: Gunakan hanya words yang muncul ≥25 kali\n",
        "# Alasan:\n",
        "# - Words yang rare (muncul 1-2 kali) tidak informatif, cuma menambah noise\n",
        "# - Mengurangi vocabulary size -> lebih kecil embedding matrix -> faster training\n",
        "# - Hasil: ~11,800 words dipilih dari 133,714 unique words\n",
        "n_vocab = (freq_df >= 25).sum()\n",
        "\n",
        "print(f\"\\n✓ Vocabulary size (freq >= 25): {n_vocab}\")\n",
        "print(f\"✓ Percentage of all words: {n_vocab/len(freq_df)*100:.2f}%\")\n",
        "\n",
        "# ===== STEP 6: Visualize distribution =====\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot histogram dari word frequencies\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Subplot 1: Frequency distribution (semua words)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(freq_df.values, bins=100, edgecolor='black')\n",
        "plt.xlabel('Word Frequency')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Word Frequency Distribution (All Words)')\n",
        "plt.xscale('log')  # Log scale karena distribution sangat skewed\n",
        "\n",
        "# Subplot 2: Vocabulary size vs frequency threshold\n",
        "plt.subplot(1, 2, 2)\n",
        "freq_thresholds = range(1, 100)\n",
        "vocab_sizes = [(freq_df >= t).sum() for t in freq_thresholds]\n",
        "plt.plot(freq_thresholds, vocab_sizes)\n",
        "plt.xlabel('Minimum Frequency Threshold')\n",
        "plt.ylabel('Vocabulary Size')\n",
        "plt.title('Vocabulary Size vs Frequency Threshold')\n",
        "plt.axvline(x=25, color='red', linestyle='--', label='Chosen threshold (25)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Analyze Sequence Length\n",
        "\n",
        "**Penjelasan Ringkas:**\n",
        "Program ini menganalisis **panjang setiap review** (jumlah words setelah cleaning). Ini penting karena:\n",
        "1. LSTM bisa handle variable-length sequences, TAPI tidak efisien jika panjang sangat berbeda-beda\n",
        "2. Dengan bucketing (grouping reviews dengan panjang similar), kita bisa reduce padding waste dan improve efficiency\n",
        "3. Dari analysis, kita tentukan bucket boundaries untuk grouping reviews\n",
        "\n",
        "Insight: Kebanyakan reviews 5-16 kata. Ada beberapa outliers dengan 50+ kata. Gunakan bucket boundaries [5, 15] untuk membagi menjadi 3 groups.\n",
        "\n",
        "```python\n",
        "# ===== STEP 1: Get sequence length untuk setiap review =====\n",
        "# tr_x adalah pandas Series of lists\n",
        "# .str.len() tidak bisa langsung untuk list, jadi kita harus extract length dulu\n",
        "seq_length_ser = tr_x.apply(lambda x: len(x))  # length of each review\n",
        "\n",
        "print(f\"Sample sequence lengths: {seq_length_ser.head(10).tolist()}\")\n",
        "\n",
        "# ===== STEP 2: Get percentiles =====\n",
        "# Percentile = nilai di mana X% dari data lebih kecil\n",
        "# 10th percentile = 10% data lebih kecil, 90% lebih besar\n",
        "# 90th percentile = 90% data lebih kecil, 10% lebih besar\n",
        "p_10 = seq_length_ser.quantile(0.1)   # 10th percentile\n",
        "p_25 = seq_length_ser.quantile(0.25)  # 25th percentile\n",
        "p_50 = seq_length_ser.quantile(0.50)  # median (50th percentile)\n",
        "p_75 = seq_length_ser.quantile(0.75)  # 75th percentile\n",
        "p_90 = seq_length_ser.quantile(0.90)  # 90th percentile\n",
        "\n",
        "print(f\"\\nSequence length percentiles:\")\n",
        "print(f\"  10th: {p_10:.0f} words\")\n",
        "print(f\"  25th: {p_25:.0f} words\")\n",
        "print(f\"  50th (median): {p_50:.0f} words\")\n",
        "print(f\"  75th: {p_75:.0f} words\")\n",
        "print(f\"  90th: {p_90:.0f} words\")\n",
        "\n",
        "# Expected output (dari Bab):\n",
        "# 10th: 1, 25th: 3, 50th: 10, 75th: 16, 90th: 74\n",
        "\n",
        "# ===== STEP 3: Get summary statistics (excluding outliers) =====\n",
        "# Untuk better summary, hapus extreme outliers\n",
        "# Gunakan 10th-90th percentile range\n",
        "filtered_lengths = seq_length_ser[\n",
        "    (seq_length_ser >= p_10) & (seq_length_ser < p_90)\n",
        "]\n",
        "\n",
        "print(f\"\\nDetailed statistics (10th-90th percentile):\")\n",
        "print(filtered_lengths.describe(percentiles=[0.33, 0.66]))\n",
        "\n",
        "# Output akan show:\n",
        "# 33% of reviews <= 5 words (SHORT)\n",
        "# 66% of reviews <= 16 words (MEDIUM)\n",
        "# 90% of reviews <= 74 words (LONG)\n",
        "\n",
        "# ===== STEP 4: Determine bucketing strategy =====\n",
        "# Bucketing = group reviews dengan similar length untuk efficient batching\n",
        "# Bucket boundaries = split points\n",
        "\n",
        "# Strategy: Gunakan percentiles sebagai bucket boundaries\n",
        "# bucket_boundaries = [p_33, p_66] = [5, 16]\n",
        "# Artinya:\n",
        "# Bucket 1: 0-5 words (short reviews) - batch_size=128\n",
        "# Bucket 2: 5-15 words (medium reviews) - batch_size=128  \n",
        "# Bucket 3: 15+ words (long reviews) - batch_size=128\n",
        "\n",
        "bucket_boundaries = [5, 15]\n",
        "\n",
        "print(f\"\\n✓ Bucket boundaries selected: {bucket_boundaries}\")\n",
        "print(\"  - Bucket 1 (0-5 words): batch_size=128\")\n",
        "print(\"  - Bucket 2 (5-15 words): batch_size=128\")\n",
        "print(\"  - Bucket 3 (15+ words): batch_size=128\")\n",
        "\n",
        "# ===== STEP 5: Visualize sequence length distribution =====\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Subplot 1: Histogram\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(seq_length_ser, bins=100, edgecolor='black')\n",
        "plt.xlabel('Sequence Length (number of words)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Review Lengths')\n",
        "plt.axvline(x=p_50, color='red', linestyle='--', label=f'Median: {p_50:.0f}')\n",
        "plt.legend()\n",
        "\n",
        "# Subplot 2: CDF (Cumulative Distribution Function)\n",
        "plt.subplot(1, 2, 2)\n",
        "sorted_lengths = np.sort(seq_length_ser)\n",
        "cdf = np.arange(1, len(sorted_lengths)+1) / len(sorted_lengths)\n",
        "plt.plot(sorted_lengths, cdf)\n",
        "plt.axvline(x=5, color='red', linestyle='--', alpha=0.5, label='Bucket 1-2 boundary')\n",
        "plt.axvline(x=15, color='orange', linestyle='--', alpha=0.5, label='Bucket 2-3 boundary')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Cumulative Probability')\n",
        "plt.title('CDF of Review Lengths')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Keras Tokenizer\n",
        "\n",
        "**Penjelasan Ringkas:**\n",
        "Keras Tokenizer mengkonversi **text (words) menjadi numbers (IDs)** karena neural networks hanya bisa memproses numbers, bukan strings. Tokenizer:\n",
        "1. Fits pada training data untuk build word-to-ID mapping dictionary\n",
        "2. Menggunakan mapping ini untuk convert semua reviews (train/val/test) menjadi sequences of IDs\n",
        "3. Unknown words (tidak di vocab) di-assign ID khusus (OOV = Out Of Vocabulary)\n",
        "\n",
        "Contoh:\n",
        "```\n",
        "Text: ['great', 'game', 'love', 'it']\n",
        "Sequence IDs: [14, 2, 157, 96]\n",
        "```\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# ===== STEP 1: Create Tokenizer object =====\n",
        "# Tokenizer adalah dict-like object yang store word-to-ID mapping\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=n_vocab,  # Hanya gunakan top 11,800 words (dari vocab analysis)\n",
        "    oov_token='unk',    # OOV = Out Of Vocabulary, untuk unknown words\n",
        "    lower=False,        # Already lowercase di preprocessing, jadi set ke False\n",
        "    # filters = characters yang akan di-remove saat tokenization\n",
        "    # Kita sudah remove ini di cleaning, tapi keras tokenizer punya default\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    split=' ',          # Split by space (sudah dilakukan di preprocessing)\n",
        "    char_level=False    # False = word level (bukan character level)\n",
        ")\n",
        "\n",
        "# ===== STEP 2: Fit tokenizer on training data =====\n",
        "# PENTING: Fit hanya pada TRAINING SET!\n",
        "# Jangan fit pada validation/test set (data leakage)\n",
        "# .fit_on_texts() melihat semua words di training data dan build vocab dictionary\n",
        "tokenizer.fit_on_texts(tr_x.tolist())\n",
        "\n",
        "print(\"✓ Tokenizer fitted on training data\")\n",
        "\n",
        "# ===== STEP 3: Inspect word-to-ID mapping =====\n",
        "# tokenizer.word_index = dictionary {word: ID}\n",
        "# Contoh struktur:\n",
        "# {'game': 2, 'great': 3, 'love': 4, 'fun': 5, ...}\n",
        "# Note: ID 1 reserved untuk padding, ID 0 reserved untuk masking\n",
        "\n",
        "print(f\"\\nWord-to-ID mapping examples:\")\n",
        "print(f\"  'game' -> ID {tokenizer.word_index.get('game', 'unknown')}\")\n",
        "print(f\"  'great' -> ID {tokenizer.word_index.get('great', 'unknown')}\")\n",
        "print(f\"  'play' -> ID {tokenizer.word_index.get('play', 'unknown')}\")\n",
        "\n",
        "# tokenizer.index_word = reverse mapping {ID: word}\n",
        "# Untuk debugging/visualization\n",
        "print(f\"\\nID-to-word mapping examples:\")\n",
        "print(f\"  ID 2 -> '{tokenizer.index_word.get(2, 'unknown')}'\")\n",
        "print(f\"  ID 3 -> '{tokenizer.index_word.get(3, 'unknown')}'\")\n",
        "\n",
        "# ===== STEP 4: Convert texts to sequences =====\n",
        "# texts_to_sequences() menggunakan mapping untuk convert words to IDs\n",
        "# Input: list of list of words\n",
        "# Output: list of list of IDs\n",
        "\n",
        "print(\"\\nConverting texts to sequences...\")\n",
        "\n",
        "tr_x_seq = tokenizer.texts_to_sequences(tr_x.tolist())\n",
        "v_x_seq = tokenizer.texts_to_sequences(v_x.tolist())\n",
        "ts_x_seq = tokenizer.texts_to_sequences(ts_x.tolist())\n",
        "\n",
        "# ===== STEP 5: Inspect converted sequences =====\n",
        "print(f\"\\nExample original text: {tr_x.iloc[0]}\")\n",
        "print(f\"Example converted sequence: {tr_x_seq[0]}\")\n",
        "\n",
        "# ===== STEP 6: Handle unknown words =====\n",
        "# OOV token 'unk' akan di-assign ID sebagai:\n",
        "# oov_token_id = num_words + 1 = 11,801\n",
        "# Jadi words yang tidak di vocabulary akan punya ID 11,801\n",
        "# Test: review dengan unknown word\n",
        "test_review = ['unknown_word', 'great', 'game']\n",
        "test_seq = tokenizer.texts_to_sequences([test_review])\n",
        "print(f\"\\nTest with unknown word:\")\n",
        "print(f\"  Text: {test_review}\")\n",
        "print(f\"  Sequence: {test_seq}\")\n",
        "# Output: [[11801, <great_id>, <game_id>]]\n",
        "# (unknown_word -> 11801, great -> standard_id, game -> standard_id)\n",
        "\n",
        "print(\"✓ Text-to-sequence conversion complete\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Kesimpulan Tahap Preprocessing\n",
        "\n",
        "Setelah semua program di atas selesai, kita punya:\n",
        "\n",
        "1. **Clean text data** - dengan lemmatization, stop word removal, dll\n",
        "2. **Proper train/val/test split** - balanced val/test untuk fair evaluation\n",
        "3. **Vocabulary** - top 11,800 words yang muncul ≥25 kali\n",
        "4. **Sequence length analysis** - untuk bucketing strategy\n",
        "5. **Tokenized sequences** - numbers siap untuk neural network\n",
        "\n",
        "Next: TensorFlow data pipeline dengan bucketing, LSTM models, dan word embeddings...\n"
      ],
      "metadata": {
        "id": "QnjDCRCrMW6B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VUX2nMI2McGs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}