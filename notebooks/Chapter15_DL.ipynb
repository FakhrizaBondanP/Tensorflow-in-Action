{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BAB 15: TFX - MLOps dan Deployment Model dengan TensorFlow\n",
        "\n",
        "## Ringkasan\n",
        "\n",
        "Bab ini membahas **TFX (TensorFlow Extended)**, framework komprehensif untuk implementasi MLOps (Machine Learning Operations) dan productionization model machine learning. TFX menyediakan pipeline end-to-end yang mengotomatisasi seluruh workflow mulai dari ingestion data, transformasi features, training model, evaluasi, hingga deployment ke production environment. Bab mencakup definisi MLOps versus productionization, komponen-komponen TFX (CsvExampleGen, StatisticsGen, SchemaGen, Transform, Trainer), feature engineering dengan tensorflow_transform, anomaly detection dengan tensorflow_data_validation, model training dengan Keras API, SignatureDefs untuk serving, containerization dengan Docker, dan deployment via TensorFlow Serving untuk expose model sebagai REST API.\n",
        "\n",
        "---\n",
        "\n",
        "## Bagian 1: Menulis Data Pipeline dengan TFX\n",
        "\n",
        "### Konsep MLOps dan Productionization\n",
        "\n",
        "**MLOps (Machine Learning Operations)**: Workflow yang mengotomatisasi sebagian besar langkah dari pengumpulan data hingga delivery model terlatih, dengan minimal intervensi manusia. MLOps menggabungkan filosofi ML dan DevOps untuk meningkatkan velocity development dan operasi model.\n",
        "\n",
        "**Productionization**: Proses deployment trained model (di private server atau cloud) yang memungkinkan customer menggunakan model untuk tujuan yang dirancang dengan cara robust. Include design scalable API yang dapat handle ribuan request per detik.\n",
        "\n",
        "**Analogi**: MLOps adalah **perjalanan** (journey), productionization adalah **destinasi** (deployment final model).\n",
        "\n",
        "**Mengapa MLOps Penting**:\n",
        "- Untuk perusahaan besar (Google, Facebook, Amazon), ratusan atau ribuan model produce predictions setiap detik\n",
        "- Model tidak boleh menjadi stale, perlu continuous training/fine-tuning dengan new incoming data\n",
        "- MLOps dapat ingest data, train models, automatic evaluate, dan push ke production jika pass validation check\n",
        "- Validation check penting untuk safeguard terhadap rogue underperforming models\n",
        "\n",
        "**TFX (TensorFlow Extended)**: Library yang menyediakan semua komponen untuk implement machine learning pipeline yang ingest data, transform ke features, train model, dan push ke production environment.\n",
        "\n",
        "### Case Study: Prediksi Severity Kebakaran Hutan\n",
        "\n",
        "**Dataset**: Historical forest fires di Montesinho park, Portugal (tersedia di UCI ML Repository). Format CSV dengan 13 features:\n",
        "- **X, Y**: Spatial coordinates dalam park map\n",
        "- **month, day**: Bulan dan hari dalam minggu\n",
        "- **FFMC (Fine Fuel Moisture Code)**: Fuel moisture dari forest litter\n",
        "- **DMC (Duff Moisture Code)**: Numerical rating average moisture content soil\n",
        "- **DC (Drought Code)**: Depth dryness dalam soil\n",
        "- **ISI (Initial Spread Index)**: Expected rate fire spread\n",
        "- **temp**: Suhu (Celsius)\n",
        "- **RH**: Relative humidity (%)\n",
        "- **wind**: Kecepatan angin (km/h)\n",
        "- **rain**: Outside rain (mm/m²)\n",
        "- **area**: Burnt area forest (hectares) - **Target variable**\n",
        "\n",
        "**Task**: Regression problem untuk predict burnt area given semua features lainnya.\n",
        "\n",
        "### Setup Environment dan Download Data\n",
        "\n",
        "**Requirement Environment**:\n",
        "- **Operating System**: Linux (Ubuntu) highly recommended, TFX tidak tested untuk Windows\n",
        "- **TFX Version**: 1.6.0 (versi later memiliki bug di interactive environment)\n",
        "- **Docker**: Untuk containerization model serving\n",
        "- **Anaconda Environment**:\n",
        "  ```bash\n",
        "  conda create -n manning.tf2.tfx python=3.6\n",
        "  conda activate manning.tf2.tfx\n",
        "  pip install --use-deprecated=legacy-resolver -r requirements.txt\n",
        "  ```\n",
        "\n",
        "**Download Data**:\n",
        "```python\n",
        "import requests\n",
        "import os\n",
        "\n",
        "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv\"\n",
        "r = requests.get(url)\n",
        "os.makedirs(os.path.join('data', 'csv'), exist_ok=True)\n",
        "with open(os.path.join('data', 'csv', 'forestfires.csv'), 'wb') as f:\n",
        "    f.write(r.content)\n",
        "```\n",
        "\n",
        "**Data Splitting**:\n",
        "- 95% untuk training/validation\n",
        "- 5% untuk testing (dedicated test set tidak seen oleh model)\n",
        "\n",
        "### Komponen 1: CsvExampleGen\n",
        "\n",
        "**Purpose**: Membaca data dari CSV file, split ke train/eval, convert ke TFRecord format.\n",
        "\n",
        "**Implementation**:\n",
        "```python\n",
        "from tfx.components import CsvExampleGen\n",
        "\n",
        "example_gen = CsvExampleGen(input_base=os.path.join('data', 'csv', 'train'))\n",
        "context.run(example_gen)\n",
        "```\n",
        "\n",
        "**InteractiveContext**: Context untuk run various TFX steps, manage states between steps, maintain metadata store (SQLite database).\n",
        "\n",
        "**Metadata Store**: Database yang log informasi tentang inputs, outputs, execution-related outputs (component run identifier, errors). Immensely helpful untuk debugging complex TFX pipelines.\n",
        "\n",
        "**Output CsvExampleGen**:\n",
        "- **Directory Structure**:\n",
        "  ```\n",
        "  pipeline/examples/forest_fires_pipeline/CsvExampleGen/examples/1/\n",
        "    ├── Split-train/  (TFRecord files .gz)\n",
        "    └── Split-eval/   (TFRecord files .gz)\n",
        "  ```\n",
        "- **Split Method**: Hashing-based splitting. TFX menggunakan hash buckets (default 3 buckets: 2 untuk train, 1 untuk eval) untuk assign examples ke train/eval\n",
        "- **Hash Buckets**: TFX generate hash dari values dalam record, hash value determine bucket assignment. Contoh: hash 7 → bucket = 7 % 3 = 1\n",
        "\n",
        "**TFRecord Format**: Data stored sebagai byte streams. Efficient untuk TensorFlow pipeline, dapat retrieved sebagai tf.data.Dataset.\n",
        "\n",
        "**Inspecting TFRecord Data**:\n",
        "```python\n",
        "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n",
        "tfrecord_filenames = [os.path.join(train_uri, name) for name in os.listdir(train_uri)]\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "for tfrecord in dataset.take(2):\n",
        "    serialized_example = tfrecord.numpy()\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(serialized_example)\n",
        "    print(example)\n",
        "```\n",
        "\n",
        "**Data Structure dalam TFRecord**: tf.train.Example dengan collection features, setiap feature punya key (column name) dan value (float_list, int64_list, atau bytes_list).\n",
        "\n",
        "### Komponen 2: StatisticsGen\n",
        "\n",
        "**Purpose**: Generate basic statistics dan visualizations untuk Exploratory Data Analysis (EDA).\n",
        "\n",
        "**Implementation**:\n",
        "```python\n",
        "from tfx.components import StatisticsGen\n",
        "\n",
        "statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
        "context.run(statistics_gen)\n",
        "context.show(statistics_gen.outputs['statistics'])\n",
        "```\n",
        "\n",
        "**Output Visualizations**: Rich collection graphs untuk understand data quality dan distribution.\n",
        "\n",
        "**Key Information Displayed**:\n",
        "1. **Numerical Features Section**:\n",
        "   - Count, missing percentage\n",
        "   - Mean, standard deviation\n",
        "   - Min, max, median\n",
        "   - Histogram distribution (dapat sangat skewed, contoh: FFMC concentrated di 80-90 range)\n",
        "\n",
        "2. **Categorical Features Section**:\n",
        "   - Unique values count\n",
        "   - Missing percentage\n",
        "   - Mode (most frequent value) count\n",
        "   - Bar graph untuk features dengan sedikit unique values\n",
        "   - Line graph untuk features dengan banyak unique values (less cluttered)\n",
        "\n",
        "**Dashboard Controls**:\n",
        "- **Search bar**: Filter features by name\n",
        "- **Data type filter**: Show only numerical atau categorical\n",
        "- **Chart type**: Standard histogram atau quantile-based\n",
        "- **Sort order**: Berbagai sorting options\n",
        "\n",
        "### Komponen 3: SchemaGen\n",
        "\n",
        "**Purpose**: Automatically derive schema dari data. Schema = blueprint untuk data, expressing structure dan important attributes.\n",
        "\n",
        "**Database Schema Analogy**: Sama seperti database schema yang define table structure, TFX schema define data structure dan validation rules.\n",
        "\n",
        "**Implementation**:\n",
        "```python\n",
        "from tfx.components import SchemaGen\n",
        "\n",
        "schema_gen = SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False  # Important untuk downstream Transform step\n",
        ")\n",
        "context.run(schema_gen)\n",
        "context.show(schema_gen.outputs['schema'])\n",
        "```\n",
        "\n",
        "**infer_feature_shape Argument**:\n",
        "- **False**: Tensors passed ke Transform step sebagai tf.SparseTensor (more flexibility untuk feature manipulations)\n",
        "- **True**: Tensors sebagai tf.Tensor dengan known shape\n",
        "\n",
        "**Schema Components**:\n",
        "1. **Feature name**: Column identifier\n",
        "2. **Type**: INT, FLOAT, STRING, BOOLEAN\n",
        "3. **Presence**: required, optional\n",
        "4. **Valency**: single, repeated\n",
        "5. **Domain**: Constraints untuk feature values\n",
        "\n",
        "**Domain Types** (defined di schema.proto):\n",
        "- **Integer domain**: Define min/max untuk integer features\n",
        "- **Float domain**: Define min/max untuk float features\n",
        "- **String domain**: Define allowed values/tokens untuk string features\n",
        "- **Boolean domain**: Custom values untuk true/false states\n",
        "- **Struct domain**: Recursive domains atau domains dengan multiple features\n",
        "- **Natural language domain**: Define vocabulary untuk language features\n",
        "- **Image domain**: Restrict maximum byte size gambar\n",
        "- **Time domain**: Define date/time features\n",
        "- **Time of day domain**: Define time tanpa date\n",
        "\n",
        "**Protobuf Library**: Library untuk object serialization/deserialization developed by Google. Schema defined dengan .proto files, deserialization via functions seperti ParseFromString().\n",
        "\n",
        "### Komponen 4: Transform\n",
        "\n",
        "**Purpose**: Convert raw data columns ke model-ready features dengan various transformations.\n",
        "\n",
        "**Feature Types yang Dibuat**:\n",
        "1. **Dense floating-point features**: Values passed as-is dengan optional normalization (contoh: temperature, wind speed)\n",
        "2. **Bucketized features**: Numerical values binned ke predefined intervals (contoh: RH dibucketize ke low/medium/high)\n",
        "3. **Categorical features**: Values dari predefined set, converted ke integer indices menggunakan vocabulary (contoh: day, month)\n",
        "\n",
        "**Feature Assignment untuk Dataset**:\n",
        "- **Dense float features**: X, Y, wind, rain, FFMC, DMC, DC, ISI, temp (Z-score normalization)\n",
        "- **Bucketized features**: RH (buckets: [-inf, 33), [33, 66), [66, inf])\n",
        "- **Categorical features**: month (12 categories), day (7 categories)\n",
        "- **Label feature**: area (kept as numerical untuk regression)\n",
        "\n",
        "**Constants Definition** (forest_fires_constants.py):\n",
        "```python\n",
        "VOCAB_FEATURE_KEYS = ['day', 'month']\n",
        "MAX_CATEGORICAL_FEATURE_VALUES = [7, 12]\n",
        "DENSE_FLOAT_FEATURE_KEYS = ['DC', 'DMC', 'FFMC', 'ISI', 'rain', 'temp', 'wind', 'X', 'Y']\n",
        "BUCKET_FEATURE_KEYS = ['RH']\n",
        "BUCKET_FEATURE_BOUNDARIES = [(33, 66)]\n",
        "LABEL_KEY = 'area'\n",
        "\n",
        "def transformed_name(key):\n",
        "    return key + '_xf'  # Suffix untuk distinguish transformed features\n",
        "```\n",
        "\n",
        "**Preprocessing Function** (forest_fires_transform.py):\n",
        "```python\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "    outputs = {}\n",
        "    \n",
        "    # Dense features: Z-score normalization\n",
        "    for key in DENSE_FLOAT_FEATURE_KEYS:\n",
        "        outputs[transformed_name(key)] = tft.scale_to_z_score(\n",
        "            sparse_to_dense(inputs[key])\n",
        "        )\n",
        "    \n",
        "    # Vocabulary-based categorical: Build vocab, convert to integer ID\n",
        "    for key in VOCAB_FEATURE_KEYS:\n",
        "        outputs[transformed_name(key)] = tft.compute_and_apply_vocabulary(\n",
        "            sparse_to_dense(inputs[key]),\n",
        "            num_oov_buckets=1  # Assign unseen strings ke special category\n",
        "        )\n",
        "    \n",
        "    # Bucketized features: Apply bucket boundaries\n",
        "    for key, boundary in zip(BUCKET_FEATURE_KEYS, BUCKET_FEATURE_BOUNDARIES):\n",
        "        outputs[transformed_name(key)] = tft.apply_buckets(\n",
        "            sparse_to_dense(inputs[key]),\n",
        "            bucket_boundaries=[boundary]\n",
        "        )\n",
        "    \n",
        "    # Label: Keep as-is\n",
        "    outputs[transformed_name(LABEL_KEY)] = sparse_to_dense(inputs[LABEL_KEY])\n",
        "    \n",
        "    return outputs\n",
        "\n",
        "def sparse_to_dense(x):\n",
        "    return tf.squeeze(\n",
        "        tf.sparse.to_dense(\n",
        "            tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1])\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "```\n",
        "\n",
        "**tensorflow_transform Library**: Sub-library focused pada feature transformations. Menyediakan functions untuk:\n",
        "- Bucketizing features\n",
        "- Bag-of-words dari string column\n",
        "- Covariance matrices\n",
        "- Mean, std, min, max, count columns\n",
        "\n",
        "**Z-score Normalization Formula**:\n",
        "```\n",
        "z = (x - μ(x)) / σ(x)\n",
        "```\n",
        "Dimana μ(x) = mean value column, σ(x) = standard deviation column.\n",
        "\n",
        "**Transform Component Implementation**:\n",
        "```python\n",
        "from tfx.components import Transform\n",
        "\n",
        "transform = Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath('forest_fires_transform.py')\n",
        ")\n",
        "context.run(transform)\n",
        "```\n",
        "\n",
        "**Inspecting Transform Output**:\n",
        "- Transformed features disimpan sebagai TFRecord files dengan suffix `_xf`\n",
        "- Float features: Normalized values (contoh: DC_xf = 0.4196)\n",
        "- Categorical features: Integer indices (contoh: day_xf = 2)\n",
        "- Bucketized features: Bucket indices (contoh: RH_xf = 0)\n",
        "\n",
        "**Rule of Thumb**: Always check pipeline interim outputs whenever possible untuk sanity-check. TFX low visibility, not highly matured—probe components untuk verify inputs/outputs.\n",
        "\n",
        "---\n",
        "\n",
        "## Bagian 2: Training Regression Neural Network dengan TFX Trainer API\n",
        "\n",
        "### Keras Model Definition dengan Feature Columns\n",
        "\n",
        "**tf.feature_column**: Feature representation standard accepted by TensorFlow models. Handy untuk define data column-oriented fashion (each feature = column). Suitable untuk structured data.\n",
        "\n",
        "**Feature Column Types**:\n",
        "1. **tf.feature_column.numeric_column**: Dense floating-point fields (contoh: temperature)\n",
        "2. **tf.feature_column.categorical_column_with_identity**: Categorical/bucketized fields dengan integer index (contoh: day, month)\n",
        "3. **tf.feature_column.indicator_column**: Convert categorical_column_with_identity ke one-hot encoded representation\n",
        "4. **tf.feature_column.embedding_column**: Generate embedding dari integer-based column\n",
        "\n",
        "**Building Feature Columns**:\n",
        "```python\n",
        "def _build_keras_model():\n",
        "    # Dense float features\n",
        "    real_valued_columns = [\n",
        "        tf.feature_column.numeric_column(key=key, shape=(1,))\n",
        "        for key in transformed_names(DENSE_FLOAT_FEATURE_KEYS)\n",
        "    ]\n",
        "    \n",
        "    # Bucketized features (one-hot encoded)\n",
        "    categorical_columns = [\n",
        "        tf.feature_column.indicator_column(\n",
        "            tf.feature_column.categorical_column_with_identity(\n",
        "                key,\n",
        "                num_buckets=len(boundaries)+1\n",
        "            )\n",
        "        )\n",
        "        for key, boundaries in zip(\n",
        "            transformed_names(BUCKET_FEATURE_KEYS),\n",
        "            BUCKET_FEATURE_BOUNDARIES\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Vocabulary-based categorical features (one-hot encoded)\n",
        "    categorical_columns += [\n",
        "        tf.feature_column.indicator_column(\n",
        "            tf.feature_column.categorical_column_with_identity(\n",
        "                key,\n",
        "                num_buckets=num_buckets,\n",
        "                default_value=num_buckets-1  # Unseen categories assigned here\n",
        "            )\n",
        "        )\n",
        "        for key, num_buckets in zip(\n",
        "            transformed_names(VOCAB_FEATURE_KEYS),\n",
        "            MAX_CATEGORICAL_FEATURE_VALUES\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Build model\n",
        "    model = _dnn_regressor(\n",
        "        columns=real_valued_columns + categorical_columns,\n",
        "        dnn_hidden_units=[128, 64]\n",
        "    )\n",
        "    return model\n",
        "```\n",
        "\n",
        "**Toy Example - numeric_column**:\n",
        "```python\n",
        "a = tf.feature_column.numeric_column(\"a\")\n",
        "x = tf.keras.layers.DenseFeatures(a)({'a': [0.5, 0.6]})\n",
        "print(x)  # [[0.5], [0.6]], shape=(2, 1)\n",
        "```\n",
        "\n",
        "**Toy Example - categorical with indicator**:\n",
        "```python\n",
        "b = tf.feature_column.indicator_column(\n",
        "    tf.feature_column.categorical_column_with_identity('b', num_buckets=10)\n",
        ")\n",
        "y = tf.keras.layers.DenseFeatures(b)({'b': [5, 2]})\n",
        "print(y)  # [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
        "```\n",
        "\n",
        "### Deep Neural Network Regressor\n",
        "\n",
        "**Architecture**:\n",
        "```python\n",
        "def _dnn_regressor(columns, dnn_hidden_units):\n",
        "    # Input layers: dictionary mapping feature name → Input layer\n",
        "    input_layers = {\n",
        "        colname: tf.keras.layers.Input(name=colname, shape=(), dtype=tf.float32)\n",
        "        for colname in transformed_names(DENSE_FLOAT_FEATURE_KEYS)\n",
        "    }\n",
        "    input_layers.update({\n",
        "        colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
        "        for colname in transformed_names(VOCAB_FEATURE_KEYS)\n",
        "    })\n",
        "    input_layers.update({\n",
        "        colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
        "        for colname in transformed_names(BUCKET_FEATURE_KEYS)\n",
        "    })\n",
        "    \n",
        "    # DenseFeatures layer: aggregate all Input layers → single tensor\n",
        "    output = tf.keras.layers.DenseFeatures(columns)(input_layers)\n",
        "    \n",
        "    # Hidden layers\n",
        "    for numnodes in dnn_hidden_units:\n",
        "        output = tf.keras.layers.Dense(numnodes, activation='tanh')(output)\n",
        "    \n",
        "    # Regression output layer\n",
        "    output = tf.keras.layers.Dense(1)(output)  # Linear activation\n",
        "    \n",
        "    # Compile model\n",
        "    model = tf.keras.Model(input_layers, output)\n",
        "    model.compile(\n",
        "        loss='mean_squared_error',\n",
        "        optimizer=tf.keras.optimizers.Adam(lr=0.001)\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "```\n",
        "\n",
        "**DenseFeatures Layer Functionality**:\n",
        "- **Input**: Dictionary Input layers + list feature columns\n",
        "- **Process**: Map each Input layer ke corresponding feature column\n",
        "- **Output**: Single tensor output (contoh: shape [batch_size, 31] untuk 31 total features)\n",
        "\n",
        "**Model Summary**:\n",
        "- Input layer untuk setiap feature (12 features total)\n",
        "- DenseFeatures aggregate ke [None, 31] tensor (one-hot encoding expand dimensions)\n",
        "- Hidden layer 1: 128 nodes (tanh activation)\n",
        "- Hidden layer 2: 64 nodes (tanh activation)\n",
        "- Output layer: 1 node (linear activation untuk regression)\n",
        "- Total parameters: ~12,417\n",
        "\n",
        "### Data Input Function\n",
        "\n",
        "**Purpose**: Generate tf.data.Dataset objects dari TFRecord files untuk training dan evaluation.\n",
        "\n",
        "**Type Hinting in Python**: Visual cue untuk ensure correct input/output types (not enforced by interpreter).\n",
        "- Syntax: `def function(argument: type) -> return_type:`\n",
        "- Complex types: Use `typing` library (contoh: `List[Text]` = list of strings)\n",
        "\n",
        "**Implementation**:\n",
        "```python\n",
        "from typing import List, Text\n",
        "\n",
        "def _input_fn(file_pattern: List[Text],\n",
        "              data_accessor: tfx.components.DataAccessor,\n",
        "              tf_transform_output: tft.TFTransformOutput,\n",
        "              batch_size: int = 200) -> tf.data.Dataset:\n",
        "    \n",
        "    return data_accessor.tf_dataset_factory(\n",
        "        file_pattern,\n",
        "        tfxio.TensorFlowDatasetOptions(\n",
        "            batch_size=batch_size,\n",
        "            label_key=transformed_name(LABEL_KEY)\n",
        "        ),\n",
        "        tf_transform_output.transformed_metadata.schema\n",
        "    )\n",
        "```\n",
        "\n",
        "**Arguments**:\n",
        "- **file_pattern**: List file paths containing data\n",
        "- **data_accessor**: Special TFX object untuk create tf.data.Dataset dari filenames\n",
        "- **tf_transform_output**: Transformation graph untuk convert raw examples ke features\n",
        "- **batch_size**: Batch size integer\n",
        "\n",
        "**DataAccessor Functionality**: Takes file paths, data set options (batch size, label key), schema → returns tf.data.Dataset dengan features separated dari label.\n",
        "\n",
        "### Model Training Function\n",
        "\n",
        "**FnArgs Object**: Utility object dalam TensorFlow untuk declare training-related user-defined arguments.\n",
        "\n",
        "**Key Attributes dalam FnArgs**:\n",
        "- **train_files**: List train filenames\n",
        "- **eval_files**: List evaluation filenames\n",
        "- **train_steps**: Number training steps\n",
        "- **eval_steps**: Number evaluation steps\n",
        "- **schema_path**: Path ke schema generated by SchemaGen\n",
        "- **transform_graph_path**: Path ke transform graph by Transform\n",
        "- **serving_model_dir**: Output directory untuk serve-able model\n",
        "- **model_run_dir**: Output directory untuk model run artifacts\n",
        "\n",
        "**run_fn Implementation** (forest_fires_trainer.py):\n",
        "```python\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "    # Log fn_args untuk visibility\n",
        "    absl.logging.info(\"=\" * 50)\n",
        "    absl.logging.info(\"Printing fn_args object\")\n",
        "    absl.logging.info(fn_args)\n",
        "    absl.logging.info(\"=\" * 50)\n",
        "    \n",
        "    # Load transform graph\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = _input_fn(\n",
        "        fn_args.train_files,\n",
        "        fn_args.data_accessor,\n",
        "        tf_transform_output,\n",
        "        batch_size=40\n",
        "    )\n",
        "    eval_dataset = _input_fn(\n",
        "        fn_args.eval_files,\n",
        "        fn_args.data_accessor,\n",
        "        tf_transform_output,\n",
        "        batch_size=40\n",
        "    )\n",
        "    \n",
        "    # Build model\n",
        "    model = _build_keras_model()\n",
        "    \n",
        "    # CSV logger callback\n",
        "    csv_write_dir = os.path.join(fn_args.model_run_dir, 'model_performance')\n",
        "    os.makedirs(csv_write_dir, exist_ok=True)\n",
        "    csv_callback = tf.keras.callbacks.CSVLogger(\n",
        "        os.path.join(csv_write_dir, 'performance.csv'),\n",
        "        append=False\n",
        "    )\n",
        "    \n",
        "    # Train model\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        steps_per_epoch=fn_args.train_steps,\n",
        "        validation_data=eval_dataset,\n",
        "        validation_steps=fn_args.eval_steps,\n",
        "        epochs=10,\n",
        "        callbacks=[csv_callback]\n",
        "    )\n",
        "    \n",
        "    # Define signatures\n",
        "    signatures = {\n",
        "        'serving_default': _get_serve_tf_examples_fn(\n",
        "            model, tf_transform_output\n",
        "        ).get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
        "        )\n",
        "    }\n",
        "    \n",
        "    # Save model\n",
        "    model.save(\n",
        "        fn_args.serving_model_dir,\n",
        "        save_format='tf',\n",
        "        signatures=signatures\n",
        "    )\n",
        "```\n",
        "\n",
        "### SignatureDefs: Defining Model Behavior untuk API Requests\n",
        "\n",
        "**Purpose**: Signatures define how model behaves when data sent via API call saat model deployed. Similar to personal signatures uniquely identify person, TensorFlow signatures uniquely determine model behavior untuk HTTP requests.\n",
        "\n",
        "**Signature Components**:\n",
        "- **Key**: Unique identifier define exact URL untuk activate signature\n",
        "- **Value**: TensorFlow function (@tf.function decorated) define how input handled dan passed ke model\n",
        "\n",
        "**TensorFlow Signature Names** (defined constants):\n",
        "1. **PREDICT_METHOD_NAME** (`'tensorflow/serving/predict'`): Predict target untuk incoming inputs (no target required)\n",
        "2. **REGRESS_METHOD_NAME** (`'tensorflow/serving/regress'`): Regress dari example (expects input + target)\n",
        "3. **CLASSIFY_METHOD_NAME** (`'tensorflow/serving/classify'`): Classify example (expects input + target)\n",
        "4. **DEFAULT_SERVING_SIGNATURE_DEF_KEY** (`'serving_default'`): Default signature (minimum requirement)\n",
        "\n",
        "**@tf.function Decorator**: Takes function dengan TensorFlow operations, traces steps, converts ke data-flow graph. Required untuk signature definitions.\n",
        "\n",
        "**Serving Function Implementation**:\n",
        "```python\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "    # Attach transform layer ke model\n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "    \n",
        "    @tf.function\n",
        "    def serve_tf_examples_fn(serialized_tf_examples):\n",
        "        # Get raw feature specs (column name → Feature type mapping)\n",
        "        feature_spec = tf_transform_output.raw_feature_spec()\n",
        "        feature_spec.pop(LABEL_KEY)  # Remove label dari input\n",
        "        \n",
        "        # Parse serialized examples\n",
        "        parsed_features = tf.io.parse_example(\n",
        "            serialized_tf_examples,\n",
        "            feature_spec\n",
        "        )\n",
        "        \n",
        "        # Transform raw columns ke features\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        \n",
        "        # Return model predictions\n",
        "        return model(transformed_features)\n",
        "    \n",
        "    return serve_tf_examples_fn\n",
        "```\n",
        "\n",
        "**Feature Spec Structure**: Dictionary mapping column names ke Feature types (VarLenFeature with dtype).\n",
        "```python\n",
        "{\n",
        "    'DC': VarLenFeature(dtype=tf.float32),\n",
        "    'RH': VarLenFeature(dtype=tf.int64),\n",
        "    'day': VarLenFeature(dtype=tf.string),\n",
        "    ...\n",
        "}\n",
        "```\n",
        "\n",
        "**get_concrete_function()**: Returns traced function only (tidak execute graph). Traces function dan creates data-flow graph tanpa execution.\n",
        "\n",
        "**TransformFeaturesLayer**: Keras layer yang know how to convert parsed examples ke batch inputs dengan multiple features.\n",
        "\n",
        "### Training dengan TFX Trainer Component\n",
        "\n",
        "**Calculate Training Steps**:\n",
        "```python\n",
        "n_dataset_size = df.shape[0]\n",
        "batch_size = 40\n",
        "\n",
        "# Training: 2/3 data (2 hash buckets)\n",
        "n_train_steps = int(2 * n_dataset_size / (3 * batch_size))\n",
        "n_train_steps_mod = 2 * n_dataset_size % (3 * batch_size)\n",
        "if n_train_steps_mod != 0:\n",
        "    n_train_steps += 1\n",
        "\n",
        "# Evaluation: 1/3 data (1 hash bucket)\n",
        "n_eval_steps = int(n_dataset_size / (3 * batch_size))\n",
        "n_eval_steps_mod = n_dataset_size % (3 * batch_size)\n",
        "if n_eval_steps_mod != 0:\n",
        "    n_eval_steps += 1\n",
        "```\n",
        "\n",
        "**Trainer Component**:\n",
        "```python\n",
        "from tfx.components import Trainer\n",
        "from tfx.proto import trainer_pb2\n",
        "\n",
        "trainer = Trainer(\n",
        "    module_file=os.path.abspath(\"forest_fires_trainer.py\"),\n",
        "    transformed_examples=transform.outputs['transformed_examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    train_args=trainer_pb2.TrainArgs(num_steps=n_train_steps),\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=n_eval_steps)\n",
        ")\n",
        "context.run(trainer)\n",
        "```\n",
        "\n",
        "**Training Output Log**:\n",
        "- Generates wheel package (.whl) dari training code\n",
        "- Prints model summary (Input layers, DenseFeatures, hidden layers, output)\n",
        "- Warning tentang tf.function retracing (unavoidable dengan TFX Trainer behavior)\n",
        "- Training progress dengan loss per epoch\n",
        "- Final model saved ke `<pipeline_root>/Trainer/model/<execution_id>/Format-Serving`\n",
        "\n",
        "**Training Results**:\n",
        "- Epoch 1: loss ~13,636, val_loss ~574\n",
        "- Epoch 10: loss ~1,105, val_loss ~456\n",
        "- Final MSE ~456 → error ~22 hectares (0.22 km²) per example\n",
        "- Error cukup besar, disebabkan anomalies dalam data\n",
        "\n",
        "### Anomaly Detection dan Removal\n",
        "\n",
        "**Problem**: Validation loss 456 (MSE) → prediction off by 22 hectares. Data mengandung banyak outliers.\n",
        "\n",
        "**tensorflow_data_validation (tfdv) Library**: Provides functions untuk validate data against schema, display anomalies, edit schema.\n",
        "\n",
        "**Key Functions**:\n",
        "- **tfdv.validate_statistics()**: Validate data against schema\n",
        "- **tfdv.display_anomalies()**: List anomalous samples\n",
        "- **tfdv.get_feature()**: Edit schema untuk modify outlier criteria\n",
        "- **tfdv.visualize_statistics()**: Visualize original vs cleaned data\n",
        "\n",
        "**Schema Editing Example**:\n",
        "```python\n",
        "isi_feature = tfdv.get_feature(schema, 'ISI')\n",
        "isi_feature.float_domain.max = 30.0  # Change maximum allowed value\n",
        "```\n",
        "\n",
        "**ExampleValidator Component**: TFX component untuk ensure no anomalies dalam data set setelah cleaning.\n",
        "\n",
        "---\n",
        "\n",
        "## Bagian 3: Containerization dengan Docker\n",
        "\n",
        "### Konsep Docker\n",
        "\n",
        "**Container**: Lightweight, portable, isolated environment untuk run applications dengan all dependencies. Berbeda dengan Virtual Machines (VMs):\n",
        "\n",
        "**Docker vs Virtual Machines**:\n",
        "- **VM**: Include full OS, heavy (GBs), slow startup, resource-intensive\n",
        "- **Container**: Share host OS kernel, lightweight (MBs), fast startup (<1 second), efficient resource usage\n",
        "\n",
        "**Docker Components**:\n",
        "1. **Docker Image**: Blueprint untuk container (static snapshot dengan application + dependencies)\n",
        "2. **Docker Container**: Running instance dari image (isolated runtime environment)\n",
        "3. **Dockerfile**: Text file dengan instructions untuk build Docker image\n",
        "\n",
        "**Analogy**: Image = recipe, Container = cooked dish dari recipe.\n",
        "\n",
        "### Dockerfile untuk TensorFlow Serving\n",
        "\n",
        "**Base Image**: `tensorflow/serving:2.6.0` (official TensorFlow serving image).\n",
        "\n",
        "**Dockerfile Structure**:\n",
        "```dockerfile\n",
        "FROM tensorflow/serving:2.6.0\n",
        "\n",
        "# Environment variables\n",
        "ENV MODEL_BASE_PATH=/models\n",
        "ENV MODEL_NAME=forest_fires_model\n",
        "\n",
        "# Copy model ke container\n",
        "COPY --from=<source> <model_path> ${MODEL_BASE_PATH}/${MODEL_NAME}/1\n",
        "\n",
        "# Expose port untuk API\n",
        "EXPOSE 8501\n",
        "\n",
        "# Run TensorFlow serving server\n",
        "CMD [\"tensorflow_model_server\", \\\n",
        "     \"--rest_api_port=8501\", \\\n",
        "     \"--model_name=${MODEL_NAME}\", \\\n",
        "     \"--model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME}\"]\n",
        "```\n",
        "\n",
        "**Key Instructions**:\n",
        "- **FROM**: Specify base image\n",
        "- **ENV**: Set environment variables\n",
        "- **COPY**: Copy files ke container\n",
        "- **EXPOSE**: Declare network port\n",
        "- **CMD**: Command untuk run saat container starts\n",
        "\n",
        "**Model Versioning**: Models organized dalam directories dengan version numbers (contoh: `/models/forest_fires_model/1`). Allows serving multiple versions simultaneously.\n",
        "\n",
        "### Building Docker Image\n",
        "\n",
        "**Build Command**:\n",
        "```bash\n",
        "docker build -t forest-fires-model-server:latest .\n",
        "```\n",
        "\n",
        "**Arguments**:\n",
        "- **-t**: Tag name untuk image\n",
        "- **.**: Build context (current directory)\n",
        "\n",
        "**Build Process**:\n",
        "1. Download base image jika not exists locally\n",
        "2. Execute Dockerfile instructions sequentially\n",
        "3. Create layers untuk each instruction (cached untuk efficiency)\n",
        "4. Tag final image dengan specified name\n",
        "\n",
        "**List Images**:\n",
        "```bash\n",
        "docker images\n",
        "```\n",
        "\n",
        "### Running Docker Container\n",
        "\n",
        "**Run Command**:\n",
        "```bash\n",
        "docker run -p 8501:8501 --name forest-fires-server forest-fires-model-server:latest\n",
        "```\n",
        "\n",
        "**Arguments**:\n",
        "- **-p 8501:8501**: Port mapping (host:container)\n",
        "- **--name**: Container name\n",
        "- Last argument: Image name\n",
        "\n",
        "**Port Mapping**: Maps host machine port ke container port. Allows access container services dari host.\n",
        "\n",
        "**Container Management Commands**:\n",
        "```bash\n",
        "docker ps                    # List running containers\n",
        "docker ps -a                 # List all containers (including stopped)\n",
        "docker stop <container_name> # Stop container\n",
        "docker start <container_name># Start stopped container\n",
        "docker rm <container_name>   # Remove container\n",
        "docker logs <container_name> # View container logs\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Bagian 4: Deployment dan Serving via REST API\n",
        "\n",
        "### REST API Concepts\n",
        "\n",
        "**REST (Representational State Transfer)**: Architectural style untuk designing networked applications. Uses HTTP requests untuk access dan manipulate resources.\n",
        "\n",
        "**HTTP Methods**:\n",
        "- **GET**: Request data dari server (no request body, can be cached)\n",
        "- **POST**: Send data ke server (has request body, more secure untuk sensitive data)\n",
        "- **PUT**: Update existing resource\n",
        "- **DELETE**: Remove resource\n",
        "\n",
        "**HTTP Request Anatomy**:\n",
        "1. **Method type**: GET, POST, PUT, DELETE\n",
        "2. **Path**: URL ke endpoint\n",
        "3. **Body**: Payload needed untuk complete request\n",
        "4. **Header**: Additional information (contoh: content-type)\n",
        "\n",
        "**HTTP Response Anatomy**:\n",
        "1. **Status code**: Indicate request success/failure (200 = success, 404 = not found, 500 = server error)\n",
        "2. **Header**: Metadata about response\n",
        "3. **Body**: Response data (contoh: predictions)\n",
        "\n",
        "### TensorFlow Serving API Endpoints\n",
        "\n",
        "**Base URL Format**: `http://<hostname>:<port>/v1/models/<model_name>`\n",
        "\n",
        "**Available Endpoints**:\n",
        "1. **:predict**\n",
        "   - **URL**: `/v1/models/forest_fires_model:predict`\n",
        "   - **Method**: POST\n",
        "   - **Purpose**: Predict output tanpa target\n",
        "   - **Input**: serialized examples\n",
        "   - **Output**: predictions\n",
        "\n",
        "2. **:regress**\n",
        "   - **URL**: `/v1/models/forest_fires_model:regress`\n",
        "   - **Method**: POST\n",
        "   - **Purpose**: Regression dengan input + target\n",
        "   - **Output**: predictions + error metrics\n",
        "\n",
        "3. **:classify**\n",
        "   - **URL**: `/v1/models/forest_fires_model:classify`\n",
        "   - **Method**: POST\n",
        "   - **Purpose**: Classification dengan input + target\n",
        "   - **Output**: class predictions + error metrics\n",
        "\n",
        "4. **/metadata**\n",
        "   - **URL**: `/v1/models/forest_fires_model/metadata`\n",
        "   - **Method**: GET\n",
        "   - **Purpose**: Get metadata tentang available endpoints/signatures\n",
        "   - **Output**: model signature information\n",
        "\n",
        "### Making API Requests dengan Python\n",
        "\n",
        "**Request Body Format**:\n",
        "```python\n",
        "{\n",
        "    \"signature_name\": \"serving_default\",\n",
        "    \"instances\": [<serialized_examples>]\n",
        "}\n",
        "```\n",
        "\n",
        "**Base64 Encoding**: Encodes byte stream (binary input) ke ASCII text string. Required untuk serialize examples dalam HTTP request.\n",
        "\n",
        "**Python Implementation**:\n",
        "```python\n",
        "import base64\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# Prepare request body\n",
        "req_body = {\n",
        "    \"signature_name\": \"serving_default\",\n",
        "    \"instances\": [\n",
        "        str(base64.b64encode(\n",
        "            b'{\"X\": 7, \"Y\": 4, \"month\": \"oct\", \"day\": \"fri\", '\n",
        "            b'\"FFMC\": 60, \"DMC\": 30, \"DC\": 200, \"ISI\": 9, '\n",
        "            b'\"temp\": 30, \"RH\": 50, \"wind\": 10, \"rain\": 0}'\n",
        "        ))\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert ke JSON\n",
        "data = json.dumps(req_body)\n",
        "\n",
        "# Send POST request\n",
        "json_response = requests.post(\n",
        "    'http://localhost:8501/v1/models/forest_fires_model:predict',\n",
        "    data=data,\n",
        "    headers={\"content-type\": \"application/json\"}\n",
        ")\n",
        "\n",
        "# Parse response\n",
        "predictions = json.loads(json_response.text)\n",
        "print(predictions)  # {'predictions': [[2.77522683]]}\n",
        "```\n",
        "\n",
        "**Response Structure**:\n",
        "```python\n",
        "{\n",
        "    'predictions': [[predicted_value]]\n",
        "}\n",
        "```\n",
        "\n",
        "### End-to-End Workflow Visualization\n",
        "\n",
        "**Complete Pipeline**:\n",
        "1. **Client** sends HTTP POST request dengan input data\n",
        "2. **TensorFlow Serving Server** (dalam Docker container) listens pada port 8501\n",
        "3. **API Endpoint** receives request, routes ke appropriate signature\n",
        "4. **Signature Function** parses serialized examples, transforms features\n",
        "5. **Model** processes transformed features, generates predictions\n",
        "6. **Response** flows back: Model → Signature → API → Server → Client\n",
        "\n",
        "**Architecture Benefits**:\n",
        "- **Scalability**: Docker containers dapat replicated untuk handle high traffic\n",
        "- **Isolation**: Each service runs independently dalam isolated environment\n",
        "- **Portability**: Docker image dapat deployed anywhere (local, cloud, on-premise)\n",
        "- **Version Control**: Multiple model versions dapat served simultaneously\n",
        "- **Monitoring**: Centralized logging dan metrics collection\n",
        "\n",
        "---\n",
        "\n",
        "## Program-Program Implementasi\n",
        "\n",
        "### Program 1: Complete Data Pipeline Setup\n",
        "\n",
        "```python\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, Transform\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "import absl.logging\n",
        "\n",
        "# Setup\n",
        "absl.logging.set_verbosity(absl.logging.INFO)\n",
        "_pipeline_root = os.path.join(os.getcwd(), 'pipeline', 'examples', 'forest_fires_pipeline')\n",
        "\n",
        "# Create context\n",
        "context = InteractiveContext(\n",
        "    pipeline_name=\"forest_fires\",\n",
        "    pipeline_root=_pipeline_root\n",
        ")\n",
        "\n",
        "# 1. Load data dari CSV\n",
        "example_gen = CsvExampleGen(input_base=os.path.join('data', 'csv', 'train'))\n",
        "context.run(example_gen)\n",
        "\n",
        "# 2. Generate statistics\n",
        "statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
        "context.run(statistics_gen)\n",
        "context.show(statistics_gen.outputs['statistics'])\n",
        "\n",
        "# 3. Infer schema\n",
        "schema_gen = SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False\n",
        ")\n",
        "context.run(schema_gen)\n",
        "context.show(schema_gen.outputs['schema'])\n",
        "\n",
        "# 4. Transform features\n",
        "transform = Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath('forest_fires_transform.py')\n",
        ")\n",
        "context.run(transform)\n",
        "```\n",
        "\n",
        "**Penjelasan**: Program establish complete data pipeline dari raw CSV hingga transformed features. CsvExampleGen split data, StatisticsGen generate visualizations, SchemaGen infer structure, Transform convert columns ke features. Each component output fed ke next component.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 2: Feature Transformation Module\n",
        "\n",
        "```python\n",
        "# File: forest_fires_transform.py\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import forest_fires_constants\n",
        "\n",
        "_DENSE_FLOAT_FEATURE_KEYS = forest_fires_constants.DENSE_FLOAT_FEATURE_KEYS\n",
        "_VOCAB_FEATURE_KEYS = forest_fires_constants.VOCAB_FEATURE_KEYS\n",
        "_BUCKET_FEATURE_KEYS = forest_fires_constants.BUCKET_FEATURE_KEYS\n",
        "_BUCKET_FEATURE_BOUNDARIES = forest_fires_constants.BUCKET_FEATURE_BOUNDARIES\n",
        "_LABEL_KEY = forest_fires_constants.LABEL_KEY\n",
        "_transformed_name = forest_fires_constants.transformed_name\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "    \"\"\"Convert raw data columns ke model-ready features\"\"\"\n",
        "    outputs = {}\n",
        "    \n",
        "    # Dense float features: Z-score normalization\n",
        "    for key in _DENSE_FLOAT_FEATURE_KEYS:\n",
        "        outputs[_transformed_name(key)] = tft.scale_to_z_score(\n",
        "            sparse_to_dense(inputs[key])\n",
        "        )\n",
        "    \n",
        "    # Vocabulary-based categorical: Integer ID mapping\n",
        "    for key in _VOCAB_FEATURE_KEYS:\n",
        "        outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(\n",
        "            sparse_to_dense(inputs[key]),\n",
        "            num_oov_buckets=1  # Handle unseen values\n",
        "        )\n",
        "    \n",
        "    # Bucketized features: Assign ke bins\n",
        "    for key, boundary in zip(_BUCKET_FEATURE_KEYS, _BUCKET_FEATURE_BOUNDARIES):\n",
        "        outputs[_transformed_name(key)] = tft.apply_buckets(\n",
        "            sparse_to_dense(inputs[key]),\n",
        "            bucket_boundaries=[boundary]\n",
        "        )\n",
        "    \n",
        "    # Label: Keep as numerical\n",
        "    outputs[_transformed_name(_LABEL_KEY)] = sparse_to_dense(inputs[_LABEL_KEY])\n",
        "    \n",
        "    return outputs\n",
        "\n",
        "def sparse_to_dense(x):\n",
        "    \"\"\"Convert SparseTensor ke DenseTensor\"\"\"\n",
        "    return tf.squeeze(\n",
        "        tf.sparse.to_dense(\n",
        "            tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1])\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "```\n",
        "\n",
        "**Penjelasan**: Module define preprocessing_fn yang required oleh TFX Transform component. Three types transformations: Z-score normalization untuk continuous features, vocabulary mapping untuk categorical strings, bucketization untuk discretizing continuous values. sparse_to_dense utility function handle sparse tensor conversion.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 3: Model Definition dengan Feature Columns\n",
        "\n",
        "```python\n",
        "# File: forest_fires_trainer.py (part 1)\n",
        "import tensorflow as tf\n",
        "import forest_fires_constants\n",
        "\n",
        "_DENSE_FLOAT_FEATURE_KEYS = forest_fires_constants.DENSE_FLOAT_FEATURE_KEYS\n",
        "_VOCAB_FEATURE_KEYS = forest_fires_constants.VOCAB_FEATURE_KEYS\n",
        "_BUCKET_FEATURE_KEYS = forest_fires_constants.BUCKET_FEATURE_KEYS\n",
        "_BUCKET_FEATURE_BOUNDARIES = forest_fires_constants.BUCKET_FEATURE_BOUNDARIES\n",
        "_MAX_CATEGORICAL_FEATURE_VALUES = forest_fires_constants.MAX_CATEGORICAL_FEATURE_VALUES\n",
        "_transformed_names = lambda keys: [key + '_xf' for key in keys]\n",
        "\n",
        "def _build_keras_model():\n",
        "    \"\"\"Build regression neural network dengan feature columns\"\"\"\n",
        "    \n",
        "    # Dense float feature columns\n",
        "    real_valued_columns = [\n",
        "        tf.feature_column.numeric_column(key=key, shape=(1,))\n",
        "        for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\n",
        "    ]\n",
        "    \n",
        "    # Bucketized feature columns (one-hot encoded)\n",
        "    categorical_columns = [\n",
        "        tf.feature_column.indicator_column(\n",
        "            tf.feature_column.categorical_column_with_identity(\n",
        "                key,\n",
        "                num_buckets=len(boundaries)+1\n",
        "            )\n",
        "        )\n",
        "        for key, boundaries in zip(\n",
        "            _transformed_names(_BUCKET_FEATURE_KEYS),\n",
        "            _BUCKET_FEATURE_BOUNDARIES\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Vocab-based categorical columns (one-hot encoded)\n",
        "    categorical_columns += [\n",
        "        tf.feature_column.indicator_column(\n",
        "            tf.feature_column.categorical_column_with_identity(\n",
        "                key,\n",
        "                num_buckets=num_buckets,\n",
        "                default_value=num_buckets-1  # OOV handling\n",
        "            )\n",
        "        )\n",
        "        for key, num_buckets in zip(\n",
        "            _transformed_names(_VOCAB_FEATURE_KEYS),\n",
        "            _MAX_CATEGORICAL_FEATURE_VALUES\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Build DNN regressor\n",
        "    model = _dnn_regressor(\n",
        "        columns=real_valued_columns + categorical_columns,\n",
        "        dnn_hidden_units=[128, 64]\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "def _dnn_regressor(columns, dnn_hidden_units):\n",
        "    \"\"\"Define deep neural network architecture\"\"\"\n",
        "    \n",
        "    # Input layers untuk each feature type\n",
        "    input_layers = {\n",
        "        colname: tf.keras.layers.Input(name=colname, shape=(), dtype=tf.float32)\n",
        "        for colname in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\n",
        "    }\n",
        "    input_layers.update({\n",
        "        colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
        "        for colname in _transformed_names(_VOCAB_FEATURE_KEYS)\n",
        "    })\n",
        "    input_layers.update({\n",
        "        colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
        "        for colname in _transformed_names(_BUCKET_FEATURE_KEYS)\n",
        "    })\n",
        "    \n",
        "    # DenseFeatures layer aggregate all inputs\n",
        "    output = tf.keras.layers.DenseFeatures(columns)(input_layers)\n",
        "    \n",
        "    # Hidden layers dengan tanh activation\n",
        "    for numnodes in dnn_hidden_units:\n",
        "        output = tf.keras.layers.Dense(numnodes, activation='tanh')(output)\n",
        "    \n",
        "    # Regression output (linear activation)\n",
        "    output = tf.keras.layers.Dense(1)(output)\n",
        "    \n",
        "    # Compile model\n",
        "    model = tf.keras.Model(input_layers, output)\n",
        "    model.compile(\n",
        "        loss='mean_squared_error',\n",
        "        optimizer=tf.keras.optimizers.Adam(lr=0.001)\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "```\n",
        "\n",
        "**Penjelasan**: Program define Keras model dengan feature columns approach. real_valued_columns untuk continuous features, categorical_columns untuk discretized/categorical features dengan one-hot encoding. DenseFeatures layer aggregate all Input layers ke single tensor. Architecture: Input → DenseFeatures → Dense(128) → Dense(64) → Dense(1). MSE loss untuk regression.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 4: Training Function dengan Signatures\n",
        "\n",
        "```python\n",
        "# File: forest_fires_trainer.py (part 2)\n",
        "import tensorflow_transform as tft\n",
        "import tfx.components\n",
        "from typing import List, Text\n",
        "\n",
        "def _input_fn(file_pattern: List[Text],\n",
        "              data_accessor: tfx.components.DataAccessor,\n",
        "              tf_transform_output: tft.TFTransformOutput,\n",
        "              batch_size: int = 200):\n",
        "    \"\"\"Generate tf.data.Dataset dari TFRecord files\"\"\"\n",
        "    return data_accessor.tf_dataset_factory(\n",
        "        file_pattern,\n",
        "        tfxio.TensorFlowDatasetOptions(\n",
        "            batch_size=batch_size,\n",
        "            label_key=_transformed_name(_LABEL_KEY)\n",
        "        ),\n",
        "        tf_transform_output.transformed_metadata.schema\n",
        "    )\n",
        "\n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "    \"\"\"Create signature function untuk serving\"\"\"\n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "    \n",
        "    @tf.function\n",
        "    def serve_tf_examples_fn(serialized_tf_examples):\n",
        "        # Get feature specs dan remove label\n",
        "        feature_spec = tf_transform_output.raw_feature_spec()\n",
        "        feature_spec.pop(_LABEL_KEY)\n",
        "        \n",
        "        # Parse serialized examples\n",
        "        parsed_features = tf.io.parse_example(\n",
        "            serialized_tf_examples,\n",
        "            feature_spec\n",
        "        )\n",
        "        \n",
        "        # Transform features\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        \n",
        "        # Return predictions\n",
        "        return model(transformed_features)\n",
        "    \n",
        "    return serve_tf_examples_fn\n",
        "\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "    \"\"\"Main training function called by TFX Trainer\"\"\"\n",
        "    \n",
        "    # Load transform graph\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = _input_fn(\n",
        "        fn_args.train_files,\n",
        "        fn_args.data_accessor,\n",
        "        tf_transform_output,\n",
        "        batch_size=40\n",
        "    )\n",
        "    eval_dataset = _input_fn(\n",
        "        fn_args.eval_files,\n",
        "        fn_args.data_accessor,\n",
        "        tf_transform_output,\n",
        "        batch_size=40\n",
        "    )\n",
        "    \n",
        "    # Build model\n",
        "    model = _build_keras_model()\n",
        "    \n",
        "    # Setup CSV logger\n",
        "    csv_write_dir = os.path.join(fn_args.model_run_dir, 'model_performance')\n",
        "    os.makedirs(csv_write_dir, exist_ok=True)\n",
        "    csv_callback = tf.keras.callbacks.CSVLogger(\n",
        "        os.path.join(csv_write_dir, 'performance.csv'),\n",
        "        append=False\n",
        "    )\n",
        "    \n",
        "    # Train model\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        steps_per_epoch=fn_args.train_steps,\n",
        "        validation_data=eval_dataset,\n",
        "        validation_steps=fn_args.eval_steps,\n",
        "        epochs=10,\n",
        "        callbacks=[csv_callback]\n",
        "    )\n",
        "    \n",
        "    # Define serving signatures\n",
        "    signatures = {\n",
        "        'serving_default': _get_serve_tf_examples_fn(\n",
        "            model, tf_transform_output\n",
        "        ).get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
        "        )\n",
        "    }\n",
        "    \n",
        "    # Save model dengan signatures\n",
        "    model.save(\n",
        "        fn_args.serving_model_dir,\n",
        "        save_format='tf',\n",
        "        signatures=signatures\n",
        "    )\n",
        "```\n",
        "\n",
        "**Penjelasan**: run_fn orchestrate entire training process. _input_fn create tf.data.Dataset dari files, _get_serve_tf_examples_fn define signature untuk handle API requests. Training dengan CSVLogger untuk track performance. Signatures critical untuk TensorFlow Serving—define how model processes incoming HTTP requests. get_concrete_function trace function tanpa execution untuk create serving signature.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 5: Deploying dan Testing Model via API\n",
        "\n",
        "```python\n",
        "# Dockerfile untuk TensorFlow Serving\n",
        "\"\"\"\n",
        "FROM tensorflow/serving:2.6.0\n",
        "\n",
        "ENV MODEL_BASE_PATH=/models\n",
        "ENV MODEL_NAME=forest_fires_model\n",
        "\n",
        "COPY <pipeline_root>/Trainer/model/<execution_id>/Format-Serving \\\n",
        "     ${MODEL_BASE_PATH}/${MODEL_NAME}/1\n",
        "\n",
        "EXPOSE 8501\n",
        "\n",
        "CMD [\"tensorflow_model_server\", \\\n",
        "     \"--rest_api_port=8501\", \\\n",
        "     \"--model_name=${MODEL_NAME}\", \\\n",
        "     \"--model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME}\"]\n",
        "\"\"\"\n",
        "\n",
        "# Build Docker image\n",
        "# Terminal: docker build -t forest-fires-model-server:latest .\n",
        "\n",
        "# Run Docker container\n",
        "# Terminal: docker run -p 8501:8501 --name forest-fires-server \\\n",
        "#                      forest-fires-model-server:latest\n",
        "\n",
        "# Python client untuk send requests\n",
        "import base64\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# Prepare input data\n",
        "input_data = {\n",
        "    \"X\": 7, \"Y\": 4,\n",
        "    \"month\": \"oct\", \"day\": \"fri\",\n",
        "    \"FFMC\": 60, \"DMC\": 30, \"DC\": 200, \"ISI\": 9,\n",
        "    \"temp\": 30, \"RH\": 50, \"wind\": 10, \"rain\": 0\n",
        "}\n",
        "\n",
        "# Create request body\n",
        "req_body = {\n",
        "    \"signature_name\": \"serving_default\",\n",
        "    \"instances\": [\n",
        "        str(base64.b64encode(json.dumps(input_data).encode('utf-8')))\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Send POST request\n",
        "response = requests.post(\n",
        "    'http://localhost:8501/v1/models/forest_fires_model:predict',\n",
        "    data=json.dumps(req_body),\n",
        "    headers={\"content-type\": \"application/json\"}\n",
        ")\n",
        "\n",
        "# Parse predictions\n",
        "if response.status_code == 200:\n",
        "    predictions = json.loads(response.text)\n",
        "    print(f\"Predicted burnt area: {predictions['predictions'][0][0]:.2f} hectares\")\n",
        "else:\n",
        "    print(f\"Error: {response.status_code}\")\n",
        "    print(response.text)\n",
        "\n",
        "# Get model metadata\n",
        "metadata_response = requests.get(\n",
        "    'http://localhost:8501/v1/models/forest_fires_model/metadata'\n",
        ")\n",
        "if metadata_response.status_code == 200:\n",
        "    metadata = json.loads(metadata_response.text)\n",
        "    print(\"Available signatures:\", metadata['metadata']['signature_def'].keys())\n",
        "```\n",
        "\n",
        "**Penjelasan**: Complete deployment workflow. Dockerfile define container dengan TensorFlow Serving, model copied dari pipeline output. Docker build create image, docker run launch container expose port 8501. Python client construct request dengan base64-encoded input, send POST request ke predict endpoint. Response contains predictions. Metadata endpoint provide info tentang available signatures. Architecture enable scalable, production-ready model serving.\n",
        "\n",
        "---\n",
        "\n",
        "## Kesimpulan\n",
        "\n",
        "TFX menyediakan robust framework untuk implement end-to-end MLOps workflows, dramatically reducing time untuk productionize machine learning models. Key advantages: automated data validation dengan StatisticsGen dan SchemaGen prevent bad data dari corrupting models, feature transformations dengan tensorflow_transform ensure consistent preprocessing antara training dan serving, TFX Trainer API standardize model training dengan support untuk custom training loops, SignatureDefs enable flexible model serving behavior untuk different use cases, Docker containerization ensure portability dan isolation, TensorFlow Serving provide production-grade model serving dengan REST API interface. Complete pipeline—dari raw CSV hingga deployed model accessible via HTTP requests—demonstrate power TFX untuk streamline ML workflows di production environments. Untuk companies dengan dozens atau hundreds models, MLOps dengan TFX dramatically reduce operational overhead, improve model quality dengan systematic validation, dan accelerate deployment cycles. Future directions: advanced validation checks, A/B testing frameworks, model monitoring dashboards, automated retraining triggers, multi-model serving optimization."
      ],
      "metadata": {
        "id": "ONAWAMKQXE4x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XpeepnG3XFbt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}