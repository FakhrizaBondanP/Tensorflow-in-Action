{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CHAPTER 14: TensorBoard - Big Brother of TensorFlow\n",
        "\n",
        "## Ringkasan\n",
        "\n",
        "Chapter ini membahas **TensorBoard**, toolkit visualisasi powerful untuk memantau, mendebug, dan menganalisis kinerja model machine learning. TensorBoard memungkinkan visualisasi real-time metrik training (loss, accuracy), histogram aktivasi layer untuk mendeteksi masalah konvergensi, profiling performa untuk menemukan bottleneck komputasi, dan proyeksi embedding untuk menganalisis data dimensi tinggi seperti word vectors. Bab mencakup visualisasi image Fashion-MNIST, tracking metrik training untuk model dense dan CNN, logging metrik custom dengan API tf.summary, profiling performa dengan rekomendasi optimisasi (mixed precision training, perbaikan tf.data pipeline), dan visualisasi word vector dengan embedding GloVe pada dataset IMDB.\n",
        "\n",
        "---\n",
        "\n",
        "## Bagian 1: Visualisasi Data dengan TensorBoard\n",
        "\n",
        "### Dataset Fashion-MNIST\n",
        "\n",
        "Fashion-MNIST adalah pengganti modern untuk klasifikasi digit MNIST—task yang lebih menantang dengan format sama (gambar 28×28 skala abu-abu, 10 kelas). Kategori: Kaos/atas, Celana, Pullover, Gaun, Mantel, Sandal, Kemeja, Sneaker, Tas, Boot pergelangan kaki. Dataset diunduh via library `tensorflow_datasets`, mengembalikan split train/test dengan format dictionary `{image: (28,28,1), label: int}`.\n",
        "\n",
        "### Setup Pipeline Data\n",
        "\n",
        "Pipeline mengkonversi record dictionary ke format tuple untuk kompatibilitas training model. Langkah-langkah kunci:\n",
        "1. **Shuffle dan Konversi**: Data training di-shuffle (buffer 20×batch_size) dan dikonversi dari dict ke tuple `(image, label)`\n",
        "2. **Flatten Opsional**: Untuk fully connected networks, gambar 2D (28×28) di-flatten ke vektor 1D (784)\n",
        "3. **Split Train/Validasi**: 10.000 pertama dari training set menjadi validasi, sisanya training\n",
        "\n",
        "### Logging Gambar ke TensorBoard\n",
        "\n",
        "**tf.summary.SummaryWriter**: Mekanisme inti untuk menulis data ke direktori log. Langkah-langkah:\n",
        "1. **Buat writer**: `tf.summary.create_file_writer(log_dir)` dengan direktori yang ditimestamp\n",
        "2. **Buka context**: `with writer.as_default():` untuk mengaktifkan writer\n",
        "3. **Tulis gambar**: `tf.summary.image(name, data, step, max_outputs)` untuk log batch gambar\n",
        "\n",
        "**Argumen**:\n",
        "- `name`: Tag untuk pengelompokan gambar di dashboard\n",
        "- `data`: Tensor dengan shape `[batch, height, width, channels]`\n",
        "- `step`: Integer untuk tracking iterasi berbeda\n",
        "- `max_outputs`: Maksimal gambar untuk ditampilkan (default 3)\n",
        "\n",
        "**Dua pendekatan logging**:\n",
        "1. **Gambar individual dengan label class**: Loop melalui 10 gambar, log masing-masing dengan label sebagai tag—gambar dikelompokkan berdasarkan class\n",
        "2. **Logging batch**: Log 20 gambar sekaligus dalam satu batch—ditampilkan bersama\n",
        "\n",
        "### Meluncurkan TensorBoard\n",
        "\n",
        "**Di Jupyter Notebook**:\n",
        "```python\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs --port 6006\n",
        "```\n",
        "\n",
        "**Browser Standalone**: Navigate ke `http://localhost:6006` setelah command notebook dijalankan.\n",
        "\n",
        "Fitur dashboard: slider brightness/contrast, pemilihan subdirectory (toggle visibility), inspeksi gambar, organisasi run dengan timestamp.\n",
        "\n",
        "---\n",
        "\n",
        "## Bagian 2: Tracking dan Monitoring Performa Model\n",
        "\n",
        "### Definisi Model\n",
        "\n",
        "**Dense Network**:\n",
        "- 3 layer: 512 neuron (ReLU), 256 neuron (ReLU), 10 output (softmax)\n",
        "- Input: Vektor 784-D yang di-flatten\n",
        "- Loss: Sparse categorical cross-entropy\n",
        "- Optimizer: Adam\n",
        "- Metrics: Accuracy\n",
        "\n",
        "**CNN Network**:\n",
        "- Conv2D (32 filter, kernel 5×5, stride 2×2, ReLU)\n",
        "- Conv2D (16 filter, kernel 3×3, stride 1×1, ReLU)\n",
        "- Flatten\n",
        "- Dense (10 output, softmax)\n",
        "- Input: Gambar 28×28×1\n",
        "- Setelan kompilasi sama\n",
        "\n",
        "### TensorBoard Callback\n",
        "\n",
        "**tf.keras.callbacks.TensorBoard()**: Callback built-in Keras untuk automatic logging. Argumen penting:\n",
        "\n",
        "- **log_dir**: Direktori untuk menulis log (default 'logs'). Gunakan subdirectory dengan timestamp untuk unique run\n",
        "- **histogram_freq**: Interval (dalam epoch) untuk log histogram aktivasi layer (0 = disabled)\n",
        "- **write_graph**: Tulis arsitektur model graph (default True)\n",
        "- **update_freq**: Frekuensi penulisan—'batch', 'epoch', atau integer (interval batch)\n",
        "- **profile_batch**: Range batch untuk profiling (integer atau [start, end])\n",
        "- **embeddings_freq**: Interval untuk visualisasi embedding (0 = disabled)\n",
        "\n",
        "**Penggunaan**:\n",
        "```python\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=log_dir, profile_batch=0\n",
        ")\n",
        "model.fit(train_ds, validation_data=valid_ds, epochs=10, callbacks=[tb_callback])\n",
        "```\n",
        "\n",
        "### Organisasi Run\n",
        "\n",
        "**Praktik Terbaik**: Struktur direktori berhirarki dengan timestamp untuk membedakan experiment. Contoh:\n",
        "\n",
        "**Flat dengan timestamp**:\n",
        "```\n",
        "./logs/dense_20210527031421\n",
        "./logs/dense_20210527090252\n",
        "./logs/conv_20210527101209\n",
        "```\n",
        "\n",
        "**Nested berdasarkan hyperparameter**:\n",
        "```\n",
        "./logs/dense/lr=0.01/20210527031421\n",
        "./logs/dense/lr=0.001/20210527090252\n",
        "./logs/conv/filters=32/20210527101209\n",
        "```\n",
        "\n",
        "Timestamp memastikan folder unique dan memudahkan perbandingan historis.\n",
        "\n",
        "### Kontrol Dashboard\n",
        "\n",
        "**Smoothing Parameter**: Slider (0.0 hingga 0.99) untuk smooth metrik noisy, mengungkap tren mendasar. Nilai lebih tinggi = garis lebih smooth, original ditampilkan faded di background.\n",
        "\n",
        "**Y-axis Scale**: Toggle linear ↔ log scale untuk visualisasi lebih baik ketika metrik spanning multiple order magnitude.\n",
        "\n",
        "**Pemilihan Run**: Checkbox untuk show/hide run berbeda, enabling perbandingan terfokus.\n",
        "\n",
        "**Ukuran Plot**: Toggle standard ↔ full-size untuk pemeriksaan detail.\n",
        "\n",
        "**Auto-fit**: Kontrol zoom untuk fit garis dalam area plot.\n",
        "\n",
        "### Histogram Aktivasi\n",
        "\n",
        "**Purpose**: Visualisasi distribusi aktivasi layer sepanjang epoch training. Kritis untuk mendeteksi masalah konvergensi, dead neuron, exploding/vanishing gradient.\n",
        "\n",
        "**Enable**: Set `histogram_freq=2` (log setiap 2 epoch) dalam TensorBoard callback.\n",
        "\n",
        "**Interpretasi**:\n",
        "- **Distribusi converging**: Weight settling dalam pola stabil (typically approximately normal)\n",
        "- **Distribusi multi-modal**: Multiple peaks mengindikasikan diverse activation pattern\n",
        "- **Exploding distribution**: Weight shifting ke nilai extreme (training instability)\n",
        "- **Dead neuron**: Konsentrasi besar di zero (ReLU saturation)\n",
        "\n",
        "**Visualisasi**: Histogram 3D berstacked dengan warna lebih terang = epoch lebih recent. X-axis = binned weight value, Y-axis = count. TensorBoard menggunakan exponential bin (fine-grained dekat zero, wider bin jauh dari zero) dan resampling untuk uniform visualization.\n",
        "\n",
        "---\n",
        "\n",
        "## Bagian 3: Custom Metric Logging dengan tf.summary\n",
        "\n",
        "### Motivasi: Menganalisis Efek Batch Normalization\n",
        "\n",
        "**Experiment**: Bandingkan statistik weight (mean, std) dari Dense layer dengan dan tanpa batch normalization sepanjang training step.\n",
        "\n",
        "**Model**:\n",
        "1. **Standard Dense**: 512 → 256 → 10 neuron (track layer 'log_layer')\n",
        "2. **Dense dengan BatchNorm**: 512 → BatchNorm → 256 → BatchNorm → 10 (track 'log_layer_bn')\n",
        "\n",
        "### Custom Training Loop\n",
        "\n",
        "**Mengapa Custom Loop**: Metrik standard Keras tidak include statistik weight spesifik-layer. Perlu komputasi manual dan logging setiap batch.\n",
        "\n",
        "**Langkah-langkah Implementasi**:\n",
        "1. **Buat writer**: `tf.summary.create_file_writer(log_dir)`\n",
        "2. **Training loop**: Iterasi epoch → batch\n",
        "3. **Per-batch operation**:\n",
        "   - `model.train_on_batch(x, y)`: Update weight\n",
        "   - Extract weight: `model.get_layer(name).get_weights()[0]` (index 0 = weight, 1 = bias)\n",
        "   - Hitung metrik: `np.mean(np.abs(weights))`, `np.std(np.abs(weights))`\n",
        "   - Log scalar: `tf.summary.scalar('mean_weights', value, step=step)`\n",
        "4. **Flush buffer**: `writer.flush()` untuk menulis dari memory ke disk\n",
        "\n",
        "### tf.summary API\n",
        "\n",
        "**Tipe umum**:\n",
        "- `tf.summary.scalar()`: Nilai numerik tunggal (loss, accuracy, metrik custom)\n",
        "- `tf.summary.histogram()`: Visualisasi distribusi\n",
        "- `tf.summary.image()`: Data gambar\n",
        "- `tf.summary.audio()`: File audio\n",
        "- `tf.summary.text()`: Data teks raw\n",
        "\n",
        "**Argumen**:\n",
        "- `name`: Identifier metrik (ditampilkan di dashboard)\n",
        "- `data`: Value untuk di-log\n",
        "- `step`: Integer counter step (x-axis di dashboard)\n",
        "\n",
        "### Interpretasi Hasil\n",
        "\n",
        "Dengan batch normalization, mean dan std dari weight vary **significantly lebih banyak** dibanding model standard. Alasan: Normalisasi eksplisit antara layer memberikan weight lebih kebebasan bergerak selama optimisasi, karena batch norm re-scale aktivasi.\n",
        "\n",
        "---\n",
        "\n",
        "## Bagian 4: Performance Profiling\n",
        "\n",
        "### Dataset Klasifikasi Bunga\n",
        "\n",
        "Dataset flower 17-kategori (https://www.robots.ox.ac.uk/~vgg/data/flowers) dengan gambar diorganisir berdasarkan numbering filename: 80 gambar pertama = class 0, 80 berikutnya = class 1, dll. Data pipeline membaca file .jpg, extract label dari filename, resize ke 64×64, membuat train/valid split.\n",
        "\n",
        "### Arsitektur CNN Model\n",
        "\n",
        "**Layer**:\n",
        "- Conv2D (64 filter, 5×5, stride 1, ReLU) + BatchNorm + MaxPool (3×3, stride 2)\n",
        "- Conv2D (128 filter, 3×3, stride 1, ReLU) + BatchNorm\n",
        "- Conv2D (256 filter, 3×3, stride 1, ReLU) + BatchNorm\n",
        "- Conv2D (512 filter, 3×3, stride 1, ReLU) + BatchNorm + AvgPool (2×2, stride 2)\n",
        "- Flatten\n",
        "- Dense (512) + LeakyReLU + LayerNorm\n",
        "- Dense (256) + LeakyReLU + LayerNorm\n",
        "- Dense (17, softmax dengan dtype float32)\n",
        "\n",
        "### Prerequisite untuk Profiling\n",
        "\n",
        "**Install Requirement**:\n",
        "1. **Python package**: `pip install tensorboard_plugin_profile`\n",
        "2. **CUDA Profiling Toolkit (libcupti)**:\n",
        "   - **Ubuntu**: `sudo apt-get install libcupti-dev`\n",
        "   - **Windows**: Copy DLL dari CUDA extras folder ke bin/lib directory, enable GPU profiling untuk semua user via NVIDIA Control Panel\n",
        "\n",
        "### Interface Profiling\n",
        "\n",
        "**Enable**: Set `profile_batch=[10, 20]` dalam TensorBoard callback untuk profile batch 10-20.\n",
        "\n",
        "**Metrik Overview Page**:\n",
        "- **Input time**: Operasi pembacaan data (tf.data.Dataset)\n",
        "- **Host compute time**: Komputasi CPU (model-related)\n",
        "- **Device-to-device time**: Transfer data CPU → GPU\n",
        "- **Kernel launch time**: CPU meluncurkan GPU kernel\n",
        "- **Device compute time**: Komputasi GPU (ideal: proporsi tertinggi)\n",
        "- **Device collective communication**: Multi-GPU/multi-node communication\n",
        "- **Other time**: Kompilasi, output, miscellaneous\n",
        "\n",
        "**Informasi Tambahan**:\n",
        "- % TensorFlow ops ditempatkan di host vs device\n",
        "- Data type breakdown (float16 vs float32)\n",
        "- Utilisasi TensorCore (specialized matrix multiplication hardware)\n",
        "\n",
        "**Recommendation Section**: Saran otomatis untuk perbaikan performa berdasarkan hasil profiling.\n",
        "\n",
        "### Optimisasi 1: tf.data Pipeline\n",
        "\n",
        "**Masalah yang Teridentifikasi**:\n",
        "- Input time tinggi (data-bound)\n",
        "- Pembacaan data sequential dari disk\n",
        "\n",
        "**Solusi**:\n",
        "1. **Prefetching**: `dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)` - Pipeline menyiapkan batch berikutnya saat model melatih current batch\n",
        "2. **Parallel map**: `dataset.map(fn, num_parallel_calls=tf.data.AUTOTUNE)` - Decoding gambar di-parallelize\n",
        "3. **Dedicated GPU thread**: Set environment variable `TF_GPU_THREAD_MODE=gpu_private` untuk allocate dedicated CPU thread untuk GPU kernel launch, preventing delay dari CPU contention\n",
        "\n",
        "**Setting Environment Variable**:\n",
        "- **Linux**: `export TF_GPU_THREAD_MODE=gpu_private` di terminal sebelum start notebook server\n",
        "- **Windows**: System setting → Environment variable → Tambah `TF_GPU_THREAD_MODE=gpu_private`\n",
        "- **Conda**: `conda env config vars set TF_GPU_THREAD_MODE=gpu_private`, deactivate/reactivate environment\n",
        "\n",
        "**Kritis**: Restart notebook server setelah set environment variable (child process inherit environment).\n",
        "\n",
        "### Optimisasi 2: Mixed Precision Training\n",
        "\n",
        "**Konsep**: Gunakan 16-bit floating point (float16) untuk komputasi, 32-bit (float32) untuk penyimpanan variable. Benefit:\n",
        "- **Speed**: Operasi float16 lebih cepat di GPU modern dengan Tensor Core\n",
        "- **Memory**: Pengurangan memori 50% (5.48 GB → 1.25 GB dalam contoh)\n",
        "- **Precision preservation**: Variable disimpan sebagai float32 untuk stable gradient update\n",
        "\n",
        "**Enable di Keras**:\n",
        "```python\n",
        "from tensorflow.keras import mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "```\n",
        "\n",
        "**Data Type Distribution**:\n",
        "- **Input**: float16\n",
        "- **Output**: float16\n",
        "- **Variable** (weight, bias): float32\n",
        "- **Komputasi**: float16 dimana possible\n",
        "\n",
        "**Loss Scaling**: Automatically handled oleh Keras. `LossScaleOptimizer` wrap optimizer, scale loss untuk avoid underflow (float16 punya dynamic range lebih kecil dari float32: max 65.504 vs 3.4×10³⁸). Gradient di-scale up saat forward pass, di-scale down saat weight update.\n",
        "\n",
        "**bfloat16**: Alternatif format 16-bit dari Google, same exponent range seperti float32 tapi lower precision. Better untuk TPU dan GPU dengan Tensor Core.\n",
        "\n",
        "### Hasil\n",
        "\n",
        "**Perbaikan Time**: Input time drastis berkurang (data bukan lagi bottleneck) setelah optimisasi pipeline. Device compute time slightly improved dengan mixed precision.\n",
        "\n",
        "**Perbaikan Memory**: Pengurangan 76% (5.48 GB → 1.25 GB) dengan mixed precision training.\n",
        "\n",
        "### Memory Profile View\n",
        "\n",
        "**Heap vs Stack**:\n",
        "- **Stack**: Function call tracking (automatically managed, popped setelah function return)\n",
        "- **Heap**: Object dan variable yang dibuat saat execution (persist sampai garbage collected)\n",
        "\n",
        "**Analysis**: Linear heap growth = tidak ada memory leak. Spike = temporary allocation saat operasi spesifik.\n",
        "\n",
        "**Operation Breakdown Table**: Shows memory usage per TensorFlow operation. Contoh: Dense layer [115200, 512] menggunakan paling banyak memori (first Dense setelah Flatten bottleneck).\n",
        "\n",
        "### Trace Viewer\n",
        "\n",
        "**Visualisasi longitudinal** menunjukkan kapan dan bagaimana operasi dijalankan di CPU/GPU sepanjang waktu. Fitur:\n",
        "- **Timeline**: X-axis = waktu, Y-axis = execution stream berbeda (CPU core, GPU, GPU private thread)\n",
        "- **Operation block**: Color-coded bar mengindikasikan operation type dan duration\n",
        "- **Zoom/pan**: Inspect interval waktu spesifik\n",
        "- **Detail**: Click operasi untuk lihat metadata (duration, input/output shape)\n",
        "\n",
        "**Insight**: Confirm operasi tf.data berjalan di CPU, operasi model di GPU, dedicated thread menangani kernel launch.\n",
        "\n",
        "---\n",
        "\n",
        "## Bagian 5: Visualisasi Word Vector\n",
        "\n",
        "### GloVe Vectors\n",
        "\n",
        "**GloVe (Global Vectors)** dari Stanford: Pretrained word embedding yang dipelajari dari large corpus. Berbeda dengan Skip-gram/CBOW (local context only), GloVe menggabungkan global co-occurrence statistic.\n",
        "\n",
        "**Variant**:\n",
        "- Wikipedia 2014 + Gigaword 5: 6B token, 400K vocab, 50/100/200/300D\n",
        "- Common Crawl (42B): 42B token, 1.9M vocab, 300D\n",
        "- Common Crawl (840B): 840B token, 2.2M vocab, 300D (cased)\n",
        "- Twitter: 2B tweet, 27B token, 1.2M vocab, 25/50/100/200D\n",
        "\n",
        "Bab menggunakan versi Wikipedia 50D (smallest).\n",
        "\n",
        "### Data Pipeline\n",
        "\n",
        "1. **Load GloVe**: `pd.read_csv('glove.6B.50d.txt')` return DataFrame dengan word index, 50 embedding column\n",
        "2. **Load IMDB review**: `tfds.load('imdb_reviews')` untuk corpus\n",
        "3. **Extract vocabulary**: Counter untuk find 5.000 word paling common dalam review\n",
        "4. **Find intersection**: `df.loc[df.index.isin(most_common_words)]` berikan ~3.600 token common\n",
        "\n",
        "### Saving untuk TensorBoard\n",
        "\n",
        "**Langkah**:\n",
        "1. **Create variable**: `weights = tf.Variable(df_common.values)` (shape [vocab_size, embedding_dim])\n",
        "2. **Save checkpoint**: `tf.train.Checkpoint(embedding=weights).save(path)` save weight\n",
        "3. **Save metadata**: TSV file dengan satu token per baris (row i = token untuk embedding row i)\n",
        "4. **Configure projector**:\n",
        "   ```python\n",
        "   config = projector.ProjectorConfig()\n",
        "   embedding = config.embeddings.add()\n",
        "   embedding.metadata_path = 'metadata.tsv'\n",
        "   projector.visualize_embeddings(log_dir, config)\n",
        "   ```\n",
        "\n",
        "**ProjectorConfig**: Mengandung model_checkpoint_directory, embedding configuration.\n",
        "\n",
        "**EmbeddingInfo**: Mengandung tensor_name, metadata_path untuk label.\n",
        "\n",
        "### Dimensionality Reduction\n",
        "\n",
        "**Problem**: Vektor 50D tidak bisa divisualisasikan langsung. Perlu proyeksi ke space 2D/3D.\n",
        "\n",
        "**Algoritma**:\n",
        "- **PCA (Principal Component Analysis)**: Proyeksi linear yang maksimalkan variance\n",
        "- **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: Non-linear, preserve local structure\n",
        "- **UMAP (Uniform Manifold Approximation and Projection)**: Cepat, preserve both local dan global structure\n",
        "\n",
        "TensorBoard menyediakan ketiga dengan adjustable hyperparameter (perplexity untuk t-SNE, neighbor untuk UMAP, dll).\n",
        "\n",
        "### Fitur Visualisasi\n",
        "\n",
        "**Dashboard Control**:\n",
        "- **Dimensionality**: Toggle 2D ↔ 3D visualization\n",
        "- **Algorithm selection**: Pilih PCA/t-SNE/UMAP dengan parameter slider\n",
        "- **Hover inspection**: Mouse over point reveal word label\n",
        "- **Search**: Regex-based search highlight match\n",
        "\n",
        "**Contoh Analysis**: Regex `(?:fred|larry|mrs\\.|mr\\.|michelle|comedy|theater)` highlight nama orang dan movie-related term, showing semantic clustering (nama orang cluster bersama, genre word cluster terpisah).\n",
        "\n",
        "---\n",
        "\n",
        "## Program-Program Implementasi\n",
        "\n",
        "### Program 1: Logging Gambar ke TensorBoard\n",
        "\n",
        "```python\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "\n",
        "# Setup logging directory dengan timestamp\n",
        "log_timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "image_logdir = f\"./logs/data_{log_timestamp}/train\"\n",
        "\n",
        "# Create writer\n",
        "image_writer = tf.summary.create_file_writer(image_logdir)\n",
        "\n",
        "# Log gambar individual dengan class label\n",
        "id2label_map = {\n",
        "    0: \"Kaos/atas\", 1: \"Celana\", 2: \"Pullover\", 3: \"Gaun\",\n",
        "    4: \"Mantel\", 5: \"Sandal\", 6: \"Kemeja\", 7: \"Sneaker\",\n",
        "    8: \"Tas\", 9: \"Boot pergelangan kaki\"\n",
        "}\n",
        "\n",
        "with image_writer.as_default():\n",
        "    # Log 10 gambar individual dengan grouping berbasis class\n",
        "    for data in fashion_ds[\"train\"].batch(1).take(10):\n",
        "        label = int(data[\"label\"].numpy())\n",
        "        tf.summary.image(\n",
        "            id2label_map[label],  # Tag dengan nama class\n",
        "            data[\"image\"],\n",
        "            max_outputs=1,\n",
        "            step=0\n",
        "        )\n",
        "    \n",
        "    # Log batch 20 gambar bersama\n",
        "    for data in fashion_ds[\"train\"].batch(20).take(1):\n",
        "        tf.summary.image(\n",
        "            \"Training batch\",\n",
        "            data[\"image\"],\n",
        "            max_outputs=20,\n",
        "            step=0\n",
        "        )\n",
        "\n",
        "# Luncurkan TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs --port 6006\n",
        "```\n",
        "\n",
        "**Penjelasan**: Writer dibuat dengan direktori log yang ditimestamp. Gambar individual di-log dengan class label sebagai tag (dikelompokkan berdasarkan class di dashboard). Batch 20 ditampilkan bersama. TensorBoard diluncurkan inline di Jupyter notebook.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 2: Training Model dengan TensorBoard Callback\n",
        "\n",
        "```python\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Define model\n",
        "dense_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "dense_model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "conv_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (5,5), strides=2, padding='same',\n",
        "                           activation='relu', input_shape=(28,28,1)),\n",
        "    tf.keras.layers.Conv2D(16, (3,3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "conv_model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Setup direktori log terpisah\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "dense_log_dir = os.path.join(\"logs\", f\"dense_{timestamp}\")\n",
        "conv_log_dir = os.path.join(\"logs\", f\"conv_{timestamp}\")\n",
        "\n",
        "# Siapkan data\n",
        "batch_size = 64\n",
        "tr_ds_flat, v_ds_flat, _ = get_train_valid_test_datasets(\n",
        "    fashion_ds, batch_size, flatten_images=True\n",
        ")\n",
        "tr_ds_img, v_ds_img, _ = get_train_valid_test_datasets(\n",
        "    fashion_ds, batch_size, flatten_images=False\n",
        ")\n",
        "\n",
        "# Train dense model\n",
        "dense_tb = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=dense_log_dir,\n",
        "    histogram_freq=0,  # Tidak ada histogram\n",
        "    profile_batch=0    # Tidak ada profiling\n",
        ")\n",
        "dense_model.fit(tr_ds_flat, validation_data=v_ds_flat,\n",
        "                epochs=10, callbacks=[dense_tb])\n",
        "\n",
        "# Train CNN dengan activation histogram\n",
        "conv_tb = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=conv_log_dir,\n",
        "    histogram_freq=2,  # Log histogram setiap 2 epoch\n",
        "    profile_batch=0\n",
        ")\n",
        "conv_model.fit(tr_ds_img, validation_data=v_ds_img,\n",
        "               epochs=10, callbacks=[conv_tb])\n",
        "\n",
        "# View hasil (kedua run ditampilkan di same dashboard)\n",
        "%tensorboard --logdir ./logs\n",
        "```\n",
        "\n",
        "**Penjelasan**: Dua model di-train dengan direktori log terpisah, enabling side-by-side comparison. Dense model track hanya metrik, CNN track metrik + activation histogram setiap 2 epoch. Run yang ditimestamp prevent directory conflict. TensorBoard automatically detect dan display kedua run.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 3: Custom Metric Logging\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define model (standard vs batch norm)\n",
        "dense_standard = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(256, activation='relu', name='log_layer'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "dense_standard.compile(loss='sparse_categorical_crossentropy',\n",
        "                       optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "dense_bn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(256, activation='relu', name='log_layer_bn'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "dense_bn.compile(loss='sparse_categorical_crossentropy',\n",
        "                 optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Custom training function\n",
        "def train_model(model, dataset, log_dir, log_layer_name, epochs):\n",
        "    writer = tf.summary.create_file_writer(log_dir)\n",
        "    step = 0\n",
        "    \n",
        "    with writer.as_default():\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            for batch in dataset:\n",
        "                # Training step\n",
        "                model.train_on_batch(*batch)\n",
        "                \n",
        "                # Extract weight dari target layer\n",
        "                weights = model.get_layer(log_layer_name).get_weights()[0]\n",
        "                \n",
        "                # Hitung dan log statistik\n",
        "                tf.summary.scalar('mean_weights',\n",
        "                                 np.mean(np.abs(weights)), step=step)\n",
        "                tf.summary.scalar('std_weights',\n",
        "                                 np.std(np.abs(weights)), step=step)\n",
        "                \n",
        "                # Flush ke disk\n",
        "                writer.flush()\n",
        "                step += 1\n",
        "\n",
        "# Train kedua model\n",
        "tr_ds, _, _ = get_train_valid_test_datasets(\n",
        "    fashion_ds, batch_size=64, flatten_images=True\n",
        ")\n",
        "\n",
        "train_model(dense_standard, tr_ds, 'logs/standard', 'log_layer', 5)\n",
        "train_model(dense_bn, tr_ds, 'logs/bn', 'log_layer_bn', 5)\n",
        "```\n",
        "\n",
        "**Penjelasan**: Custom training loop manually log weight statistic per batch. `train_on_batch()` perform single gradient update, weight extracted dari layer spesifik, statistik dihitung dan di-log via tf.summary.scalar(). Writer.flush() ensure immediate disk write. Hasil menunjukkan batch norm increase weight variability.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 4: Profiling Performa dan Optimisasi\n",
        "\n",
        "```python\n",
        "import os\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# Optimized data pipeline\n",
        "def get_flower_datasets_optimized(image_dir, batch_size):\n",
        "    dataset = tf.data.Dataset.list_files(\n",
        "        os.path.join(image_dir, '*.jpg'), shuffle=False\n",
        "    )\n",
        "    \n",
        "    def get_image_and_label(file_path):\n",
        "        tokens = tf.strings.split(file_path, os.path.sep)\n",
        "        label = (tf.strings.to_number(\n",
        "            tf.strings.split(tf.strings.split(tokens[-1], '.')[0], '_')[-1]\n",
        "        ) - 1) // 80\n",
        "        \n",
        "        img = tf.io.read_file(file_path)\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "        return tf.image.resize(img, [64, 64]), label\n",
        "    \n",
        "    # Parallel map + prefetch optimization\n",
        "    dataset = dataset.map(\n",
        "        get_image_and_label,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).shuffle(400)\n",
        "    \n",
        "    valid_ds = dataset.take(250).batch(batch_size)\n",
        "    train_ds = dataset.skip(250).batch(batch_size).prefetch(\n",
        "        tf.data.AUTOTUNE\n",
        "    )\n",
        "    \n",
        "    return train_ds, valid_ds\n",
        "\n",
        "# Enable mixed precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "# Rebuild model dengan mixed precision\n",
        "conv_model = get_cnn_model()  # Sama arsitektur\n",
        "\n",
        "# Verify data type\n",
        "print(f\"Input dtype: {conv_model.layers[0].input.dtype}\")       # float16\n",
        "print(f\"Variable dtype: {conv_model.layers[0].trainable_variables[0].dtype}\")  # float32\n",
        "print(f\"Output dtype: {conv_model.layers[0].output.dtype}\")     # float16\n",
        "\n",
        "# Profile optimized model\n",
        "tr_ds, v_ds = get_flower_datasets_optimized('data/17flowers/jpg', batch_size=32)\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir='logs/optimized',\n",
        "    profile_batch=[10, 20]\n",
        ")\n",
        "\n",
        "conv_model.fit(tr_ds, validation_data=v_ds, epochs=2, callbacks=[tb_callback])\n",
        "\n",
        "# Bandingkan profile: pengurangan memori 76% (5.48GB → 1.25GB)\n",
        "```\n",
        "\n",
        "**Penjelasan**: Optimisasi include parallel map (decode gambar simultaneously), prefetch (siapkan batch saat model train), dan mixed precision (float16 ops, float32 variable). Environment variable TF_GPU_THREAD_MODE set terpisah. Profiling menunjukkan pengurangan memori dramatis dan perbaikan input time. Trace viewer confirm GPU utilization optimization.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 5: Visualisasi Word Vector\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from collections import Counter\n",
        "from tensorboard.plugins import projector\n",
        "\n",
        "# Load GloVe vector\n",
        "df = pd.read_csv(\n",
        "    'data/glove.6B.50d.txt',\n",
        "    header=None,\n",
        "    index_col=0,\n",
        "    sep=None,\n",
        "    error_bad_lines=False,\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "# Load IMDB review dan extract vocabulary\n",
        "review_ds = tfds.load('imdb_reviews')\n",
        "corpus = []\n",
        "for data in review_ds[\"train\"]:\n",
        "    text = str(np.char.decode(data[\"text\"].numpy(), encoding='utf-8')).lower()\n",
        "    corpus.append(text)\n",
        "\n",
        "corpus_text = \" \".join(corpus)\n",
        "counter = Counter(corpus_text.split())\n",
        "most_common_words = [w for w, _ in counter.most_common(5000)]\n",
        "\n",
        "# Temukan token common\n",
        "df_common = df.loc[df.index.isin(most_common_words)]\n",
        "print(f\"Token common: {len(df_common)}\")  # ~3.600\n",
        "\n",
        "# Save embedding dan metadata\n",
        "log_dir = 'logs/embeddings'\n",
        "weights = tf.Variable(df_common.values)\n",
        "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
        "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
        "\n",
        "# Save metadata (token)\n",
        "with open(os.path.join(log_dir, 'metadata.tsv'), 'w') as f:\n",
        "    for word in df_common.index:\n",
        "        f.write(word + '\\n')\n",
        "\n",
        "# Configure projector\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "embedding.metadata_path = 'metadata.tsv'\n",
        "projector.visualize_embeddings(log_dir, config)\n",
        "\n",
        "# Luncurkan TensorBoard (must gunakan exact directory)\n",
        "%tensorboard --logdir logs/embeddings/ --port 6007\n",
        "\n",
        "# Contoh regex search: highlight word spesifik\n",
        "# (?:fred|larry|comedy|theater|loving|sadistic|marvelous)\n",
        "```\n",
        "\n",
        "**Penjelasan**: Vektor GloVe di-load sebagai DataFrame, IMDB vocabulary extracted via Counter, intersection dihitung. Embedding di-save sebagai TensorFlow checkpoint, metadata TSV contain token yang sesuai. Projector dikonfigurasi dan divisualisasikan. Dashboard provide PCA/t-SNE/UMAP dengan adjustable parameter, hover inspection, regex search highlighting. Relationship semantik visible (word similar cluster bersama).\n",
        "\n",
        "---\n",
        "\n",
        "## Kesimpulan\n",
        "\n",
        "TensorBoard adalah tool indispensable untuk modern ML development, menyediakan comprehensive visibility ke training process. Kapabilitas key: real-time metrics tracking enable early detection training issue, activation histogram reveal layer convergence pattern, profiling identify bottleneck (memory, computation, data pipeline), custom logging support arbitrary metrik via tf.summary API, dan embedding visualization enable high-dimensional data analysis. Optimisasi yang dipelajari (mixed precision training, parallel data loading, dedicated GPU thread) demonstrate bagaimana profiling insight langsung translate ke actionable improvement. TensorBoard extensibility (HParams untuk hyperparameter optimization, Debugger v2 untuk numerical issue, What-If Tool untuk interpretability) menjadikan comprehensive platform untuk full ML lifecycle—dari data exploration hingga production monitoring.\n",
        "Z"
      ],
      "metadata": {
        "id": "UkL4InAlVVzB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LPlYaWQBVXsd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}