{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BAB 10: Language Modeling dengan TensorFlow\n",
        "\n",
        "## Ringkasan\n",
        "\n",
        "Bab ini membahas **language modeling**, yaitu tugas memprediksi kata berikutnya berdasarkan sequence kata sebelumnya. Ini adalah dasar dari model NLP modern seperti BERT dan GPT. Model yang dibangun akan menggunakan dataset cerita anak-anak bAbI, dengan teknik n-gram untuk mengurangi vocabulary size, GRU sebagai arsitektur model, dan beam search untuk meningkatkan kualitas teks yang dihasilkan.\n",
        "\n",
        "---\n",
        "\n",
        "## Konsep Utama\n",
        "\n",
        "### Language Modeling: Definisi dan Aplikasi\n",
        "\n",
        "Language modeling menghitung probabilitas P(w_n | w₁, w₂, ..., w_{n-1}) - yaitu, berapa kemungkinan kata w_n muncul setelah sequence kata sebelumnya. Contoh praktis adalah saat Anda menulis pesan di smartphone dan keyboard menyarankan kata berikutnya. Ini adalah supervised learning task tanpa label eksplisit - kita hanya perlu melatih model untuk memprediksi next word dari previous words.\n",
        "\n",
        "### Markov Property\n",
        "\n",
        "Menghitung probabilitas dengan melihat seluruh history computationally infeasible untuk teks panjang. Markov Property menawarkan solusi: kita bisa mengaproksimasi probabilitas dengan hanya melihat k kata terakhir, bukan seluruh history. Formula: P(w_n | w₁, w₂, ..., w_{n-1}) ≈ P(w_n | w_k, w_{k+1}, ..., w_{n-1}). Ini adalah trade-off antara akurasi dan efisiensi komputasi.\n",
        "\n",
        "### N-Grams: Mengatasi Vocabulary Besar\n",
        "\n",
        "Dataset bAbI memiliki 14,473 unique words (yang muncul minimal 10 kali). Ini membuat final softmax layer sangat besar dan computationally expensive. **N-grams** mengatasi ini dengan memecah teks menjadi sub-unit karakter dengan panjang tetap n.\n",
        "\n",
        "Contoh bigrams dari \"I went to\":\n",
        "- \"I \", \" w\", \"we\", \"en\", \"nt\", \"t \", \" t\", \"to\", \"o \"\n",
        "\n",
        "Dengan menggunakan bigrams, vocabulary size turun drastis dari 14,473 menjadi hanya 735! Keuntungan n-grams:\n",
        "1. **Vocabulary size reduction** - Signifikan mengurangi parameters\n",
        "2. **Fewer OOV words** - Kata baru bisa dikonstruksi dari n-grams yang dikenal\n",
        "3. **Better generalization** - Model belajar pola di level granular\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset dan Preprocessing\n",
        "\n",
        "### Dataset bAbI\n",
        "\n",
        "bAbI adalah dataset cerita anak-anak dari Facebook Research yang ideal untuk language modeling karena:\n",
        "- Bahasa sederhana dan tidak kompleks\n",
        "- Struktur naratif yang jelas\n",
        "- Vocabulary terbatas dan repetitif\n",
        "- Dataset berisi 98 training stories, 5 validation stories, 5 test stories\n",
        "\n",
        "Setiap cerita diawali dengan marker \"_BOOK_TITLE_\" yang memungkinkan pemisahan cerita individual.\n",
        "\n",
        "### Tokenization\n",
        "\n",
        "Setelah mengkonversi text menjadi n-grams, kita perlu mengubahnya menjadi numerical IDs karena neural networks hanya memproses numbers. **Keras Tokenizer** melakukan ini dalam dua fase:\n",
        "\n",
        "1. **Fitting phase**: Membaca semua training data dan membangun vocabulary dictionary yang memetakan n-gram ke integer ID\n",
        "2. **Transformation phase**: Menggunakan dictionary untuk mengkonversi sequences of n-grams menjadi sequences of IDs\n",
        "\n",
        "Penting: Tokenizer hanya di-fit pada training data untuk menghindari data leakage.\n",
        "\n",
        "### TensorFlow Data Pipeline\n",
        "\n",
        "Pipeline mengubah sequence panjang arbitrary menjadi fixed-size windows yang bisa diproses neural network. Prosesnya:\n",
        "\n",
        "1. Convert text ke sequences of IDs\n",
        "2. Gunakan `tf.data.Dataset.window()` untuk membuat windowed sequences\n",
        "3. Gunakan `flat_map()` untuk menghilangkan nested structure\n",
        "4. Batch data\n",
        "5. Split menjadi input-target pairs (target = input shifted right by 1)\n",
        "\n",
        "Alasan menggunakan window size `n_seq + 1`: kita butuh extra element agar target dapat digenerate dengan benar (input dan target harus same length).\n",
        "\n",
        "---\n",
        "\n",
        "## GRU: Gated Recurrent Unit\n",
        "\n",
        "### Perbedaan LSTM dan GRU\n",
        "\n",
        "**GRU** adalah simplifikasi dari LSTM yang tetap mempertahankan performance. Perbedaan utama:\n",
        "\n",
        "| Aspek | LSTM | GRU |\n",
        "|-------|------|-----|\n",
        "| **States** | Dual (cell state, output state) | Single (hidden state) |\n",
        "| **Gates** | 3 gates (input, forget, output) | 2 gates (update, reset) |\n",
        "| **Complexity** | Lebih kompleks, training lebih lambat | Lebih sederhana, training lebih cepat |\n",
        "| **Performance** | Sedikit lebih baik untuk long dependencies | Comparable performance dengan LSTM |\n",
        "\n",
        "### GRU Mechanics\n",
        "\n",
        "GRU memiliki dua gates dan mengikuti persamaan ini:\n",
        "\n",
        "1. **Reset gate**: \\( r_t = σ(W_{rh}h_{t-1} + W_{rx}x_t + b_r) \\)\n",
        "   - Mengontrol berapa banyak previous state yang di-reset\n",
        "   - Nilai 0 = reset semuanya, 1 = pertahankan semuanya\n",
        "\n",
        "2. **Update gate**: \\( z_t = σ(W_{zh}h_{t-1} + W_{zx}x_t + b_z) \\)\n",
        "   - Mengontrol balance antara previous state dan new information\n",
        "\n",
        "3. **New state candidate**: \\( \\tilde{h}_t = tanh(W_h(r_t ⊙ h_{t-1}) + W_xx_t + b) \\)\n",
        "   - Compute candidate untuk new state\n",
        "   - Perhatikan perkalian elemen-wise dengan reset gate\n",
        "\n",
        "4. **Final state**: \\( h_t = z_t ⊙ h_{t-1} + (1 - z_t) ⊙ \\tilde{h}_t \\)\n",
        "   - Weighted average antara previous state dan new candidate\n",
        "   - Controlled by update gate\n",
        "\n",
        "GRU cocok untuk language modeling karena: proses sequential, dapat handle variable-length context, dan computational efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluasi Model: Perplexity\n",
        "\n",
        "### Apa itu Perplexity?\n",
        "\n",
        "**Perplexity** adalah metric untuk mengukur seberapa baik language model. Secara intuitif, perplexity mengukur \"berapakah jumlah equally likely choices yang model pikir ada untuk next word.\"\n",
        "\n",
        "Perplexity dihitung sebagai exponential dari cross-entropy loss:\n",
        "\\[ Perplexity = e^{H(X)} \\]\n",
        "\n",
        "Di mana H(X) adalah entropy (uncertainty) dari distribusi probabilitas model.\n",
        "\n",
        "### Intuisi Perplexity\n",
        "\n",
        "- **Perplexity rendah** (misal, 2): Model sangat confident, hanya 2 equally likely candidates\n",
        "- **Perplexity tinggi** (misal, 1000): Model tidak confident, banyak equally likely candidates\n",
        "\n",
        "**Contoh**: Untuk kalimat \"I went swimming in the ____\":\n",
        "- Model yang prediksi {pool: 0.8, sea: 0.15, lake: 0.05} → Perplexity ≈ 1.5 (confident)\n",
        "- Model yang prediksi {10 words, each 0.1} → Perplexity ≈ 10 (uncertain)\n",
        "\n",
        "Model pertama lebih baik - lebih percaya diri dan fokus pada pilihan yang masuk akal.\n",
        "\n",
        "### Perplexity vs Accuracy\n",
        "\n",
        "Mengapa tidak gunakan accuracy saja? Karena accuracy terlalu strict - jika true word adalah \"dog\" tapi model prediksi \"cat\", accuracy = 0%, padahal \"cat\" dan \"dog\" semantically similar. Perplexity lebih nuanced - bahkan jika tidak exactly correct, jika model assign reasonable probability ke similar words, perplexity tetap moderate.\n",
        "\n",
        "---\n",
        "\n",
        "## Text Generation Strategies\n",
        "\n",
        "### Greedy Decoding\n",
        "\n",
        "**Greedy decoding** adalah approach paling sederhana: di setiap timestep, pilih word dengan probabilitas tertinggi sebagai next word, gunakan sebagai input untuk timestep berikutnya.\n",
        "\n",
        "**Kelemahan:**\n",
        "1. **Repetitive text** - Model bisa stuck dalam loops (\"said the king said the king...\")\n",
        "2. **Myopic** - Hanya melihat satu step ahead, tidak bisa long-term planning\n",
        "3. **No exploration** - Tidak explore alternative paths yang mungkin lebih baik\n",
        "\n",
        "Mitigasi sederhana: Occasionally random pilih dari top-3 predictions instead of top-1 untuk break repetitive patterns.\n",
        "\n",
        "### Beam Search\n",
        "\n",
        "**Beam search** adalah strategi yang jauh lebih sophisticated. Daripada commit ke single best choice setiap step, beam search explore multiple promising paths simultaneously.\n",
        "\n",
        "**Parameters:**\n",
        "- **Beam width (k)**: Number of top candidates to explore (misal, k=3)\n",
        "- **Beam depth (d)**: How many steps ahead to look (misal, d=5)\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1. **Timestep 1**: Dari semua vocabulary, keep top-3 candidates\n",
        "2. **Timestep 2**: Untuk setiap dari 3 candidates, generate top-3 next words → 9 total candidates\n",
        "3. **Pruning**: Keep hanya top-3 dari 9 berdasarkan joint probability\n",
        "4. **Repeat**: Continue untuk d timesteps\n",
        "5. **Final selection**: Choose sequence dengan highest joint probability\n",
        "\n",
        "**Keuntungan:**\n",
        "- Explore multiple paths, tidak stuck di single path\n",
        "- Better long-term planning\n",
        "- Empirically menghasilkan teks lebih coherent dan less repetitive\n",
        "- Tradeof: Computational cost lebih tinggi (k^d possibilities explored)\n",
        "\n",
        "**Diverse beam search**: Enhancement dari standard beam search dengan menambahkan diversity term untuk encourage truly different paths, bukan hanya variations dari same idea.\n",
        "\n",
        "---\n",
        "\n",
        "## Program-Program Implementasi\n",
        "\n",
        "### Program 1: Download dan Extract Dataset\n",
        "\n",
        "```python\n",
        "import os\n",
        "import requests\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "# Cek apakah file sudah ada di local disk\n",
        "if not os.path.exists(os.path.join('data', 'lm','CBTest.tgz')):\n",
        "    \n",
        "    # Download dari server\n",
        "    url = \"http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz\"\n",
        "    r = requests.get(url)\n",
        "    \n",
        "    # Buat direktori jika belum ada\n",
        "    if not os.path.exists(os.path.join('data','lm')):\n",
        "        os.makedirs(os.path.join('data','lm'), exist_ok=True)\n",
        "    \n",
        "    # Write binary file\n",
        "    with open(os.path.join('data', 'lm', 'CBTest.tgz'), 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    print(\"✓ File downloaded\")\n",
        "else:\n",
        "    print(\"✓ File already exists, skip download\")\n",
        "\n",
        "# Extract compressed archive\n",
        "if not os.path.exists(os.path.join('data', 'lm', 'CBTest')):\n",
        "    with tarfile.open(os.path.join(\"data\",\"lm\",\"CBTest.tgz\"), 'r') as tarf:\n",
        "        tarf.extractall(os.path.join(\"data\",\"lm\"))\n",
        "    print(\"✓ File extracted\")\n",
        "else:\n",
        "    print(\"✓ File already extracted\")\n",
        "```\n",
        "\n",
        "**Penjelasan**: Program ini download dataset dari server (dengan caching - jika sudah ada, tidak download lagi) dan mengextract compressed archive. Ini adalah idempotent - running multiple times tidak ada masalah.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 2: Membaca Stories\n",
        "\n",
        "```python\n",
        "def read_data(path):\n",
        "    \"\"\"\n",
        "    Baca stories dari text file.\n",
        "    Dataset structure: setiap story diawali dengan \"_BOOK_TITLE_\"\n",
        "    \"\"\"\n",
        "    stories = []     # Hold all completed stories\n",
        "    s = []           # Hold lines dari current story\n",
        "    \n",
        "    with open(path, 'r') as f:\n",
        "        for row in f:\n",
        "            # Cek jika ini marker untuk story baru\n",
        "            if row.startswith(\"_BOOK_TITLE_\"):\n",
        "                # Jika ada story yang sudah terakumulasi, add ke list\n",
        "                if len(s) > 0:\n",
        "                    stories.append(' '.join(s).lower())\n",
        "                # Reset untuk story baru\n",
        "                s = []\n",
        "            # Append baris saat ini ke current story\n",
        "            s.append(row)\n",
        "    \n",
        "    # Handle edge case: final story belum ditambahkan\n",
        "    if len(s) > 0:\n",
        "        stories.append(' '.join(s).lower())\n",
        "    \n",
        "    return stories\n",
        "\n",
        "# Load datasets\n",
        "stories = read_data(os.path.join('data','lm','CBTest','data','cbt_train.txt'))\n",
        "val_stories = read_data(os.path.join('data','lm','CBTest','data','cbt_valid.txt'))\n",
        "test_stories = read_data(os.path.join('data','lm','CBTest','data','cbt_test.txt'))\n",
        "\n",
        "print(f\"Loaded {len(stories)} training stories\")\n",
        "```\n",
        "\n",
        "**Penjelasan**: Function ini menggunakan state machine pattern untuk parse file structure. Setiap kali ketemu \"_BOOK_TITLE_\", program close previous story dan start new one. Ini handle edge case untuk final story yang belum diproses.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 3: Generate N-Grams\n",
        "\n",
        "```python\n",
        "def get_ngrams(text, n):\n",
        "    \"\"\"\n",
        "    Generate n-grams dari text\n",
        "    Menggunakan non-overlapping approach (stride = n)\n",
        "    \"\"\"\n",
        "    return [text[i:i+n] for i in range(0, len(text), n)]\n",
        "\n",
        "# Test\n",
        "test_string = \"I like chocolates\"\n",
        "print(\"Original:\", test_string)\n",
        "for i in range(1, 4):\n",
        "    print(f\"{i}-grams: {get_ngrams(test_string, i)}\")\n",
        "\n",
        "# Analyze vocabulary size dengan bigrams\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "ngrams = 2\n",
        "text = chain(*[get_ngrams(s, ngrams) for s in stories])\n",
        "cnt = Counter(text)\n",
        "\n",
        "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
        "\n",
        "n_vocab = (freq_df >= 10).sum()\n",
        "print(f\"Vocabulary size (frequency >= 10): {n_vocab}\")\n",
        "# Output: ~735 (vs 14,473 words - huge reduction!)\n",
        "```\n",
        "\n",
        "**Penjelasan**: N-grams diproduksi dengan sliding window. Program ini menunjukkan bagaimana bigrams sangat reduce vocabulary size dari ~14K words menjadi ~735 n-grams. Ini adalah massive reduction yang membuat model lebih efficient.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 4: Keras Tokenizer\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Generate n-grams dari semua training stories\n",
        "train_ngram_stories = [get_ngrams(s, 2) for s in stories]\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
        "\n",
        "# Fit HANYA pada training data (penting: avoid data leakage)\n",
        "tokenizer.fit_on_texts(train_ngram_stories)\n",
        "\n",
        "# Convert text ke sequences of IDs\n",
        "train_data_seq = tokenizer.texts_to_sequences(train_ngram_stories)\n",
        "val_ngram_stories = [get_ngrams(s, 2) for s in val_stories]\n",
        "val_data_seq = tokenizer.texts_to_sequences(val_ngram_stories)\n",
        "test_ngram_stories = [get_ngrams(s, 2) for s in test_stories]\n",
        "test_data_seq = tokenizer.texts_to_sequences(test_ngram_stories)\n",
        "\n",
        "# Check mapping\n",
        "print(f\"Bigram 'th' -> ID {tokenizer.word_index.get('th')}\")\n",
        "print(f\"ID 2 -> Bigram '{tokenizer.index_word.get(2)}'\")\n",
        "\n",
        "# Example: original vs tokenized\n",
        "print(\"\\nOriginal bigrams:\", test_ngram_stories[0][:10])\n",
        "print(\"Tokenized IDs:\", test_data_seq[0][:10])\n",
        "```\n",
        "\n",
        "**Penjelasan**: Tokenizer build dictionary yang map setiap n-gram ke unique integer ID. Penting fit HANYA pada training data untuk avoid data leakage. Output adalah sequences of IDs yang siap diproses neural network.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 5: TensorFlow Data Pipeline\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_tf_pipeline(data_seq, n_seq, batch_size=64, shift=1, shuffle=True):\n",
        "    \"\"\"\n",
        "    Buat tf.data pipeline yang convert sequences ke fixed-length windows.\n",
        "    Window size = n_seq + 1 (extra 1 untuk generate target)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create dataset dari ragged tensor (variable-length sequences)\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq))\n",
        "    \n",
        "    # Shuffle di level story\n",
        "    if shuffle:\n",
        "        text_ds = text_ds.shuffle(buffer_size=len(data_seq)//2)\n",
        "    \n",
        "    # Flat map untuk window creation\n",
        "    # This adalah complex nested operation - dijelaskan detail di atas\n",
        "    text_ds = text_ds.flat_map(\n",
        "        lambda x: tf.data.Dataset.from_tensor_slices(x).window(\n",
        "            n_seq+1, shift=shift\n",
        "        ).flat_map(\n",
        "            lambda window: window.batch(n_seq+1, drop_remainder=True)\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Shuffle di level window\n",
        "    if shuffle:\n",
        "        text_ds = text_ds.shuffle(buffer_size=10*batch_size)\n",
        "    \n",
        "    # Batch data\n",
        "    text_ds = text_ds.batch(batch_size)\n",
        "    \n",
        "    # Split setiap sequence ke input dan target\n",
        "    # Target = input shifted right by 1\n",
        "    text_ds = text_ds.map(lambda x: (x[:,:-1], x[:, 1:])).prefetch(\n",
        "        buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    \n",
        "    return text_ds\n",
        "\n",
        "# Create pipelines\n",
        "batch_size = 64\n",
        "train_ds = get_tf_pipeline(train_data_seq, n_seq=100, batch_size=batch_size, shuffle=True)\n",
        "valid_ds = get_tf_pipeline(val_data_seq, n_seq=100, batch_size=batch_size, shuffle=False)\n",
        "test_ds = get_tf_pipeline(test_data_seq, n_seq=100, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Verify pipeline output\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"Input shape: {inputs.shape}\")\n",
        "    print(f\"Target shape: {targets.shape}\")\n",
        "    print(f\"Sample input: {inputs[0][:5]}\")\n",
        "    print(f\"Sample target: {targets[0][:5]}\")\n",
        "```\n",
        "\n",
        "**Penjelasan**: Pipeline menggunakan windowing untuk membuat fixed-size sequences dari variable-length stories. Proses ini complex karena nested operations, tapi hasil akhirnya adalah batch dari (input, target) pairs di mana target adalah input shifted right by 1.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 6: GRU Model Architecture\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "# Model definition\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Embedding layer - convert IDs ke dense vectors\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=n_vocab,      # Vocabulary size\n",
        "        output_dim=128,         # Embedding dimension\n",
        "        mask_zero=True,         # Mask padding zeros\n",
        "        input_shape=(n_seq,)\n",
        "    ),\n",
        "    \n",
        "    # GRU layer - process sequence\n",
        "    tf.keras.layers.GRU(\n",
        "        units=1024,             # Hidden state dimension\n",
        "        return_sequences=True   # Return all timesteps\n",
        "    ),\n",
        "    \n",
        "    # Dense layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    \n",
        "    # Output layer - predict next n-gram ID\n",
        "    tf.keras.layers.Dense(n_vocab)  # No activation (raw logits)\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=[PerplexityMetric()]  # Custom metric\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "**Penjelasan**: Model menggunakan Embedding layer untuk convert IDs ke dense vectors, GRU untuk process sequences, dan Dense output layer untuk predict next n-gram ID. Loss adalah SparseCategoricalCrossentropy karena target adalah single integer ID, bukan one-hot vector.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 7: Custom Perplexity Metric\n",
        "\n",
        "```python\n",
        "class PerplexityMetric(tf.keras.metrics.Mean):\n",
        "    \"\"\"Custom metric untuk menghitung perplexity\"\"\"\n",
        "    \n",
        "    def __init__(self, name='perplexity', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "    \n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Compute cross-entropy loss\n",
        "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=tf.cast(tf.reshape(y_true, [-1]), tf.int32),\n",
        "            logits=tf.reshape(y_pred, [-1, n_vocab])\n",
        "        )\n",
        "        \n",
        "        # Compute perplexity = exp(loss)\n",
        "        perplexity = tf.exp(loss)\n",
        "        \n",
        "        # Update metric\n",
        "        super().update_state(perplexity, sample_weight)\n",
        "\n",
        "# Usage\n",
        "metrics = [PerplexityMetric()]\n",
        "```\n",
        "\n",
        "**Penjelasan**: Custom metric menghitung perplexity sebagai exponential dari cross-entropy loss. Lower perplexity = better model.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 8: Training dengan Callbacks\n",
        "\n",
        "```python\n",
        "# Define callbacks\n",
        "callbacks = [\n",
        "    # Early stopping jika validation perplexity tidak improve\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_perplexity',\n",
        "        mode='min',\n",
        "        patience=5,\n",
        "        verbose=1\n",
        "    ),\n",
        "    \n",
        "    # Reduce learning rate jika validation perplexity plateau\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_perplexity',\n",
        "        mode='min',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        verbose=1\n",
        "    ),\n",
        "    \n",
        "    # Save best model\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        'best_model.h5',\n",
        "        monitor='val_perplexity',\n",
        "        mode='min',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=valid_ds,\n",
        "    epochs=20,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "```\n",
        "\n",
        "**Penjelasan**: Training dengan callbacks untuk early stopping, learning rate reduction, dan model checkpointing. Ini standard practice untuk prevent overfitting dan save best model.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 9: Greedy Text Generation\n",
        "\n",
        "```python\n",
        "def generate_text_greedy(model, seed_text, length=100, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate text menggunakan greedy decoding dengan randomness.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        seed_text: Initial text (string)\n",
        "        length: Berapa banyak bigrams untuk generate\n",
        "        temperature: Control randomness (1.0 = use model's distribution)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convert seed text to n-grams\n",
        "    seed_ngrams = get_ngrams(seed_text, 2)\n",
        "    input_seq = tokenizer.texts_to_sequences([seed_ngrams])[0]\n",
        "    \n",
        "    # Pad ke n_seq length jika perlu\n",
        "    input_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [input_seq], maxlen=n_seq, padding='pre')[0]\n",
        "    \n",
        "    # Generate loop\n",
        "    generated_ngrams = seed_ngrams.copy()\n",
        "    \n",
        "    for _ in range(length):\n",
        "        # Prepare input\n",
        "        current_input = tf.reshape(input_seq[-n_seq:], [1, n_seq])\n",
        "        \n",
        "        # Predict logits\n",
        "        logits = model.predict(current_input, verbose=0)[0, -1, :]\n",
        "        \n",
        "        # Apply temperature\n",
        "        logits = logits / temperature\n",
        "        \n",
        "        # Get probabilities\n",
        "        probs = tf.nn.softmax(logits).numpy()\n",
        "        \n",
        "        # Sample next ID\n",
        "        # Greedy: always pick max\n",
        "        next_id = np.argmax(probs)\n",
        "        \n",
        "        # Optional: Add randomness - sometimes pick from top-3\n",
        "        if np.random.rand() < 0.1:  # 10% chance\n",
        "            top_3_ids = np.argsort(probs)[-3:]\n",
        "            next_id = np.random.choice(top_3_ids)\n",
        "        \n",
        "        # Convert ID back to n-gram\n",
        "        next_ngram = tokenizer.index_word.get(next_id, 'unk')\n",
        "        generated_ngrams.append(next_ngram)\n",
        "        \n",
        "        # Update input for next iteration\n",
        "        input_seq = np.append(input_seq[1:], next_id)\n",
        "    \n",
        "    # Join n-grams to form text\n",
        "    generated_text = ''.join(generated_ngrams)\n",
        "    return generated_text\n",
        "\n",
        "# Generate example\n",
        "seed = \"once upon a time\"\n",
        "generated = generate_text_greedy(model, seed, length=200)\n",
        "print(generated)\n",
        "```\n",
        "\n",
        "**Penjelasan**: Greedy decoding selalu pick word dengan highest probability. Kami tambah optional randomness - 10% waktu pick dari top-3 untuk avoid repetition. Temperature parameter control randomness dari distribution.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 10: Beam Search Text Generation\n",
        "\n",
        "```python\n",
        "def generate_text_beam_search(model, seed_text, length=50, beam_width=3):\n",
        "    \"\"\"\n",
        "    Generate text menggunakan beam search.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        seed_text: Initial text\n",
        "        length: Berapa banyak bigrams untuk generate\n",
        "        beam_width: Berapa candidate paths untuk track\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize seed\n",
        "    seed_ngrams = get_ngrams(seed_text, 2)\n",
        "    input_seq = tokenizer.texts_to_sequences([seed_ngrams])[0]\n",
        "    input_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [input_seq], maxlen=n_seq, padding='pre')[0]\n",
        "    \n",
        "    # Beam search candidates: list of (sequence, log_probability)\n",
        "    candidates = [(input_seq.copy(), 0.0)]\n",
        "    \n",
        "    # Generate loop\n",
        "    for step in range(length):\n",
        "        next_candidates = []\n",
        "        \n",
        "        # For each current candidate\n",
        "        for current_seq, current_log_prob in candidates:\n",
        "            \n",
        "            # Get model predictions\n",
        "            current_input = tf.reshape(current_seq[-n_seq:], [1, n_seq])\n",
        "            logits = model.predict(current_input, verbose=0)[0, -1, :]\n",
        "            probs = tf.nn.softmax(logits).numpy()\n",
        "            \n",
        "            # Get top beam_width candidates\n",
        "            top_ids = np.argsort(probs)[-beam_width:]\n",
        "            \n",
        "            # For each top candidate\n",
        "            for next_id in top_ids:\n",
        "                # Calculate log probability (more stable than probability)\n",
        "                log_prob = np.log(probs[next_id])\n",
        "                new_log_prob = current_log_prob + log_prob\n",
        "                \n",
        "                # Create new sequence\n",
        "                new_seq = np.append(current_seq[1:], next_id)\n",
        "                \n",
        "                # Add to candidates\n",
        "                next_candidates.append((new_seq, new_log_prob))\n",
        "        \n",
        "        # Keep only top beam_width candidates based on log probability\n",
        "        next_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "        candidates = next_candidates[:beam_width]\n",
        "    \n",
        "    # Return best candidate\n",
        "    best_seq, _ = candidates[0]\n",
        "    \n",
        "    # Convert IDs back to n-grams\n",
        "    generated_ngrams = []\n",
        "    for id in best_seq:\n",
        "        ngram = tokenizer.index_word.get(int(id), 'unk')\n",
        "        generated_ngrams.append(ngram)\n",
        "    \n",
        "    # Join to form text\n",
        "    generated_text = ''.join(generated_ngrams)\n",
        "    return generated_text\n",
        "\n",
        "# Generate dengan beam search\n",
        "seed = \"once upon a time\"\n",
        "generated = generate_text_beam_search(model, seed, length=200, beam_width=3)\n",
        "print(\"Generated with beam search:\")\n",
        "print(generated)\n",
        "```\n",
        "\n",
        "**Penjelasan**: Beam search track multiple candidate sequences simultaneously. Di setiap step, kita expand top beam_width candidates menjadi top beam_width options, keeping total beam_width sequences. Ini menghasilkan more coherent text karena explore multiple paths instead of greedy single path.\n",
        "\n",
        "---\n",
        "\n",
        "## Kesimpulan\n",
        "\n",
        "Bab ini mengcover perjalanan lengkap language modeling:\n",
        "1. Download dan preprocess data cerita anak-anak\n",
        "2. Reduce vocabulary dengan n-grams (dari 14K menjadi 735)\n",
        "3. Build GRU-based language model\n",
        "4. Evaluate dengan perplexity metric\n",
        "5. Generate text dengan greedy decoding dan beam search\n",
        "\n",
        "GRU dipilih daripada LSTM karena simpler dan lebih efficient, sementara tetap maintain comparable performance. N-grams approach membuat model jauh lebih praktis dengan vocabulary size yang drastis dikurangi. Beam search menghasilkan text quality yang significantly lebih baik dibanding greedy decoding.\n",
        "\n",
        "Model language modeling ini bisa digunakan sebagai preprocessing untuk task downstream lainnya, atau sebagai foundation untuk building text generation systems seperti story generator atau chatbot.\n"
      ],
      "metadata": {
        "id": "uLkQpG4oP8g6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aUK4iwPOP962"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}