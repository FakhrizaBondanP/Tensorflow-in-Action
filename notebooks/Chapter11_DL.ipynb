{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BAB 11: Sequence-to-Sequence Learning untuk Machine Translation\n",
        "\n",
        "## Ringkasan\n",
        "\n",
        "Bab ini membahas **sequence-to-sequence (seq2seq) learning**, sebuah teknik untuk memetakan sequence input dengan panjang arbitrary ke sequence output dengan panjang arbitrary. Fokus utama adalah membangun **English-to-German machine translator** menggunakan arsitektur encoder-decoder. Model encoder memproses kalimat bahasa Inggris dan menghasilkan context vector, sementara decoder menggunakan context vector tersebut untuk menghasilkan terjemahan Bahasa Jerman. Model dilatih dengan teacher forcing, dievaluasi menggunakan BLEU score, dan kemudian diadaptasi untuk inference dengan decoder rekursif.\n",
        "\n",
        "---\n",
        "\n",
        "## Konsep Fundamental Seq2Seq\n",
        "\n",
        "### Encoder-Decoder Architecture\n",
        "\n",
        "Seq2seq adalah arsitektur dua-bagian yang dirancang khusus untuk masalah di mana panjang input dan output bisa berbeda. Encoder adalah recurrent neural network yang membaca sequence sumber (misalnya, kalimat Inggris) dan menghasilkan **context vector** (juga disebut thought vector) yang merangkum informasi dari seluruh sequence input. Context vector ini adalah representasi vektor padat yang menangkap esensi dari input. Decoder kemudian menggunakan context vector sebagai keadaan awalnya untuk memproduksi sequence target (misalnya, kalimat Bahasa Jerman) word-by-word. Dengan cara ini, model dapat menghasilkan translations yang panjangnya berbeda dari input, karena tidak terikat pada length input.\n",
        "\n",
        "### Teacher Forcing\n",
        "\n",
        "Selama training, decoder dilatih menggunakan teknik **teacher forcing**. Bukan memberi decoder hanya context vector dan membiarkan dia menebak, kita memberikan decoder sequence target yang benar (tapi shifted by 1) sebagai input, dan melatihnya untuk memprediksi kata berikutnya. Contohnya, untuk translation \"I like\" → \"Ich mag\", decoder menerima input [\"sos\", \"Ich\"] dan dilatih untuk memprediksi output [\"Ich\", \"mag\", \"eos\"]. Ini jauh lebih efisien daripada mengharapkan decoder mempelajari everything dari scratch, dan convergence jauh lebih cepat.\n",
        "\n",
        "### Bidirectional RNN\n",
        "\n",
        "Pada encoder, kami menggunakan **Bidirectional GRU** yang membaca input kedua arah: maju (left-to-right) dan mundur (right-to-left). Ini memungkinkan encoder memahami konteks dari kedua sisi, sangat penting untuk bahasa di mana makna kata tergantung pada words di depan dan belakang. Misal, dalam \"John went toward the bank on Clarence Street\" vs \"John went towards the bank of the river\", hanya dengan membaca backward kita bisa tahu apakah \"bank\" merujuk ke bank keuangan atau tepi sungai. Hasil dari forward dan backward pass digabungkan (biasanya dengan concatenation) untuk menghasilkan context vector yang lebih informatif.\n",
        "\n",
        "### SOS dan EOS Tokens\n",
        "\n",
        "Model menggunakan dua special tokens: **SOS** (Start-Of-Sentence) dan **EOS** (End-Of-Sentence). SOS ditambahkan di awal German translation untuk menandai awal sequence, sementara EOS ditambahkan di akhir untuk menandai selesai. Tokens ini crucial untuk inference—ketika generate translation, model mulai dengan SOS token dan continues memprediksi tokens sampai menghasilkan EOS, saat itu generation berhenti.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset dan Preprocessing\n",
        "\n",
        "### Machine Translation Dataset\n",
        "\n",
        "Dataset berisi ~227,000 pasangan English-German dari tatoeba.org, tersimpan dalam format tab-separated dengan struktur `<English>\\t<German>\\t<Attribution>`. Kami menggunakan subset 50,000 contoh untuk mempercepat workflow. Setiap contoh adalah pasangan sentence dari dua bahasa. Dataset dibersihkan dari Unicode issues yang problematic untuk TensorFlow components.\n",
        "\n",
        "### Data Preparation untuk Training\n",
        "\n",
        "Data dipersiapkan melalui beberapa langkah penting. Pertama, SOS dan EOS tokens ditambahkan: \"Geh.\" menjadi \"sos Geh. eos\". Kedua, data dibagi menjadi train (80%), validation (10%), dan test (10%) sets. Ketiga, karena teacher forcing, untuk setiap contoh kita membuat dua sequence: **decoder input** adalah German translation tanpa token terakhir, sementara **decoder label** adalah German translation tanpa token pertama. Misalnya untuk \"Ich mag Lernen\", decoder_input = \"sos Ich mag\" dan decoder_labels = \"Ich mag Lernen eos\". Ini melatih decoder untuk predict next word given previous words.\n",
        "\n",
        "### TextVectorization Layer\n",
        "\n",
        "TensorFlow's **TextVectorization layer** mengkonversi raw text strings ke integer token IDs. Layer ini memiliki dua tahap: **adapt** membangun vocabulary dari corpus, dan **transformation** mengkonversi new text ke sequences of IDs. Output sequence dipadding ke uniform length. Layer ini penting karena membuat model truly end-to-end—bisa menerima raw strings tanpa preprocessing external.\n",
        "\n",
        "---\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "### Encoder Structure\n",
        "\n",
        "Encoder memproses English input melalui beberapa layers. Pertama, TextVectorization layer mengkonversi string input ke token IDs. Kedua, Embedding layer mengkonversi token IDs ke dense word vectors (embedding dimension 128). Ketiga, Bidirectional GRU layer (128 units) membaca sequence tersebut, menghasilkan 256-dimensional context vector (concatenation dari forward 128D dan backward 128D). Context vector ini adalah representasi padat dari seluruh English sentence dan digunakan oleh decoder sebagai initial state.\n",
        "\n",
        "### Decoder Structure\n",
        "\n",
        "Decoder memiliki struktur yang lebih kompleks. Seperti encoder, ia dimulai dengan TextVectorization untuk German input dan Embedding layer untuk menghasilkan word vectors. Tetapi, GRU layer di decoder **bukan bidirectional** karena decoder harus generate translation sequentially—tidak bisa membaca forward dan backward karena output belum tersedia. Decoder GRU diinisialisasi dengan context vector dari encoder. Output dari GRU (256D) dipass ke Dense layer (512 units, ReLU) untuk representasi internal, lalu ke final Dense layer (vocabulary size, softmax) untuk menghasilkan probability distribution atas German vocabulary di setiap timestep.\n",
        "\n",
        "### End-to-End Training Model\n",
        "\n",
        "Model final menggabungkan encoder dan decoder dalam satu architecture yang menerima dua inputs: English sequence (untuk encoder) dan German input sequence (untuk decoder). Model dilatih dengan SparseCategoricalCrossentropy loss (karena target adalah single integer IDs, bukan one-hot vectors). Weights automatically updated via backpropagation melalui kedua components secara simultan.\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluasi Model\n",
        "\n",
        "### Metrics untuk Machine Translation\n",
        "\n",
        "Model dievaluasi menggunakan tiga metrics:\n",
        "\n",
        "**Cross-entropy Loss**: Standard multiclass loss yang mengukur mismatch antara predicted probability distribution dan true target. Lower adalah better, tapi metrik ini tidak begitu intuitive untuk translation quality.\n",
        "\n",
        "**Accuracy**: Percentage of timesteps di mana model predict exactly the correct word. Ini strict metric karena \"gato\" dan \"cat\" (yang semantically similar) keduanya dihitung sebagai error. Jadi accuracy 50% bukan berarti translation 50% baik.\n",
        "\n",
        "**BLEU Score**: More sophisticated metric khusus untuk translation evaluation. BLEU (Bilingual Evaluation Understudy) based pada modified precision yang tidak hanya count word matches, tapi juga mempertimbangkan n-grams. Modified precision ensures bahwa repeated words tidak dicount multiple times. BLEU juga computes precision untuk bigrams, trigrams, dll, yang memfavor translations dengan longer correct phrases intact. BLEU score dari 0.14 berarti model getting significant portions of translations correct.\n",
        "\n",
        "---\n",
        "\n",
        "## Inference: Dari Training ke Generation\n",
        "\n",
        "### Problem dengan Teacher Forcing di Inference\n",
        "\n",
        "Model trained dengan teacher forcing menerima dua inputs: English sequences dan German sequences. Selama training ini perfect karena kita tahu true German translation. Tapi di inference (real-world translation), kita hanya punya English input—German translation yang kita try generate. Kita tidak bisa gunakan true German tokens sebagai input decoder. Solusinya adalah **recursive decoder** yang generates one word at a time, feeding predicted word sebagai input untuk next timestep.\n",
        "\n",
        "### Inference Model Architecture\n",
        "\n",
        "Untuk inference, kami load trained model dan extract encoder dan decoder sebagai separate models. Encoder tetap sama dan dapat digunakan directly. Decoder dimodifikasi menjadi single-step decoder: bukan mengambil seluruh German sequence sebagai input, sekarang menerima single German token dan initial state sebagai input, menghasilkan single predicted token dan next state sebagai output.\n",
        "\n",
        "### Generation Process\n",
        "\n",
        "Proses generation adalah loop: (1) Pass English sentence ke encoder, dapatkan initial state (context vector). (2) Inisialisasi decoder input dengan SOS token dan initial state dari encoder. (3) Decoder predicts next German token dan next state. (4) Iteratively feed predicted token dan state ke decoder untuk timestep berikutnya. (5) Continue sampai decoder outputs EOS token. Output adalah complete German translation built token-by-token.\n",
        "\n",
        "---\n",
        "\n",
        "## Program-Program Implementasi\n",
        "\n",
        "### Program 1: Dataset Loading dan Analysis\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data dari file tab-separated\n",
        "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', header=None)\n",
        "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
        "df = df[[\"EN\", \"DE\"]]\n",
        "\n",
        "# Sample 50,000 contoh\n",
        "n_samples = 50000\n",
        "df = df.sample(n=n_samples, random_state=4321)\n",
        "\n",
        "# Tambahkan SOS dan EOS tokens\n",
        "df[\"DE\"] = 'sos ' + df[\"DE\"] + ' eos'\n",
        "\n",
        "# Split menjadi train/valid/test\n",
        "test_df = df.sample(n=int(n_samples/10), random_state=4321)\n",
        "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=int(n_samples/10), random_state=4321)\n",
        "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]\n",
        "\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(valid_df)}\")\n",
        "print(f\"Test samples: {len(test_df)}\")\n",
        "```\n",
        "\n",
        "**Penjelasan**: Program membaca dataset dari file, random sample 50,000 contoh, menambahkan special tokens ke German text, dan split menjadi 80/10/10 train/validation/test. Random sampling memastikan setiap split representative dari full dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 2: Vocabulary Analysis\n",
        "\n",
        "```python\n",
        "from collections import Counter\n",
        "\n",
        "# Collect semua words dari training data\n",
        "en_words = train_df[\"EN\"].str.split().sum()\n",
        "de_words = train_df[\"DE\"].str.split().sum()\n",
        "\n",
        "# Count frequencies\n",
        "en_counter = Counter(en_words)\n",
        "de_counter = Counter(de_words)\n",
        "\n",
        "# Vocabulary size untuk words yang appear ≥10 times\n",
        "en_vocab = sum(1 for count in en_counter.values() if count >= 10)\n",
        "de_vocab = sum(1 for count in de_counter.values() if count >= 10)\n",
        "\n",
        "# Analyze sequence lengths\n",
        "en_lengths = train_df[\"EN\"].str.split().str.len()\n",
        "de_lengths = train_df[\"DE\"].str.split().str.len()\n",
        "\n",
        "print(f\"English vocab size: {en_vocab}, Median length: {en_lengths.median()}\")\n",
        "print(f\"German vocab size: {de_vocab}, Median length: {de_lengths.median()}\")\n",
        "\n",
        "# Set model parameters\n",
        "en_seq_length = 19  # Extra padding untuk outliers\n",
        "de_seq_length = 21\n",
        "```\n",
        "\n",
        "**Penjelasan**: Program analyze vocabulary size (jumlah unique words yang appear ≥10 kali) dan sequence length distribution (median, percentiles, min/max). Ini menginformasikan model hyperparameters—vocabulary size untuk embedding layers dan max sequence length untuk padding.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 3: TextVectorization Layer Setup\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "def get_vectorizer(corpus, n_vocab, max_length=None, return_vocabulary=True, name=None):\n",
        "    # Input layer untuk strings\n",
        "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name=name+'_input')\n",
        "    \n",
        "    # TextVectorization layer\n",
        "    vectorize_layer = TextVectorization(\n",
        "        max_tokens=n_vocab+2,           # +2 untuk \"\" (padding) dan \"[UNK]\"\n",
        "        output_mode='int',              # Output token IDs\n",
        "        output_sequence_length=max_length\n",
        "    )\n",
        "    \n",
        "    # Fit pada corpus untuk build vocabulary\n",
        "    vectorize_layer.adapt(corpus)\n",
        "    \n",
        "    # Apply vectorizer\n",
        "    vectorized_out = vectorize_layer(inp)\n",
        "    \n",
        "    # Return model dan vocabulary\n",
        "    model = tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name)\n",
        "    vocab = vectorize_layer.get_vocabulary()\n",
        "    \n",
        "    return model, vocab\n",
        "\n",
        "# Create vectorizers untuk English dan German\n",
        "en_vectorizer, en_vocabulary = get_vectorizer(\n",
        "    corpus=np.array(train_df[\"EN\"].tolist()),\n",
        "    n_vocab=en_vocab,\n",
        "    max_length=en_seq_length,\n",
        "    name='en_vectorizer'\n",
        ")\n",
        "\n",
        "de_vectorizer, de_vocabulary = get_vectorizer(\n",
        "    corpus=np.array(train_df[\"DE\"].tolist()),\n",
        "    n_vocab=de_vocab,\n",
        "    max_length=de_seq_length-1,  # -1 karena decoder input offset\n",
        "    name='de_vectorizer'\n",
        ")\n",
        "```\n",
        "\n",
        "**Penjelasan**: TextVectorization layer di-wrap dalam Keras Model untuk fleksibilitas. Setiap vectorizer di-fit hanya pada training data untuk avoid data leakage. Vocab size +2 untuk accommodate special padding dan [UNK] tokens yang automatically added TensorFlow.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 4: Encoder Model\n",
        "\n",
        "```python\n",
        "def get_encoder(n_vocab, vectorizer):\n",
        "    # Input layer\n",
        "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
        "    \n",
        "    # Vectorize\n",
        "    vectorized = vectorizer(inp)\n",
        "    \n",
        "    # Embedding\n",
        "    emb = tf.keras.layers.Embedding(\n",
        "        input_dim=n_vocab+2,\n",
        "        output_dim=128,\n",
        "        mask_zero=True\n",
        "    )(vectorized)\n",
        "    \n",
        "    # Bidirectional GRU—reads forward dan backward\n",
        "    gru = tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.GRU(128)  # Outputs 256D (128+128)\n",
        "    )(emb)\n",
        "    \n",
        "    # Return model\n",
        "    return tf.keras.models.Model(inputs=inp, outputs=gru, name='encoder')\n",
        "\n",
        "encoder = get_encoder(en_vocab, en_vectorizer)\n",
        "```\n",
        "\n",
        "**Penjelasan**: Encoder pipeline: string → vectorized IDs → embeddings (128D) → bidirectional GRU (128 forward + 128 backward = 256D context vector). Bidirectional processing allows understanding context dari kedua directions.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 5: Decoder dan Final Model\n",
        "\n",
        "```python\n",
        "def get_final_seq2seq_model(n_vocab, encoder, de_vectorizer):\n",
        "    # Encoder input dan get context vector\n",
        "    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')\n",
        "    context_vector = encoder(e_inp)\n",
        "    \n",
        "    # Decoder input\n",
        "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
        "    \n",
        "    # Vectorize\n",
        "    d_vectorized = de_vectorizer(d_inp)\n",
        "    \n",
        "    # Embedding\n",
        "    d_emb = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True)(d_vectorized)\n",
        "    \n",
        "    # GRU dengan initial state dari encoder\n",
        "    d_gru = tf.keras.layers.GRU(256, return_sequences=True)(\n",
        "        d_emb, initial_state=context_vector\n",
        "    )\n",
        "    \n",
        "    # Dense layers untuk prediction\n",
        "    d_dense1 = tf.keras.layers.Dense(512, activation='relu')(d_gru)\n",
        "    d_dense2 = tf.keras.layers.Dense(n_vocab+2, activation='softmax')(d_dense1)\n",
        "    \n",
        "    # Final model menerima English dan German, output predictions\n",
        "    model = tf.keras.models.Model(\n",
        "        inputs=[e_inp, d_inp],\n",
        "        outputs=d_dense2,\n",
        "        name='seq2seq'\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "final_model = get_final_seq2seq_model(de_vocab, encoder, de_vectorizer)\n",
        "final_model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "```\n",
        "\n",
        "**Penjelasan**: Final model menggabungkan encoder dan decoder. Context vector dari encoder dipass ke decoder GRU sebagai initial_state, memungkinkan decoder untuk condition generating translation pada encoder understanding dari English input.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 6: Data Preparation untuk Training\n",
        "\n",
        "```python\n",
        "def prepare_data(train_df, valid_df, test_df):\n",
        "    \"\"\"Prepare datasets dalam format untuk model training\"\"\"\n",
        "    data_dict = {}\n",
        "    \n",
        "    for label, df in zip(['train', 'valid', 'test'], [train_df, valid_df, test_df]):\n",
        "        # Encoder inputs: raw English\n",
        "        en_inputs = np.array(df[\"EN\"].tolist())\n",
        "        \n",
        "        # Decoder inputs: German tanpa token terakhir\n",
        "        de_inputs = np.array(\n",
        "            df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:, 0].tolist()\n",
        "        )\n",
        "        \n",
        "        # Decoder labels: German tanpa token pertama\n",
        "        de_labels = np.array(\n",
        "            df[\"DE\"].str.split(n=1, expand=True).iloc[:, 1].tolist()\n",
        "        )\n",
        "        \n",
        "        data_dict[label] = {\n",
        "            'encoder_inputs': en_inputs,\n",
        "            'decoder_inputs': de_inputs,\n",
        "            'decoder_labels': de_labels\n",
        "        }\n",
        "    \n",
        "    return data_dict\n",
        "\n",
        "data = prepare_data(train_df, valid_df, test_df)\n",
        "# data['train']['decoder_inputs'] = German tanpa last word\n",
        "# data['train']['decoder_labels'] = German tanpa first word (SOS)\n",
        "```\n",
        "\n",
        "**Penjelasan**: Teacher forcing memerlukan special data formatting. Untuk setiap target sequence, kita create input (semua words except last) dan label (semua words except first). Ini melatih model predict next word given previous context.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 7: BLEU Metric untuk Evaluasi\n",
        "\n",
        "```python\n",
        "class BLEUMetric:\n",
        "    def __init__(self, vocabulary):\n",
        "        self.vocab = vocabulary\n",
        "        # StringLookup layer untuk convert IDs back to words\n",
        "        self.id_to_word = tf.keras.layers.StringLookup(\n",
        "            vocabulary=self.vocab,\n",
        "            invert=True,\n",
        "            num_oov_indices=0\n",
        "        )\n",
        "    \n",
        "    def calculate_bleu_from_predictions(self, true_ids, pred_logits):\n",
        "        # Convert logits to IDs\n",
        "        pred_ids = tf.argmax(pred_logits, axis=-1)\n",
        "        \n",
        "        # Convert IDs to tokens\n",
        "        pred_tokens = self.id_to_word(pred_ids)\n",
        "        true_tokens = self.id_to_word(true_ids)\n",
        "        \n",
        "        # Clean text (remove EOS dan setelahnya)\n",
        "        def clean_text(tokens):\n",
        "            # Join tokens to string\n",
        "            text = tf.strings.join(tf.transpose(tokens), separator=' ')\n",
        "            # Remove everything after \"eos\"\n",
        "            text = tf.strings.regex_replace(text, \"eos.*\", \"\")\n",
        "            # Convert to numpy dan split\n",
        "            text = text.numpy().astype(str)\n",
        "            return [t.split() for t in text]\n",
        "        \n",
        "        pred_clean = clean_text(pred_tokens)\n",
        "        true_clean = [[t.split()] for t in clean_text(true_tokens)]\n",
        "        \n",
        "        # Compute BLEU\n",
        "        bleu, _, _, _, _, _ = compute_bleu(true_clean, pred_clean)\n",
        "        return bleu\n",
        "\n",
        "bleu_metric = BLEUMetric(de_vocabulary)\n",
        "```\n",
        "\n",
        "**Penjelasan**: BLEU metric adalah customized metric untuk translation tasks. Metric mengkonversi predicted logits menjadi token IDs, IDs menjadi words, cleanup text (remove padding dan special tokens), lalu compute BLEU score yang measures n-gram overlap dengan reference translation.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 8: Training Loop\n",
        "\n",
        "```python\n",
        "def train_model(model, de_vectorizer, train_df, valid_df, test_df,\n",
        "                epochs=5, batch_size=128):\n",
        "    bleu_metric = BLEUMetric(de_vocabulary)\n",
        "    data = prepare_data(train_df, valid_df, test_df)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle training data\n",
        "        train_data = data['train']\n",
        "        indices = np.random.permutation(len(train_data['encoder_inputs']))\n",
        "        \n",
        "        for key in train_data:\n",
        "            train_data[key] = train_data[key][indices]\n",
        "        \n",
        "        # Training batches\n",
        "        n_batches = len(train_data['encoder_inputs']) // batch_size\n",
        "        loss_list, acc_list, bleu_list = [], [], []\n",
        "        \n",
        "        for i in range(n_batches):\n",
        "            start = i * batch_size\n",
        "            end = start + batch_size\n",
        "            \n",
        "            # Get batch\n",
        "            x = [\n",
        "                train_data['encoder_inputs'][start:end],\n",
        "                train_data['decoder_inputs'][start:end]\n",
        "            ]\n",
        "            y = de_vectorizer(train_data['decoder_labels'][start:end])\n",
        "            \n",
        "            # Train\n",
        "            model.train_on_batch(x, y)\n",
        "            \n",
        "            # Evaluate\n",
        "            loss, acc = model.evaluate(x, y, verbose=0)\n",
        "            pred_y = model.predict(x, verbose=0)\n",
        "            bleu = bleu_metric.calculate_bleu_from_predictions(y, pred_y)\n",
        "            \n",
        "            loss_list.append(loss)\n",
        "            acc_list.append(acc)\n",
        "            bleu_list.append(bleu)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"  Train - Loss: {np.mean(loss_list):.4f}, Acc: {np.mean(acc_list):.4f}, BLEU: {np.mean(bleu_list):.4f}\")\n",
        "\n",
        "train_model(final_model, de_vectorizer, train_df, valid_df, test_df, epochs=5)\n",
        "```\n",
        "\n",
        "**Penjelasan**: Training loop mengiterate epochs, shuffle training data setiap epoch, process batch-by-batch, train dan evaluate simultaneously. Log metrics tracked untuk monitoring training progress.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 9: Inference Model Setup\n",
        "\n",
        "```python\n",
        "def get_inference_model(trained_model_path):\n",
        "    # Load trained model\n",
        "    model = tf.keras.models.load_model(trained_model_path)\n",
        "    \n",
        "    # Extract encoder\n",
        "    en_model = model.get_layer(\"encoder\")\n",
        "    \n",
        "    # Build new decoder untuk single-step inference\n",
        "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "    d_state = tf.keras.Input(shape=(256,))  # Initial state dari encoder\n",
        "    \n",
        "    # Vectorize dan embed\n",
        "    d_vec = model.get_layer('d_vectorizer')(d_inp)\n",
        "    d_emb = model.get_layer('d_embedding')(d_vec)\n",
        "    \n",
        "    # GRU—return single output, not sequence\n",
        "    d_gru = model.get_layer('d_gru')\n",
        "    d_gru.return_sequences = False\n",
        "    d_gru_out = d_gru(d_emb, initial_state=d_state)\n",
        "    \n",
        "    # Dense layers\n",
        "    d_dense = model.get_layer('d_dense_1')(d_gru_out)\n",
        "    d_out = model.get_layer('d_dense_final')(d_dense)\n",
        "    \n",
        "    # Decoder returns prediction dan next state\n",
        "    de_model = tf.keras.models.Model(\n",
        "        inputs=[d_inp, d_state],\n",
        "        outputs=[d_out, d_gru_out]\n",
        "    )\n",
        "    \n",
        "    return en_model, de_model\n",
        "\n",
        "en_model, de_model = get_inference_model('models/seq2seq')\n",
        "```\n",
        "\n",
        "**Penjelasan**: Inference model extract weights dari trained model tapi restructure decoder untuk operate sequentially. Key change: decoder GRU set to `return_sequences=False` agar output single timestep, bukan sequence, memudahkan recursive feeding.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 10: Translation Generation\n",
        "\n",
        "```python\n",
        "def generate_translation(en_model, de_model, de_vocabulary, english_text):\n",
        "    # Get context vector\n",
        "    context = en_model.predict(np.array([english_text]), verbose=0)\n",
        "    \n",
        "    # Initialize with SOS token\n",
        "    current_token = 'sos'\n",
        "    translation = []\n",
        "    \n",
        "    # Generate until EOS\n",
        "    while current_token != 'eos':\n",
        "        # Predict next token\n",
        "        pred_logits, context = de_model.predict(\n",
        "            [np.array([current_token]), context],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        # Get token ID dengan highest probability\n",
        "        token_id = np.argmax(pred_logits[0])\n",
        "        current_token = de_vocabulary[token_id]\n",
        "        \n",
        "        translation.append(current_token)\n",
        "    \n",
        "    return ' '.join(translation[:-1])  # Remove EOS token\n",
        "\n",
        "# Example\n",
        "result = generate_translation(en_model, de_model, de_vocabulary, \"Hello world\")\n",
        "print(result)  # Output: \"Hallo Welt eos\"\n",
        "```\n",
        "\n",
        "**Penjelasan**: Generation process adalah autoregressive loop. Start dengan SOS token, predict next token given current token dan state, feed prediction sebagai next input, repeat sampai EOS token generated. Resulting translation adalah sequence predicted tokens joined bersama.\n",
        "\n",
        "---\n",
        "\n",
        "## Kesimpulan\n",
        "\n",
        "Bab ini menunjukkan cara membangun complete end-to-end machine translation system menggunakan seq2seq architecture. Key insights: encoder-decoder design elegantly handles variable-length sequences, teacher forcing accelerates training, TextVectorization layer enables true end-to-end models yang menerima raw strings, BLEU metric lebih appropriate daripada accuracy untuk translation evaluation, dan inference requires separate model architecture yang operates recursively untuk generate translations token-by-token. Bab selanjutnya (Chapter 12) akan improve model ini dengan attention mechanism yang memungkinkan decoder fokus pada specific parts of encoder input saat generating setiap target token.\n"
      ],
      "metadata": {
        "id": "R-x6hwLsQwiq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXgP3lfFQxLY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}