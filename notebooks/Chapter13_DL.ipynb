{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CHAPTER 13: Transformers\n",
        "\n",
        "## Ringkasan\n",
        "\n",
        "Chapter ini membahas **Transformer architecture**, model state-of-the-art yang revolusionize NLP dengan pure attention mechanism tanpa recurrence. Berbeda dari LSTM/GRU yang process sequential, Transformer dapat melihat seluruh sequence sekaligus, meningkatkan parallelization dan language understanding. Chapter mencakup implementasi full Transformer dari scratch dengan semua komponennya (token embeddings, positional embeddings, multi-head attention, layer normalization, residual connections), spam classification menggunakan pretrained BERT dari TensorFlow Hub, dan question answering dengan Hugging Face transformers library. BERT adalah encoder-only Transformer pretrained pada large corpus menggunakan masked language modeling dan next-sentence prediction, memungkinkan transfer learning untuk berbagai downstream NLP tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 1: Transformer Architecture Detail\n",
        "\n",
        "### Keuntungan Transformer vs RNN\n",
        "\n",
        "**RNN Limitations**: LSTM dan GRU model sequential—process one timestep at a time, maintaining limited state vector (memory). Ini membuat:\n",
        "- **Sequential bottleneck**: Tidak bisa parallel processing karena timestep t tergantung pada t-1\n",
        "- **Long-term dependencies**: Sulit remember information dari awal sequence untuk long texts\n",
        "- **Context limitation**: Attention mechanism di Chapter 12 membantu, tapi masih limited oleh sequential nature\n",
        "\n",
        "**Transformer Solution**: Transformer process entire sequence simultaneously menggunakan self-attention. Key advantages:\n",
        "- **Parallelization**: Semua timesteps computed sekaligus, drastically reducing training time\n",
        "- **Full context**: Setiap token dapat attend ke semua tokens lain, tidak limited by sequential processing\n",
        "- **Scalability**: Lebih mudah scale ke very large models karena parallel-friendly architecture\n",
        "\n",
        "### Self-Attention: Query, Key, Value\n",
        "\n",
        "Self-attention adalah core mechanism yang memungkinkan token \"look at\" tokens lain untuk enrich representationnya. Untuk setiap token, model generates tiga vectors melalui learned weight matrices:\n",
        "\n",
        "**Query (Q)**: Represents \"what am I looking for?\" Untuk token saat ini, query vector menentukan apa informasi yang dibutuhkan dari tokens lain.\n",
        "\n",
        "**Key (K)**: Represents \"what do I offer?\" Untuk setiap candidate token, key vector indicates informasi apa yang bisa diprovide.\n",
        "\n",
        "**Value (V)**: Represents actual content. Hidden representation dari token yang akan di-aggregate based on attention weights.\n",
        "\n",
        "**Computation Process**:\n",
        "1. **Score calculation**: Untuk setiap query, compute similarity dengan semua keys: \\( score = Q \\cdot K^T / \\sqrt{d_k} \\) (scaled dot product, division by \\(\\sqrt{d_k}\\) untuk stable gradients)\n",
        "2. **Attention weights**: Normalize scores dengan softmax: \\( \\alpha = softmax(score) \\)\n",
        "3. **Weighted sum**: Combine values weighted by attention: \\( output = \\alpha \\cdot V \\)\n",
        "\n",
        "Formula lengkap: \\( Attention(Q, K, V) = softmax(QK^T / \\sqrt{d_k}) V \\)\n",
        "\n",
        "**Multi-Head Attention**: Instead of single attention, Transformer uses multiple \"heads\" (biasanya 8 atau 12). Setiap head learn different aspects of relationships. Outputs dari all heads di-concatenate dan project back ke original dimension.\n",
        "\n",
        "### Token dan Positional Embeddings\n",
        "\n",
        "**Token Embeddings**: Seperti word embeddings standard—setiap token (word/subword) mapped ke dense vector representation. Matrix size \\( V \\times d_{model} \\) where V = vocabulary size, \\( d_{model} \\) = embedding dimension (typically 512 atau 768). Embeddings learned jointly dengan task training.\n",
        "\n",
        "**Positional Embeddings**: Critical karena Transformer tidak inherently sequential. Tanpa positional info, \"John loves Mary\" dan \"Mary loves John\" akan identik. Original paper menggunakan sinusoidal functions:\n",
        "- Even positions: \\( PE(pos, 2i) = sin(pos / 10000^{2i/d_{model}}) \\)\n",
        "- Odd positions: \\( PE(pos, 2i+1) = cos(pos / 10000^{2i/d_{model}}) \\)\n",
        "\n",
        "Frequency decreases dengan increasing position dalam embedding vector, creating unique signature untuk each position. Alternative: learned positional embeddings (model learns positions during training).\n",
        "\n",
        "**Final embedding**: Token embedding + Positional embedding (element-wise addition) = Input ke Transformer.\n",
        "\n",
        "### Residual Connections dan Layer Normalization\n",
        "\n",
        "**Residual Connections**: Add input dari sublayer ke outputnya: \\( output = Sublayer(x) + x \\). Benefits:\n",
        "- **Gradient flow**: Direct path untuk gradients flow backward, combating vanishing gradients\n",
        "- **Identity mapping**: Model dapat learn identity function dengan setting Sublayer weights ke zero\n",
        "- **Easier optimization**: Layers learn residual (difference), bukan full transformation\n",
        "\n",
        "**Layer Normalization**: Normalize activations within each sample (bukan across batch seperti batch norm). Untuk hidden layer dengan units \\( h_1, h_2, ..., h_n \\):\n",
        "- Compute mean: \\( \\mu = \\frac{1}{n} \\sum h_i \\)\n",
        "- Compute variance: \\( \\sigma^2 = \\frac{1}{n} \\sum (h_i - \\mu)^2 \\)\n",
        "- Normalize: \\( \\hat{h}_i = (h_i - \\mu) / \\sqrt{\\sigma^2 + \\epsilon} \\)\n",
        "- Scale and shift: \\( output = \\gamma \\hat{h}_i + \\beta \\) (learnable parameters)\n",
        "\n",
        "Layer norm tidak depend on batch size, lebih stable untuk variable batch sizes dan RNN-like architectures.\n",
        "\n",
        "### Encoder dan Decoder Structure\n",
        "\n",
        "**Encoder**: Stack of N layers (typically 6 atau 12), each containing:\n",
        "1. **Multi-head self-attention**: Tokens attend to all tokens dalam input sequence\n",
        "2. **Add & Norm**: Residual connection + layer normalization\n",
        "3. **Feed-forward network**: Two dense layers dengan ReLU: \\( FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2 \\)\n",
        "4. **Add & Norm**: Residual connection + layer normalization\n",
        "\n",
        "**Decoder**: Stack of N layers, each containing:\n",
        "1. **Masked multi-head self-attention**: Tokens hanya attend ke previous positions (prevent looking ahead)\n",
        "2. **Add & Norm**\n",
        "3. **Encoder-decoder attention**: Queries dari decoder attend to keys dan values dari encoder output\n",
        "4. **Add & Norm**\n",
        "5. **Feed-forward network**\n",
        "6. **Add & Norm**\n",
        "\n",
        "Masking ensures decoder cannot \"cheat\" by seeing future tokens during training.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 2: BERT (Bidirectional Encoder Representations from Transformers)\n",
        "\n",
        "### BERT Overview\n",
        "\n",
        "BERT adalah **encoder-only** Transformer (tidak ada decoder). Key innovation: **bidirectional pretraining** (model sees full context dari left dan right simultaneously, unlike GPT yang unidirectional left-to-right).\n",
        "\n",
        "**Architecture**:\n",
        "- **BERT-Base**: 12 encoder layers, 768 hidden size, 12 attention heads, 110M parameters\n",
        "- **BERT-Large**: 24 encoder layers, 1024 hidden size, 16 attention heads, 340M parameters\n",
        "\n",
        "**Three Embeddings Spaces**:\n",
        "1. **Token embeddings**: Standard word/subword vectors\n",
        "2. **Positional embeddings**: Learned positions (bukan sinusoidal)\n",
        "3. **Segment embeddings**: Distinguish sequence A vs B (untuk tasks dengan 2 input sequences)\n",
        "\n",
        "Final embedding = sum dari ketiga embeddings.\n",
        "\n",
        "### Pretraining Tasks\n",
        "\n",
        "BERT pretrained pada massive corpus (Wikipedia + BookCorpus) menggunakan two self-supervised tasks:\n",
        "\n",
        "**Masked Language Modeling (MLM)**: Randomly mask 15% tokens, model predicts masked tokens. Masking strategy:\n",
        "- 80% probability: Replace dengan [MASK] token\n",
        "- 10% probability: Replace dengan random token\n",
        "- 10% probability: Keep original token\n",
        "Ini prevents pretraining-finetuning discrepancy (karena [MASK] tidak ada di real data).\n",
        "\n",
        "**Next Sentence Prediction (NSP)**: Given two sentences A dan B, predict apakah B adalah actual next sentence after A. Training data:\n",
        "- 50% positive examples: B follows A\n",
        "- 50% negative examples: B random sentence\n",
        "NSP helps model understand sentence-level relationships.\n",
        "\n",
        "### Special Tokens\n",
        "\n",
        "**[CLS]**: Classification token, always first token. Hidden representation dari [CLS] position digunakan untuk sequence-level classification tasks.\n",
        "\n",
        "**[SEP]**: Separator token, marks boundaries antara sequences. Example: `[CLS] Question [SEP] Context [SEP]` untuk question answering.\n",
        "\n",
        "**[PAD]**: Padding token (ID=0) untuk bring sequences ke same length.\n",
        "\n",
        "**[MASK]**: Masking token untuk MLM task.\n",
        "\n",
        "### Four Task Categories\n",
        "\n",
        "BERT designed untuk solve:\n",
        "\n",
        "1. **Sequence Classification**: Single sequence → single label (sentiment analysis, spam detection). Use [CLS] representation + classification head.\n",
        "\n",
        "2. **Token Classification**: Single sequence → label per token (NER, POS tagging). Use each token's representation + classification head.\n",
        "\n",
        "3. **Question Answering**: Two sequences (question + context) → predict start/end span dalam context. Two classification heads predict start dan end positions.\n",
        "\n",
        "4. **Multiple Choice**: Multiple sequences (question + N candidates) → predict correct choice. Process each candidate separately dan compare [CLS] representations.\n",
        "\n",
        "### Transfer Learning dengan BERT\n",
        "\n",
        "Workflow standard:\n",
        "1. **Download pretrained BERT**: Model sudah trained pada massive corpus, has rich language understanding\n",
        "2. **Add task-specific head**: Simple classification layer(s) on top\n",
        "3. **Fine-tune end-to-end**: Train BERT + head jointly pada labeled task data\n",
        "4. **Benefit**: Requires far less data dan training time karena BERT already understands language\n",
        "\n",
        "---\n",
        "\n",
        "## Part 3: Spam Classification dengan BERT\n",
        "\n",
        "### Dataset dan Class Imbalance\n",
        "\n",
        "SMS Spam Collection: 5,574 messages (4,827 ham, 747 spam). Significant imbalance → need balanced splits.\n",
        "\n",
        "**Strategy**:\n",
        "1. Random undersample untuk create balanced test (100 spam, 100 ham) dan validation sets (100 each)\n",
        "2. Use **NearMiss algorithm** untuk undersample training set. NearMiss removes majority class samples yang close to minority class, increasing separation.\n",
        "\n",
        "Bag-of-words representation digunakan untuk compute distances dalam NearMiss (karena algorithm needs numerical distance metric).\n",
        "\n",
        "**Final splits**: Train (547 spam, 547 ham), Valid (100 each), Test (100 each) — all balanced.\n",
        "\n",
        "### Tokenization dengan WordPiece\n",
        "\n",
        "BERT menggunakan **WordPiece tokenization**: Breaks words into frequent subwords. Example: \"seashells\" → [\"seas\", \"##hell\", \"##s\"] where ## indicates continuation.\n",
        "\n",
        "**Advantages**:\n",
        "- **Smaller vocabulary**: \"walk\", \"walked\", \"walking\" → [\"walk\", \"##ed\", \"##ing\"]\n",
        "- **Handle OOV**: Unseen words dapat represented by combining known subwords\n",
        "- **Better generalization**: Model learns morphological patterns\n",
        "\n",
        "**Process**:\n",
        "1. Tokenizer splits input string ke subword tokens\n",
        "2. Add [CLS] at start, [SEP] at end\n",
        "3. Convert tokens ke IDs via vocabulary lookup\n",
        "4. Create attention mask (1 for real tokens, 0 for padding)\n",
        "5. Create segment IDs (all 0s untuk single sequence task)\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "**Components**:\n",
        "1. **Input layers**: `input_word_ids`, `input_mask`, `input_type_ids`\n",
        "2. **BERT encoder**: Download from TensorFlow Hub (`bert_en_uncased_L-12_H-768_A-12`)\n",
        "3. **BERT outputs**: `sequence_output` (all tokens) dan `pooled_output` ([CLS] representation)\n",
        "4. **Classification head**: BertClassifier automatically adds Dense layer on top untuk binary classification\n",
        "\n",
        "**Compilation**:\n",
        "- **Optimizer**: AdamW dengan polynomial learning rate decay dan warmup\n",
        "- **Loss**: Sparse categorical cross-entropy\n",
        "- **Metric**: Accuracy\n",
        "\n",
        "**Warmup Schedule**: Learning rate linearly increases dari 0 ke init_lr dalam warmup_steps, then polynomial decay ke 0 across remaining steps. Ini stabilizes training.\n",
        "\n",
        "### Training dan Results\n",
        "\n",
        "Train 3 epochs, batch size 56. Results:\n",
        "- Epoch 1: 45% train accuracy, 51% validation accuracy (model learning)\n",
        "- Epoch 2: 65% train, 81% validation (rapid improvement)\n",
        "- Epoch 3: 76% train, 85% validation (convergence)\n",
        "- **Test accuracy: 79.5%**\n",
        "\n",
        "Remarkable result dengan minimal effort: hanya 3 epochs, no hyperparameter tuning, small dataset. BERT's pretrained knowledge makes this possible.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 4: Question Answering dengan Hugging Face\n",
        "\n",
        "### SQuAD Dataset\n",
        "\n",
        "**Stanford Question Answering Dataset (SQuAD v1)**: 87,599 training examples, 10,570 validation examples. Each example contains:\n",
        "- **Question**: \"What color is the ball?\"\n",
        "- **Context**: \"Tippy is a dog. She loves to play with her red ball.\"\n",
        "- **Answer**: {\"text\": \"red\", \"answer_start\": 49}\n",
        "\n",
        "**Data Issues**: Some examples have character offset errors (answer_start doesn't align dengan actual answer position). Solution: Correction function yang tries offsets -2, -1, 0 dan selects correct alignment.\n",
        "\n",
        "### DistilBERT\n",
        "\n",
        "**DistilBERT** adalah smaller, faster version dari BERT:\n",
        "- **Size**: 66M parameters (vs BERT-Base 110M)\n",
        "- **Speed**: 60% faster\n",
        "- **Performance**: Retains 97% of BERT's performance\n",
        "- **Training**: Knowledge distillation (student model learns from teacher BERT)\n",
        "\n",
        "Ideal untuk production scenarios di mana speed dan size matters.\n",
        "\n",
        "### Tokenization untuk QA\n",
        "\n",
        "**Input Format**: `[CLS] Question tokens [SEP] Context tokens [SEP]`\n",
        "\n",
        "**Key Steps**:\n",
        "1. Tokenize question dan context separately\n",
        "2. Concatenate dengan special tokens\n",
        "3. Convert char-based answer positions ke token-based positions (crucial step!)\n",
        "4. Handle truncation: If answer truncated, set start/end positions ke last token index\n",
        "\n",
        "**Token-based Positions**: Model predicts token indices (bukan character indices). Must map character answer_start/answer_end ke corresponding token positions using `encodings.char_to_token()`.\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "**TFDistilBertForQuestionAnswering**:\n",
        "- Input: Token IDs + attention mask\n",
        "- BERT encoding: Generate hidden representations untuk all tokens\n",
        "- **Two classification heads**:\n",
        "  - Start position classifier: Predicts probability distribution over all token positions untuk answer start\n",
        "  - End position classifier: Predicts probability distribution untuk answer end\n",
        "- Loss: Sum dari cross-entropy losses untuk start dan end predictions\n",
        "\n",
        "**Training**:\n",
        "- Optimizer: AdamW dengan learning rate 5e-5\n",
        "- Batch size: 8\n",
        "- Epochs: 2\n",
        "- Training time: ~2 hours on GPU\n",
        "\n",
        "### Inference dan Evaluation\n",
        "\n",
        "**Inference Process**:\n",
        "1. Tokenize question + context\n",
        "2. Model predicts start/end probability distributions\n",
        "3. Select argmax positions dari each distribution\n",
        "4. Extract tokens between start dan end positions\n",
        "5. Convert token IDs back to text\n",
        "\n",
        "**Metrics**:\n",
        "- **Exact Match (EM)**: % predictions yang exactly match reference answer\n",
        "- **F1 Score**: Token-level F1 between predicted dan reference answers (more forgiving)\n",
        "\n",
        "**Example Result**:\n",
        "- Question: \"What was the theme of Super Bowl 50?\"\n",
        "- Context: \"...league emphasized the 'golden anniversary'...\"\n",
        "- Predicted: \"golden anniversary\" ✓\n",
        "- True answer: \"golden anniversary\"\n",
        "\n",
        "Model berhasil extract correct answer dari long paragraph context.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 5: Program-Program Implementasi\n",
        "\n",
        "### Program 1: Positional Embeddings\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def get_positional_encoding(n_steps, d_model):\n",
        "    \"\"\"Generate sinusoidal positional encodings\"\"\"\n",
        "    position = np.arange(n_steps)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    \n",
        "    # Create empty positional encoding matrix\n",
        "    pos_encoding = np.zeros((n_steps, d_model))\n",
        "    \n",
        "    # Apply sin to even indices\n",
        "    pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
        "    \n",
        "    # Apply cos to odd indices\n",
        "    pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
        "    \n",
        "    return tf.constant(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "# Usage\n",
        "n_steps = 100  # Sequence length\n",
        "d_model = 512  # Embedding dimension\n",
        "\n",
        "pos_encoding = get_positional_encoding(n_steps, d_model)\n",
        "print(f\"Positional encoding shape: {pos_encoding.shape}\")  # (100, 512)\n",
        "\n",
        "# Combine dengan token embeddings\n",
        "token_embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(input_ids)\n",
        "final_embeddings = token_embeddings + pos_encoding[:n_steps, :]\n",
        "```\n",
        "\n",
        "**Penjelasan**: Positional encoding menggunakan sine/cosine functions dengan different frequencies. Even indices use sine, odd indices use cosine. Frequencies decrease dengan position, creating unique signatures. Final embeddings adalah sum dari token embeddings dan positional encodings.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 2: Multi-Head Attention Layer\n",
        "\n",
        "```python\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        assert d_model % num_heads == 0\n",
        "        self.depth = d_model // num_heads\n",
        "        \n",
        "        # Dense layers untuk Q, K, V projections\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "        # Final dense layer\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split last dimension ke (num_heads, depth)\"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        # Linear projections\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "        \n",
        "        # Split heads\n",
        "        q = self.split_heads(q, batch_size)  # (batch, heads, seq_len, depth)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "        \n",
        "        # Scaled dot-product attention\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        \n",
        "        # Scale\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "        \n",
        "        # Apply mask (if provided)\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "        \n",
        "        # Softmax\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        \n",
        "        # Weighted sum\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "        \n",
        "        # Concatenate heads\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))\n",
        "        \n",
        "        # Final linear projection\n",
        "        output = self.dense(concat_attention)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "# Usage\n",
        "mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "x = tf.random.uniform((32, 100, 512))  # (batch, seq_len, d_model)\n",
        "output, attn_weights = mha(x, x, x)\n",
        "print(f\"Output shape: {output.shape}\")  # (32, 100, 512)\n",
        "```\n",
        "\n",
        "**Penjelasan**: Multi-head attention splits d_model dimension ke multiple heads, each compute attention independently. Queries, keys, dan values projected ke different subspaces, attention computed per head, results concatenated dan projected back. Multiple heads allow model learn different relationship types simultaneously.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 3: Transformer Encoder Layer\n",
        "\n",
        "```python\n",
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        \n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        \n",
        "        # Feed-forward network\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "        \n",
        "        # Layer normalization\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    \n",
        "    def call(self, x, training, mask=None):\n",
        "        # Multi-head attention\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # Residual + LayerNorm\n",
        "        \n",
        "        # Feed-forward\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # Residual + LayerNorm\n",
        "        \n",
        "        return out2\n",
        "\n",
        "# Complete encoder dengan multiple layers\n",
        "class TransformerEncoder(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, max_len, dropout_rate=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # Embeddings\n",
        "        self.token_embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = get_positional_encoding(max_len, d_model)\n",
        "        \n",
        "        # Encoder layers\n",
        "        self.enc_layers = [\n",
        "            TransformerEncoderLayer(d_model, num_heads, dff, dropout_rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "        \n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    \n",
        "    def call(self, x, training, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        \n",
        "        # Token + positional embeddings\n",
        "        x = self.token_embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # Scale embeddings\n",
        "        x += self.pos_encoding[:seq_len, :]\n",
        "        \n",
        "        x = self.dropout(x, training=training)\n",
        "        \n",
        "        # Pass through encoder layers\n",
        "        for enc_layer in self.enc_layers:\n",
        "            x = enc_layer(x, training, mask)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Usage\n",
        "encoder = TransformerEncoder(\n",
        "    num_layers=6, d_model=512, num_heads=8,\n",
        "    dff=2048, vocab_size=10000, max_len=5000\n",
        ")\n",
        "input_ids = tf.random.uniform((32, 100), maxval=10000, dtype=tf.int32)\n",
        "output = encoder(input_ids, training=True)\n",
        "print(f\"Encoder output shape: {output.shape}\")  # (32, 100, 512)\n",
        "```\n",
        "\n",
        "**Penjelasan**: Encoder layer consists of multi-head attention + residual connection + layer norm, followed by feed-forward network + residual + layer norm. Full encoder stacks multiple layers (typically 6 atau 12). Embeddings scaled by \\(\\sqrt{d_{model}}\\) untuk stabilize training.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 4: BERT Spam Classification\n",
        "\n",
        "```python\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_models as tfm\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# Download BERT dari TensorFlow Hub\n",
        "bert_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n",
        "max_seq_length = 128\n",
        "\n",
        "# Define inputs\n",
        "input_word_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "input_mask = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
        "input_type_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_type_ids\")\n",
        "\n",
        "# Load BERT encoder\n",
        "bert_layer = hub.KerasLayer(bert_url, trainable=True)\n",
        "bert_outputs = bert_layer({\n",
        "    \"input_word_ids\": input_word_ids,\n",
        "    \"input_mask\": input_mask,\n",
        "    \"input_type_ids\": input_type_ids\n",
        "})\n",
        "\n",
        "# Create encoder model\n",
        "encoder = tf.keras.Model(\n",
        "    inputs={\n",
        "        \"input_word_ids\": input_word_ids,\n",
        "        \"input_mask\": input_mask,\n",
        "        \"input_type_ids\": input_type_ids\n",
        "    },\n",
        "    outputs={\n",
        "        \"sequence_output\": bert_outputs[\"sequence_output\"],\n",
        "        \"pooled_output\": bert_outputs[\"pooled_output\"]\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add classification head\n",
        "classifier = tfm.nlp.models.BertClassifier(\n",
        "    network=encoder,\n",
        "    num_classes=2  # Binary classification: spam vs ham\n",
        ")\n",
        "\n",
        "# Compile\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "classifier.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train\n",
        "history = classifier.fit(\n",
        "    train_inputs,\n",
        "    train_labels,\n",
        "    validation_data=(valid_inputs, valid_labels),\n",
        "    epochs=3,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = classifier.evaluate(test_inputs, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "```\n",
        "\n",
        "**Penjelasan**: BERT downloaded sebagai KerasLayer dari TensorFlow Hub. Encoder outputs pooled representation dari [CLS] token. BertClassifier automatically adds Dense layer on top untuk classification. Training dengan small learning rate (3e-5) karena pretrained weights sudah good. Achieves high accuracy dengan minimal epochs.\n",
        "\n",
        "---\n",
        "\n",
        "### Program 5: Question Answering dengan Hugging Face\n",
        "\n",
        "```python\n",
        "from transformers import DistilBertTokenizerFast, TFDistilBertForQuestionAnswering\n",
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"squad\")\n",
        "train_contexts = dataset[\"train\"][\"context\"]\n",
        "train_questions = dataset[\"train\"][\"question\"]\n",
        "train_answers = dataset[\"train\"][\"answers\"]\n",
        "\n",
        "# Initialize tokenizer dan model\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "model = TFDistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenize training data\n",
        "def tokenize_and_add_positions(contexts, questions, answers):\n",
        "    # Tokenize\n",
        "    encodings = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=384,\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "    \n",
        "    # Convert char positions ke token positions\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    \n",
        "    for i, answer in enumerate(answers):\n",
        "        start_char = answer['answer_start'][0]\n",
        "        end_char = start_char + len(answer['text'][0])\n",
        "        \n",
        "        # Find token positions\n",
        "        start_token = encodings.char_to_token(i, start_char)\n",
        "        end_token = encodings.char_to_token(i, end_char - 1)\n",
        "        \n",
        "        # Handle truncation\n",
        "        if start_token is None:\n",
        "            start_token = tokenizer.model_max_length - 1\n",
        "        if end_token is None:\n",
        "            end_token = tokenizer.model_max_length - 1\n",
        "        \n",
        "        start_positions.append(start_token)\n",
        "        end_positions.append(end_token)\n",
        "    \n",
        "    encodings['start_positions'] = start_positions\n",
        "    encodings['end_positions'] = end_positions\n",
        "    \n",
        "    return encodings\n",
        "\n",
        "train_encodings = tokenize_and_add_positions(train_contexts, train_questions, train_answers)\n",
        "\n",
        "# Create tf.data dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': train_encodings['input_ids'],\n",
        "        'attention_mask': train_encodings['attention_mask']\n",
        "    },\n",
        "    {\n",
        "        'start_positions': train_encodings['start_positions'],\n",
        "        'end_positions': train_encodings['end_positions']\n",
        "    }\n",
        ")).batch(8)\n",
        "\n",
        "# Compile\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "# Train\n",
        "model.fit(train_dataset, epochs=2)\n",
        "\n",
        "# Inference function\n",
        "def answer_question(question, context):\n",
        "    inputs = tokenizer(\n",
        "        question,\n",
        "        context,\n",
        "        return_tensors='tf',\n",
        "        truncation=True,\n",
        "        padding=True\n",
        "    )\n",
        "    \n",
        "    outputs = model(inputs)\n",
        "    \n",
        "    # Get start dan end positions\n",
        "    start_idx = tf.argmax(outputs.start_logits, axis=1).numpy()[0]\n",
        "    end_idx = tf.argmax(outputs.end_logits, axis=1).numpy()[0]\n",
        "    \n",
        "    # Convert token IDs ke text\n",
        "    answer_tokens = inputs['input_ids'][0][start_idx:end_idx+1]\n",
        "    answer = tokenizer.decode(answer_tokens)\n",
        "    \n",
        "    return answer\n",
        "\n",
        "# Test\n",
        "question = \"What is the capital of France?\"\n",
        "context = \"Paris is the capital and largest city of France.\"\n",
        "answer = answer_question(question, context)\n",
        "print(f\"Answer: {answer}\")  # Output: paris\n",
        "```\n",
        "\n",
        "**Penjelasan**: DistilBERT question answering model has two classification heads (start position dan end position). Tokenizer converts question + context ke token IDs, maintaining offset mapping untuk convert char positions ke token positions. Model predicts probability distributions over all positions, argmax selects most likely start/end. Decoder converts token IDs back to readable text.\n",
        "\n",
        "---\n",
        "\n",
        "## Kesimpulan\n",
        "\n",
        "Transformers represent paradigm shift dalam NLP—dari sequential processing (RNN) ke parallel attention-based processing. Key innovations: self-attention memungkinkan model melihat full context simultaneously, positional encodings preserve order information, multi-head attention captures multiple relationship types, dan residual connections + layer normalization stabilize deep networks. BERT proves power dari transfer learning: pretrain large model pada massive corpus, fine-tune untuk specific tasks dengan minimal data. Hugging Face transformers library democratizes access ke state-of-the-art models, making powerful NLP accessible dengan simple APIs. Transformer architecture sudah expand beyond NLP ke computer vision (ViT), multimodal learning (CLIP), dan hampir setiap domain di AI.\n"
      ],
      "metadata": {
        "id": "EM88EI7KSf7q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXf1FwLISipw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}