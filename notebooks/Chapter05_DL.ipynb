{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea3b82c-af79-48ea-bde7-7f3945b20d26",
   "metadata": {},
   "source": [
    "# ðŸš€ State-of-the-art Deep Learning: Transformers\n",
    "**Source: TensorFlow in Action â€“ Chapter 5**\n",
    "\n",
    "Chapter 5 mengenalkan arsitektur **Transformer** untuk NLP, termasuk representasi teks sebagai angka, encoderâ€“decoder, self-attention (termasuk masked dan multi-head), serta contoh implementasi blok Transformer dengan Keras sub-classing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae555acf-d4e5-48e3-8971-ebe807eb8b72",
   "metadata": {},
   "source": [
    "## ðŸ”¡ Representasi Teks sebagai Angka\n",
    "\n",
    "**Theory**: Model deep learning membutuhkan input numerik, sehingga teks dikonversi menjadi urutan ID dan kemudian vektor (one-hot atau embedding).\n",
    "\n",
    "- Tokenisasi â†’ setiap kata diberi ID, misalnya `{\"I\":1, \"went\":2, ...}` sehingga kalimat jadi deret \\([1,2,3,...]\\).\n",
    "- Untuk batch, panjang urutan diseragamkan dengan **padding** token khusus (misalnya ID 0) dan/atau **truncation**.\n",
    "- Vectorisasi: tiap ID dipetakan ke vektor \\(\\mathbb{R}^d\\), sehingga batch berbentuk tensor 3D `[batch, time, d]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10be55a6-4a4d-4a4c-afb7-2e1bb2010a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5],\n",
       "       [6, 7, 8, 0, 0]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# kamus mini\n",
    "vocab = {\n",
    "    \"<PAD>\": 0,\n",
    "    \"I\": 1,\n",
    "    \"went\": 2,\n",
    "    \"to\": 3,\n",
    "    \"the\": 4,\n",
    "    \"beach\": 5,\n",
    "    \"It\": 6,\n",
    "    \"was\": 7,\n",
    "    \"cold\": 8,\n",
    "}\n",
    "\n",
    "sentences = [\n",
    "    [\"I\", \"went\", \"to\", \"the\", \"beach\"],\n",
    "    [\"It\", \"was\", \"cold\"],\n",
    "]\n",
    "\n",
    "max_len = 5\n",
    "\n",
    "def encode_and_pad(sent, vocab, max_len):\n",
    "    ids = [vocab[w] for w in sent]\n",
    "    ids = ids[:max_len]\n",
    "    ids += [0] * (max_len - len(ids))\n",
    "    return ids\n",
    "\n",
    "encoded = np.array([encode_and_pad(s, vocab, max_len) for s in sentences])\n",
    "encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63112c6-ff15-45dc-96bf-3ed42992a8a7",
   "metadata": {},
   "source": [
    "## ðŸ” Arsitektur Encoderâ€“Decoder Transformer\n",
    "\n",
    "**Theory**: Transformer untuk machine translation memetakan kalimat sumber ke representasi laten dengan **encoder**, lalu menghasilkan kalimat target dengan **decoder**.\n",
    "\n",
    "- Encoder: tumpukan layer yang masing-masing berisi *self-attention* + *feed-forward*.\n",
    "- Decoder: tumpukan layer dengan *masked self-attention*, *encoderâ€“decoder attention*, dan *feed-forward* untuk menghasilkan token demi token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f502d-c02c-4edb-bfeb-c4161d50287a",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Self-Attention (Single Head)\n",
    "\n",
    "**Theory**: Self-attention menghasilkan output pada setiap posisi sebagai kombinasi berbobot dari seluruh posisi lain dalam suatu urutan dengan menggunakan tiga proyeksi, yaitu Query \\(Q\\), Key \\(K\\), dan Value \\(V\\).\n",
    "\n",
    "Diberikan input $$\r\n",
    "X \\in \\mathbb{R}^{n \\times d_{\\text{model}}}\r\n",
    "$$\r\n",
    "\n",
    "\n",
    "$$\n",
    "Q = X W_Q,\\quad K = X W_K,\\quad V = X W_V\n",
    "$$\n",
    "\n",
    "den,\n",
    "$$\r\n",
    "W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}\r\n",
    "$$\r\n",
    "\\).\n",
    "\n",
    "Skor perhatian:\n",
    "\n",
    "$$\n",
    "\\text{scores} = \\frac{Q K^\\top}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "Probabilitas perhatian (per baris):\n",
    "\n",
    "$$\n",
    "P = \\text{softmax}(\\text{scores})\n",
    "$$\n",
    "\n",
    "Output self-attention:\n",
    "\n",
    "$$\n",
    "H = P V\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b9130d4-d9c5-4821-ac6a-ddafcafa1d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class SelfAttention(layers.Layer):\n",
    "    def __init__(self, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        d_in = input_shape[-1]\n",
    "        self.Wq = self.add_weight(\n",
    "            shape=(d_in, self.d_model),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "            name=\"Wq\"\n",
    "        )\n",
    "        self.Wk = self.add_weight(\n",
    "            shape=(d_in, self.d_model),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "            name=\"Wk\"\n",
    "        )\n",
    "        self.Wv = self.add_weight(\n",
    "            shape=(d_in, self.d_model),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "            name=\"Wv\"\n",
    "        )\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # x: [batch, time, d_in]\n",
    "        Q = tf.matmul(x, self.Wq)  # [B, T, d_model]\n",
    "        K = tf.matmul(x, self.Wk)\n",
    "        V = tf.matmul(x, self.Wv)\n",
    "\n",
    "        scores = tf.matmul(Q, K, transpose_b=True)  # [B, T, T]\n",
    "        scores = scores / tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask shape: [1, T, T] atau [B, T, T] (1 = keep, 0 = mask)\n",
    "            scores = scores + (mask * -1e9)\n",
    "\n",
    "        attn = tf.nn.softmax(scores, axis=-1)       # [B, T, T]\n",
    "        out = tf.matmul(attn, V)                    # [B, T, d_model]\n",
    "        return out, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef28934-6f4b-44e5-8003-c2d12a45a1a8",
   "metadata": {},
   "source": [
    "## ðŸ•¶ï¸ Masked Self-Attention di Decoder\n",
    "\n",
    "**Theory**: Decoder menggunakan *masked self-attention* agar pada saat memprediksi token ke-\\(t\\), model tidak dapat melihat token setelah posisi \\(t\\) (*future tokens*).\n",
    "\n",
    "Mask segitiga bawah didefinisikan sebagai:\n",
    "$$\n",
    "\\text{mask}[i,j] =\n",
    "\\begin{cases}\n",
    "1, & \\text{jika } j > i \\\\\n",
    "0, & \\text{jika } j \\le i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Mask ini ditambahkan ke skor perhatian sehingga posisi yang ter-*mask* memperoleh nilai yang sangat negatif dan menghasilkan probabilitas mendekati nol setelah fungsi *softmax*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5656ce18-e241-4ba1-8d5b-fea5388f6790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def causal_mask(seq_len):\n",
    "    # 1 untuk posisi yang HARUS di-mask (future), 0 untuk yang boleh dilihat\n",
    "    mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    mask = 1.0 - mask  # upper triangle 1, lower triangle 0\n",
    "    # ubah ke bentuk [1, T, T] agar bisa broadcast ke batch\n",
    "    return tf.reshape(mask, (1, seq_len, seq_len))\n",
    "\n",
    "T = 5\n",
    "mask = causal_mask(T)\n",
    "mask[0].numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b73c237-d286-48e5-928d-2ced192d9fd5",
   "metadata": {},
   "source": [
    "## ðŸ§  Multi-Head Attention\n",
    "\n",
    "**Theory**: Multi-head attention menjalankan beberapa self-attention secara paralel dengan dimensi yang lebih kecil, kemudian menggabungkan hasilnya.\n",
    "\n",
    "Jika terdapat (h) head dan setiap head memiliki dimensi (d_head), maka:\r",
    "\r\n",
    "\n",
    "$$\n",
    "d_{\\text{model}} = h \\cdot d_{\\text{head}}\n",
    "$$\n",
    "\n",
    "Output akhir multi-head attention dirumuskan sebagai:\n",
    "$$\n",
    "H = \\text{Concat}(H^{(1)}, \\dots, H^{(h)}) W_O\n",
    "$$\n",
    "\n",
    "dengan:\n",
    "$$\n",
    "W_O \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9863ac21-f42c-4c43-bb18-8ab05d93e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        d_in = input_shape[-1]\n",
    "        self.Wq = self.add_weight(\n",
    "            shape=(d_in, self.d_model),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "            name=\"Wq\"\n",
    "        )\n",
    "        self.Wk = self.add_weight(\n",
    "            shape=(d_in, self.d_model),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "            name=\"Wk\"\n",
    "        )\n",
    "        self.Wv = self.add_weight(\n",
    "            shape=(d_in, self.d_model),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "            name=\"Wv\"\n",
    "        )\n",
    "        self.Wo = self.add_weight(\n",
    "            shape=(self.d_model, self.d_model),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "            name=\"Wo\"\n",
    "        )\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # x: [B, T, d_model] â†’ [B, num_heads, T, depth]\n",
    "        B, T, _ = tf.unstack(tf.shape(x))\n",
    "        x = tf.reshape(x, (B, T, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        Q = tf.matmul(x, self.Wq)\n",
    "        K = tf.matmul(x, self.Wk)\n",
    "        V = tf.matmul(x, self.Wv)\n",
    "\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "\n",
    "        # skor: [B, H, T, T]\n",
    "        scores = tf.matmul(Q, K, transpose_b=True)\n",
    "        scores = scores / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask: [1, T, T] â†’ broadcast ke [B, H, T, T]\n",
    "            scores = scores + (mask * -1e9)\n",
    "\n",
    "        attn = tf.nn.softmax(scores, axis=-1)\n",
    "        out = tf.matmul(attn, V)          # [B, H, T, depth]\n",
    "\n",
    "        # gabung head: [B, T, d_model]\n",
    "        out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
    "        B, T, _, _ = tf.unstack(tf.shape(out))\n",
    "        out = tf.reshape(out, (B, T, self.d_model))\n",
    "\n",
    "        # proyeksi output\n",
    "        out = tf.matmul(out, self.Wo)\n",
    "        return out, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633293ec-24f8-4ee6-859c-ad9c7fa6d394",
   "metadata": {},
   "source": [
    "## ðŸ§± Feed-Forward Network Posisi\n",
    "\n",
    "**Theory**: Setiap posisi urutan dilewatkan melalui MLP dua lapis yang sama (position-wise FFN) untuk menambah kapasitas non-linear.\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = W_2 \\,\\text{ReLU}(x W_1 + b_1) + b_2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "203fbd6c-dfd5-455f-9575-78725e9a95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(layers.Layer):\n",
    "    def __init__(self, d_model, d_ff, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense1 = layers.Dense(d_ff, activation=\"relu\")\n",
    "        self.dense2 = layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.dense2(self.dense1(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f35f4ba-698e-4699-8f57-852f4ae58e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionwiseFFN(d_model, d_ff)\n",
    "        self.ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        # self-attention + residual + norm\n",
    "        attn_out, _ = self.mha(x, mask)\n",
    "        x = self.ln1(x + attn_out)\n",
    "        # ffn + residual + norm\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ln2(x + ffn_out)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c779021f-9fc6-4379-803a-f795174c60df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 50, 128)           640000    \n",
      "                                                                 \n",
      " encoder_block (EncoderBlock  (None, 50, 128)          197760    \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.math.reduce_mean (TFOpLa  (None, 128)              0         \n",
      " mbda)                                                           \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 838,018\n",
      "Trainable params: 838,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "d_ff = 512\n",
    "max_len = 50\n",
    "\n",
    "# input: urutan ID token\n",
    "inputs = layers.Input(shape=(max_len,))\n",
    "x = layers.Embedding(vocab_size, d_model)(inputs)\n",
    "\n",
    "# (opsional) positional encoding bisa ditambahkan di sini\n",
    "\n",
    "x = EncoderBlock(d_model, num_heads, d_ff)(x)   # 1 layer encoder\n",
    "\n",
    "# pooling sederhana (ambil vektor posisi [CLS]-like: sini pakai mean)\n",
    "x = tf.reduce_mean(x, axis=1)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)  # mis. klasifikasi biner\n",
    "\n",
    "mini_transformer = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "mini_transformer.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "mini_transformer.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402f4e3-48ac-4a1d-8a49-d861944afe7d",
   "metadata": {},
   "source": [
    "## âœ… Ringkasan Chapter 5\n",
    "\n",
    "**Theory**: Transformer menggantikan RNN/CNN pada banyak tugas NLP dengan mengandalkan self-attention penuh pada urutan input, encoderâ€“decoder, dan multi-head attention.\n",
    "\n",
    "Keras sub-classing memudahkan implementasi blok utama (self-attention, multi-head, FFN, encoder/decoder block) yang dapat disusun menjadi model Transformer end-to-end untuk tugas seperti machine translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67953ea3-5768-4474-8f8e-01b7bd1f5d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
